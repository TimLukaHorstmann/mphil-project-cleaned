{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for token-level model\n",
    "(Section 5.3 of the report)\n",
    "\n",
    "Contains the following token-level model specific components:\n",
    "- Code for dataset creation\n",
    "- Post-processing algorithm\n",
    "- Evaluation code\n",
    "\n",
    "![](../Misc/token_level_model.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tlh45/rds/hpc-work/clean_repo/.venv/lib64/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2024-06-01 15:40:42.877073: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-01 15:40:45.174767: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-01 15:41:05.013499: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('../DataPreprocessing')\n",
    "from read_into_dicts import DocReader\n",
    "\n",
    "# Model\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, EarlyStoppingCallback, AutoConfig, AutoModelForTokenClassification\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "import gc\n",
    "from multiprocessing import Pool\n",
    "from token_level_model import TokenClassificationWithDetailedLabels\n",
    "\n",
    "# WANDB\n",
    "import wandb\n",
    "import random\n",
    "import string\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL='roberta-base'\n",
      "DOMAIN='AML'\n",
      "DATA_DICT_FILE_PATH='path/to/aml_data_dict.pkl'\n",
      "DATASET='Model/datasets/token-level/<huggingface_dataset_dict_for_token-level_model.hf>'\n",
      "OVERLAP=256\n",
      "NUMBER_BIO_LABELS=4\n",
      "EPOCHS=15\n",
      "PRETRAINED_MODEL='or6jqv'\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "with open('config.yaml', 'r') as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "\n",
    "# GENERAL:\n",
    "DEBUG = config[\"general\"][\"DEBUG\"]\n",
    "MODEL = config[\"general\"][\"MODEL\"]\n",
    "DATA_SIZE = config[\"general\"][\"DATA_SIZE\"]\n",
    "DOMAIN = config[\"general\"]['DATA_DICT_FILE_PATH']['DOMAIN']\n",
    "DATA_DICT_FILE_PATH = config[\"general\"]['DATA_DICT_FILE_PATH'][DOMAIN]\n",
    "DATA_PATH = config[\"general\"][\"DATA_PATH\"]\n",
    "SPLITS_JSON = config[\"general\"][\"SPLITS_JSON\"]\n",
    "TRAIN_SIZE = config[\"general\"][\"TRAIN_SIZE\"]\n",
    "VAL_SIZE = config[\"general\"][\"VAL_SIZE\"]\n",
    "\n",
    "\n",
    "# MODEL CONFIGS\n",
    "DEFAULT_MODEL = config[\"token_level_model\"][\"DEFAULT_MODEL\"]\n",
    "RETOKENIZATION_NEEDED = config[\"token_level_model\"][\"RETOKENIZATION_NEEDED\"]\n",
    "DATASET = config[\"token_level_model\"][\"DATASET\"]\n",
    "PRETRAINED_MODEL = config[\"token_level_model\"]['PRETRAINED_MODEL']\n",
    "CHECKPOINT = config[\"token_level_model\"][\"CHECKPOINT\"]\n",
    "EPOCHS = config[\"token_level_model\"][\"EPOCHS\"]\n",
    "EARLY_STOPPING_PATIENCE = config[\"token_level_model\"][\"EARLY_STOPPING_PATIENCE\"]\n",
    "BATCH_SIZE = config[\"token_level_model\"][\"BATCH_SIZE\"]\n",
    "OVERLAP = config[\"token_level_model\"][\"SLIDING_WINDOW_OVERLAP\"]\n",
    "\n",
    "# Detailed labels\n",
    "ALPHA = config[\"token_level_model\"][\"ALPHA\"]\n",
    "HIER_LABELS_LEVELS = config[\"token_level_model\"][\"HIER_LABELS_LEVELS\"]  # also acts as flag if detailed models shall be used (i.e. model with auxiliary objective trained)\n",
    "DETAILED_LABEL_WEIGHTS = config[\"token_level_model\"][\"DETAILED_LABEL_WEIGHTS\"]\n",
    "NUMBER_OF_LEVELS = len(HIER_LABELS_LEVELS)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# define special tokens\n",
    "cls_token = tokenizer.cls_token if hasattr(tokenizer, 'cls_token') else '[CLS]'\n",
    "sep_token = tokenizer.sep_token if hasattr(tokenizer, 'sep_token') else '[SEP]'\n",
    "pad_token = tokenizer.pad_token if hasattr(tokenizer, 'pad_token') else '[PAD]'\n",
    "\n",
    "# choose mapping - NOTE: Must also be adpated in dtaaset creation code!\n",
    "label_to_index = {'O': 0, 'B': 1, 'I': 2, 'E': 3, '-100': -100}   # BIOE approach!\n",
    "# label_to_index = {'O': 0, 'B': 1, 'I': 2, '-100': -100}     # BIO approach!\n",
    "\n",
    "index_to_label = {v: k for k, v in label_to_index.items()} # reverse of label_to_index\n",
    "NUMBER_BIO_LABELS = len(label_to_index) -1 # -1 for -100 above\n",
    "\n",
    "print(f\"{MODEL=}\")\n",
    "print(f\"{DOMAIN=}\")\n",
    "print(f\"{DATA_DICT_FILE_PATH=}\")\n",
    "print(f\"{DATASET=}\")\n",
    "print(f\"{OVERLAP=}\")\n",
    "print(f\"{NUMBER_BIO_LABELS=}\")\n",
    "print(f\"{EPOCHS=}\")\n",
    "print(f\"{PRETRAINED_MODEL=}\")\n",
    "\n",
    "# Generate a random tag of 6 characters or choose defined one\n",
    "unique_tag = config[\"general\"][\"UNIQUE_TAG\"] if config[\"general\"][\"UNIQUE_TAG\"] else ''.join(random.choices(string.ascii_letters + string.digits, k=6))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set wandb settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtlh45\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tlh45/rds/hpc-work/Model/wandb/run-20240424_113239-1zn2why1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tlh45/mphil-project/runs/1zn2why1/workspace' target=\"_blank\">SENTENCE-TOKEN_LEVEL-lexlms/legal-roberta-base</a></strong> to <a href='https://wandb.ai/tlh45/mphil-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tlh45/mphil-project' target=\"_blank\">https://wandb.ai/tlh45/mphil-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tlh45/mphil-project/runs/1zn2why1/workspace' target=\"_blank\">https://wandb.ai/tlh45/mphil-project/runs/1zn2why1/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ['WANDB_DIR'] = f\"{DATA_PATH}/Model\"\n",
    "os.environ['WANDB_CACHE_DIR'] = f\"{DATA_PATH}/Model\"\n",
    "\n",
    "wandb_init = {\n",
    "    \"project\": config[\"wandb\"][\"WANDB_PROJECT\"],    \n",
    "    \"tags\": [unique_tag, f\"MODEL={MODEL}\", f\"DATA_SIZE={DATA_SIZE}\"],\n",
    "    \"group\": config[\"wandb\"][\"WANDB_GROUP\"],\n",
    "    \"name\": f'{config[\"wandb\"][\"WANDB_GROUP\"]}-TOKEN_LEVEL-{MODEL}'\n",
    "}\n",
    "wandb.login()\n",
    "run = wandb.init(**wandb_init)\n",
    "        \n",
    "run_id = run.id\n",
    "config_dict = {\n",
    "    \"model\": MODEL,\n",
    "    \"data_size\": DATA_SIZE,\n",
    "    \"train_size\": TRAIN_SIZE,\n",
    "    \"val_size\": VAL_SIZE,\n",
    "    \"test_size\": 1 - TRAIN_SIZE - VAL_SIZE,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"sliding_window_overlap\": OVERLAP,\n",
    "    \"hierarchical_labels_levels\": HIER_LABELS_LEVELS,\n",
    "    \"run_id\": run_id,\n",
    "    \"dataset\": str(DATASET)\n",
    "}\n",
    "wandb.config.update(config_dict, allow_val_change=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Full Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data_dict from /home/tlh45/rds/hpc-work/preprocessing/data_dicts/28245V231219_AML/data_dict_roberta-base-10-04.pkl.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "docReader = DocReader(MODEL, tokenizer)\n",
    "create_original_snippets = True\n",
    "add_full_page = True\n",
    "\n",
    "if DATA_DICT_FILE_PATH:\n",
    "    print(f\"Load data_dict from {DATA_PATH}/{DATA_DICT_FILE_PATH}.\")\n",
    "    with open(f'{DATA_PATH}/{DATA_DICT_FILE_PATH}', 'rb') as handle:\n",
    "        data_dict = pickle.load(handle)\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    data_dict = docReader.preprocess_folder(preprocess=False, folder_path= f'{DATA_PATH}/28245V231219'\n",
    "                                        , data_size=DATA_SIZE, num_workers=76, chunksize=1\n",
    "                                        , extract_title=True, extract_doc_long_id=True, refine_regions=True, create_original_snippets=create_original_snippets, add_full_page=add_full_page\n",
    "                                        , data_dict_folder=f\"{DATA_PATH}/preprocessing/data_dicts/28245V231219_AML\", file_name_additional_suffix=\"-06-04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data_dict file\n",
    "if not DATA_DICT_FILE_PATH:\n",
    "    with open(f'{DATA_PATH}/preprocessing/data_dicts/44704v240404_CYBER/data_dict_{MODEL}-06-04.pkl', 'wb') as handle:\n",
    "        pickle.dump(data_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logic to create training, validation, and test data doc ids --> create centralised data split file\n",
    "(Implemented here, but used across all approaches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # uncomment to run this code (only needed once to create splits)\n",
    "    \n",
    "    def extract_metadata(data_dict):\n",
    "        '''\n",
    "        Extract metadata from data_dict\n",
    "        '''\n",
    "        metadata = {}\n",
    "        for doc_id, doc_data in data_dict.items():\n",
    "            num_pages = len([page for page in doc_data if page not in ['title', 'doc_long_id']])\n",
    "            metadata[doc_id] = num_pages\n",
    "        return metadata\n",
    "\n",
    "    def stratify_and_split(metadata, random_state=42, train_size=TRAIN_SIZE, val_size=VAL_SIZE):\n",
    "        '''\n",
    "        Calculate stratified splits based on metadata and split IDs in data_dict into train, validation and test sets\n",
    "        '''\n",
    "        # Convert metadata to lists\n",
    "        doc_ids = list(metadata.keys())\n",
    "        num_pages = list(metadata.values())\n",
    "\n",
    "        test_size = 1.0 - train_size - val_size\n",
    "        \n",
    "        # Stratify based on number of pages\n",
    "        quartiles = pd.qcut(num_pages, 4, labels=False)\n",
    "        \n",
    "        # First split to separate out training\n",
    "        train_ids, test_val_ids, _, test_val_quartiles = train_test_split(doc_ids, quartiles, test_size=test_size+val_size, stratify=quartiles, random_state=random_state)\n",
    "        # Adjust test_size for second split based on remaining documents and create second split\n",
    "        test_size_adjusted = test_size / (test_size + val_size)\n",
    "        val_ids, test_ids, _, _ = train_test_split(test_val_ids, test_val_quartiles, test_size=test_size_adjusted, stratify=test_val_quartiles, random_state=random_state)\n",
    "        \n",
    "        # Log sizes\n",
    "        sizes = {'train': len(train_ids), 'validation': len(val_ids), 'test': len(test_ids)}\n",
    "\n",
    "        return train_ids, val_ids, test_ids, quartiles, sizes, random_state\n",
    "\n",
    "    def save_splits_as_json(train_ids, val_ids, test_ids, quartiles, sizes, random_state):\n",
    "        '''\n",
    "        Write information about splits to a json file\n",
    "        '''\n",
    "        splits = {\n",
    "            'train_ids': train_ids,\n",
    "            'val_ids': val_ids,\n",
    "            'test_ids': test_ids,\n",
    "            'quartiles_used': quartiles.tolist(),\n",
    "            'sizes': sizes,\n",
    "            'random_state': random_state\n",
    "        }\n",
    "        total = len(train_ids)+len(val_ids)+len(test_ids)\n",
    "        with open(f\"document_splits_for_datasets-{total}.json\", 'w') as f:\n",
    "            json.dump(splits, f, indent=4)\n",
    "\n",
    "    metadata = extract_metadata(data_dict)\n",
    "    train_ids, val_ids, test_ids, quartiles, sizes, random_state = stratify_and_split(metadata)\n",
    "    save_splits_as_json(train_ids, val_ids, test_ids, quartiles, sizes, random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load detailed labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: no HIER_LABELS_LEVELS provided.\n",
      "Total Unique Tags: 0\n",
      "Decoded Labels: []\n"
     ]
    }
   ],
   "source": [
    "# Extract and analyze tags\n",
    "import detailed_labels_handler\n",
    "import importlib\n",
    "importlib.reload(detailed_labels_handler)\n",
    "\n",
    "\n",
    "tag_statistics, mapping_dicts = detailed_labels_handler.extract_and_analyze_tags(data_dict, HIER_LABELS_LEVELS)\n",
    "\n",
    "# Calculate unique tag counts\n",
    "unique_level_tags = {level: len(tags) for level, tags in tag_statistics.items()}\n",
    "total_unique_tags = sum(len(tags) for tags in tag_statistics.values())\n",
    "\n",
    "    \n",
    "if not HIER_LABELS_LEVELS:\n",
    "    print(\"Note: no HIER_LABELS_LEVELS provided.\")\n",
    "# Print statistics and unique counts\n",
    "for level, tags in tag_statistics.items():\n",
    "    print(f\"{level.capitalize()} Tags:\", tags)\n",
    "    print(f\"Unique {level.capitalize()} Tags:\", unique_level_tags[level])\n",
    "\n",
    "# Print full tag count if available\n",
    "if 'full_tags' in tag_statistics:\n",
    "    print(\"Full Tags Count:\", tag_statistics['full_tags'])\n",
    "print(\"Total Unique Tags:\", total_unique_tags)\n",
    "\n",
    "# Print mapping dictionaries\n",
    "for level, mapping_dict in mapping_dicts.items():\n",
    "    print(f\"{level.capitalize()} Mapping Dictionary:\", mapping_dict)\n",
    "\n",
    "# Example below shows how detauled labels are a list of lists, with each sublist containing the detailed labels for a token\n",
    "encoded_labels = [[0, 0, 0, 0], [0, 0, 1, 1]]\n",
    "decoded_labels = detailed_labels_handler.decode_detailed_labels(encoded_labels, mapping_dicts, HIER_LABELS_LEVELS)\n",
    "print(\"Decoded Labels:\", decoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation (Section 5.3.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Datadict from /home/tlh45/rds/hpc-work/Model/datasets/token-level/dataset_dict_1149-docs_roberta-base-model_win256-24-04.hf.\n",
      "Datadict successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "# define output folder here\n",
    "output_folder = f'{DATA_PATH}/preprocessing/token_level_temp_files/temp_token_level_singles_{str(MODEL).replace(\"/\", \"-\")}'\n",
    "skipped_regions = []\n",
    "\n",
    "def tokenize_and_create_labels(tokenized_page_text, refined_regions, max_length=512, overlap=50, doc_id=None, page_index=None):\n",
    "    '''\n",
    "    Labelling function to create BIO(E) labels for tokenized text using indeces identified during data pre-processing\n",
    "    '''\n",
    "    \n",
    "    # Initialise labels for main and auxiliary objective with \"outside region\" labels\n",
    "    labels = ['O'] * len(tokenized_page_text)\n",
    "    detailed_labels = ['N/A'] * len(tokenized_page_text)\n",
    "\n",
    "    # Region labeling logic\n",
    "    for region_index, refined_region in enumerate(refined_regions):\n",
    "        if RETOKENIZATION_NEEDED:\n",
    "            region_text = refined_region['text']\n",
    "            region_tokens = tokenizer.tokenize(region_text)\n",
    "        else:\n",
    "            region_tokens = refined_region['tokenized_text'] # simply pull tokens from data_dict\n",
    "            \n",
    "        if len(region_tokens) == 0:\n",
    "            continue #jump to next region if region is empty\n",
    "        \n",
    "        region_tags = ';'.join(refined_region.get('tags', [])) # extract detailed labels (\"tags\"), if available\n",
    "        \n",
    "        # Extract IDs as identified during pre-processing\n",
    "        start_idx = refined_region['start_idx_in_page']\n",
    "        end_idx = refined_region['end_idx_in_page']\n",
    "        if end_idx >= len(labels) or start_idx == -1 or end_idx == -1: # skip regions that could not be identified during preprocessing\n",
    "            print(\"WARNING:\")\n",
    "            print(f\"{end_idx=}, len of labels: {len(labels)}\")\n",
    "            print(f\"Skipping {doc_id=}, {page_index=}.\")\n",
    "            skipped_regions.append((doc_id, page_index, region_index))\n",
    "            continue\n",
    "        \n",
    "        # Simply assign labels\n",
    "        labels[start_idx] = 'B'\n",
    "        for i in range(start_idx + 1, end_idx):\n",
    "            if i < len(labels):\n",
    "                labels[i] = 'I'\n",
    "        labels[end_idx] = 'E' # need to mark here whether BIOE (\"E\") or BIO (\"I\")!\n",
    "\n",
    "        # For auxiliary objective, we mark the inside region labels with the region tags\n",
    "        if HIER_LABELS_LEVELS:\n",
    "            for idx in range(start_idx, end_idx+1):\n",
    "                detailed_labels[idx] = region_tags if region_tags else \"N/A\"\n",
    "\n",
    "    # Sliding window approach: split now labelled full page into sequences with overlaps\n",
    "    # Necessary step due to context window of BERT-based models\n",
    "    start = 0\n",
    "    page_length = len(tokenized_page_text)\n",
    "    while start < page_length:\n",
    "        end = min(start + max_length - 2, page_length) # account for special start and end tokens\n",
    "        \n",
    "        # Adjust end to avoid splitting a word!\n",
    "        while end < page_length and tokenized_page_text[end].startswith('##') and end - start < max_length - 2:\n",
    "            end += 1\n",
    "\n",
    "        # Define tokens belonging to this window/sequence\n",
    "        seq_tokens = tokenized_page_text[start:end]\n",
    "        seq_labels = labels[start:end]\n",
    "        seq_detailed_labels = detailed_labels[start:end]\n",
    "        yield (seq_tokens, seq_labels, seq_detailed_labels)\n",
    "\n",
    "        # Update start for next window considering overlap\n",
    "        start = max(start + overlap, end - overlap) if end < page_length else end\n",
    "\n",
    "\n",
    "def finalize_sequences(sequences, tokenizer, max_length=512):\n",
    "    '''\n",
    "    Function to finalise all windows/sequences by adding special tokens and padding to max_length\n",
    "    '''\n",
    "    \n",
    "    # Define special tokens according to tokenizer used (i.e. special tokens are different between models such as BERT and RoBERTa)\n",
    "    # --> pull from corresponding tokenizer\n",
    "    cls_token = tokenizer.cls_token if hasattr(tokenizer, 'cls_token') else '[CLS]'\n",
    "    sep_token = tokenizer.sep_token if hasattr(tokenizer, 'sep_token') else '[SEP]'\n",
    "    pad_token = tokenizer.pad_token if hasattr(tokenizer, 'pad_token') else '[PAD]'\n",
    "\n",
    "    # Iterate over all sequences\n",
    "    for tokens, bio_labels, detailed_labels in sequences:\n",
    "        # Initial len before adding special tokens or padding\n",
    "        original_len = len(tokens)\n",
    "\n",
    "        # Add special tokens\n",
    "        tokens = [cls_token] + list(tokens) + [sep_token] # beginning and end tokens\n",
    "        bio_labels = ['-100'] + list(bio_labels) + ['-100']  # add label '-100' for special tokens\n",
    "        detailed_labels = ['-100'] + list(detailed_labels) + ['-100']  # add label '-100' for special tokens\n",
    "        if len(tokens) > max_length:\n",
    "            print(f\"WARNING: length of tokens ({len(tokens)}) exceed max length of {max_length}!\")\n",
    "\n",
    "        # Padding\n",
    "        padding_length = max_length - len(tokens)\n",
    "        tokens.extend([pad_token] * padding_length)\n",
    "        bio_labels.extend(['-100'] * padding_length)  # add label '-100' for padding tokens\n",
    "        detailed_labels.extend(['-100'] * padding_length)  # add label '-100' for padding tokens\n",
    "\n",
    "        # Convert tokens to IDs\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        attention_mask = [1 if token != pad_token else 0 for token in tokens] # create attention mask: 0 for padding tokens, 1 for real tokens\n",
    "\n",
    "        # Convert labels to numerical values\n",
    "        numerical_bio_labels = [-100] * max_length  # initialise all labels to -100\n",
    "        numerical_bio_labels[1:original_len+1] = [label_to_index[label] for label in bio_labels[1:original_len+1]]  # align labels with tokens (given special tokens)\n",
    "\n",
    "        numerical_detailed_labels = [[-1]*NUMBER_OF_LEVELS] * max_length  # initialise all labels to -100\n",
    "        numerical_detailed_labels[1:original_len+1] = detailed_labels_handler.encode_detailed_labels(detailed_labels[1:original_len+1], mapping_dicts, NUMBER_OF_LEVELS) # encoding via mapping dictionaries\n",
    "\n",
    "        yield (input_ids, attention_mask, numerical_bio_labels, numerical_detailed_labels)\n",
    "        \n",
    "def process_documents_batch(batch_args):\n",
    "    '''\n",
    "    Process batch of documents\n",
    "    '''\n",
    "    results_files = []\n",
    "    for args in batch_args:\n",
    "        results_file = process_document(args)\n",
    "        results_files.append(results_file)\n",
    "    return results_files\n",
    "\n",
    "def process_document(args):\n",
    "    '''\n",
    "    Process a single document\n",
    "    '''\n",
    "    doc_id, pages, tokenizer, max_length, overlap = args # unload arguments\n",
    "\n",
    "    results = []\n",
    "    for page_index, page_content in pages.items(): # go through all pages of the document\n",
    "        if page_index in ['title', 'doc_long_id']:\n",
    "            continue\n",
    "        \n",
    "        if RETOKENIZATION_NEEDED:\n",
    "            tokenized_page_text = tokenizer.tokenize(page_content['full_text'])\n",
    "        else:\n",
    "            tokenized_page_text = page_content['tokenized_full_text']\n",
    "        refined_regions = page_content['refined_regions']\n",
    "\n",
    "        seq_index = 0\n",
    "        # Tokenize and create labels for each window/sequence\n",
    "        for seq in tokenize_and_create_labels(tokenized_page_text, refined_regions, max_length=max_length, overlap=overlap, doc_id=doc_id, page_index=page_index):\n",
    "            # Dinalise each window/sequence on the page\n",
    "            for finalized_seq in finalize_sequences([seq], tokenizer, max_length=max_length):\n",
    "                input_ids, attention_mask, numerical_bio_labels, numerical_detailed_labels = finalized_seq\n",
    "                metadata = {'doc_id': doc_id, 'page_id': page_index, 'seq_index': seq_index} # add metadata\n",
    "                # Construct sample\n",
    "                sample = {\n",
    "                    'input_ids': input_ids\n",
    "                    , 'attention_mask': attention_mask\n",
    "                    , 'bio_labels': numerical_bio_labels\n",
    "                    , 'detailed_labels': numerical_detailed_labels\n",
    "                    , 'metadata': metadata\n",
    "                }\n",
    "                # Add to results\n",
    "                results.append(sample)\n",
    "                seq_index += 1\n",
    "\n",
    "    # Write into temporary file (necessary for more efficient memory handling)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    results_file = os.path.join(output_folder, f\"doc_{doc_id}_results.pkl\")\n",
    "    with open(results_file, 'wb') as file:\n",
    "        pickle.dump(results, file)\n",
    "\n",
    "    return results_file\n",
    "\n",
    "def load_processed_data(processed_files, document_splits):\n",
    "    '''\n",
    "    Load processed data and split into train, validation and test sets according to split definitions\n",
    "    '''\n",
    "    train_ids = set(document_splits['train_ids'])\n",
    "    val_ids = set(document_splits['val_ids'])\n",
    "    test_ids = set(document_splits['test_ids'])\n",
    "        \n",
    "    train_samples = []\n",
    "    val_samples = []\n",
    "    test_samples = []\n",
    "\n",
    "    for results_file in tqdm(processed_files):\n",
    "        with open(results_file, 'rb') as file:\n",
    "            doc_results = pickle.load(file)\n",
    "            for sample in doc_results:\n",
    "                if sample['metadata']['doc_id'] in train_ids:\n",
    "                    train_samples.append(sample)\n",
    "                if sample['metadata']['doc_id'] in val_ids:\n",
    "                    val_samples.append(sample)\n",
    "                if sample['metadata']['doc_id'] in test_ids:\n",
    "                    test_samples.append(sample)\n",
    "\n",
    "    return train_samples, val_samples, test_samples\n",
    "\n",
    "\n",
    "# MAIN ENTRY POINT\n",
    "# Process and tokenize the data\n",
    "# Create or load dataset\n",
    "if DATASET is not None:\n",
    "    print(f\"Load Datadict from {DATA_PATH}/{DATASET}.\")\n",
    "    dataset_dict = DatasetDict.load_from_disk(f\"{DATA_PATH}/{DATASET}\")\n",
    "    print(\"Datadict successfully loaded.\")\n",
    "else:\n",
    "    # Create arguments\n",
    "    args = [(doc_id, pages, tokenizer, 512, OVERLAP) for doc_id, pages in data_dict.items()]\n",
    "\n",
    "    # Debug: analyse process for a single document\n",
    "    if DEBUG:\n",
    "        for arg in tqdm(args):\n",
    "            if arg[0] == \"17222128\":\n",
    "                results = process_document(arg)\n",
    "    else:\n",
    "        print(\"Check which files need processing.\")\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        existing_files = set(os.listdir(output_folder))\n",
    "        already_processed_doc_ids = {file_name.split('_')[1] for file_name in existing_files if file_name.endswith('.pkl')}\n",
    "\n",
    "        # Filter args to exclude already processed documents\n",
    "        args = [arg for arg in args if str(arg[0]) not in already_processed_doc_ids]\n",
    "\n",
    "        if args:\n",
    "            del data_dict\n",
    "            gc.collect()\n",
    "\n",
    "            print(\"Initiate Dataset Creation.\")\n",
    "            batch_size = 10  # Number of documents per batch\n",
    "\n",
    "            with Pool(1) as pool: # allows multiprocessing, but not needed here as process using pre-determined labels is already fast enough\n",
    "                total_batches = (len(args) + batch_size - 1) // batch_size  # calculate total number of batches\n",
    "                pbar = tqdm(total=total_batches, unit=\"batch\", desc=\"Process documents in batches\")  # progress bar\n",
    "\n",
    "                for i in range(0, len(args), batch_size):\n",
    "                    batch_args = [args[i:i + batch_size]]\n",
    "                    for batch_output_files in pool.imap_unordered(process_documents_batch, batch_args):\n",
    "                        pbar.update(1)  # update pb per completed batch\n",
    "                    \n",
    "                    # force garbage collection\n",
    "                    del batch_args\n",
    "                    gc.collect() \n",
    "                pbar.close()\n",
    "\n",
    "            # force garbage collection\n",
    "            del args\n",
    "            gc.collect()\n",
    "        else:\n",
    "            print(f\"NOTE: No preprocessing necessary. You might want to delete the {output_folder=} to force new preprocessing.\")\n",
    "        print(f\"Load processed data from {output_folder}.\")\n",
    "        \n",
    "        # Split dataset into different datasets\n",
    "        with open(SPLITS_JSON, 'r') as f:\n",
    "            document_splits = json.load(f)\n",
    "            \n",
    "        output_files = set(os.listdir(output_folder))\n",
    "        output_files = [f\"{output_folder}/{file}\" for file in output_files]\n",
    "        train_samples, val_samples, test_samples = load_processed_data(output_files, document_splits)\n",
    "\n",
    "        # Create final dataset\n",
    "        print(\"Create datasets.\")\n",
    "        train_dataset = Dataset.from_list(train_samples)\n",
    "        val_dataset = Dataset.from_list(val_samples)\n",
    "        test_dataset = Dataset.from_list(test_samples)\n",
    "\n",
    "        # Combining splits into a DatasetDict\n",
    "        dataset_dict = DatasetDict({\n",
    "            'train': train_dataset,\n",
    "            'validation': val_dataset,\n",
    "            'test': test_dataset\n",
    "        })\n",
    "\n",
    "        # Save to disk\n",
    "        dataset_path = f'{DATA_PATH}/Model/datasets/token-level/dataset_dict_{DATA_SIZE}-docs_{MODEL.replace(\"/\", \"-\")}-model_win{OVERLAP}_BIO-24-04.hf'\n",
    "        print(f\"Save dataset_dict to {dataset_path}.\")\n",
    "        dataset_dict.save_to_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manually analyse preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON splits\n",
    "with open(SPLITS_JSON, 'r') as file:\n",
    "    document_splits = json.load(file)\n",
    "    \n",
    "# Convert JSON lists to sets for easier comparison\n",
    "json_train_ids = set(document_splits['train_ids'])\n",
    "json_val_ids = set(document_splits['val_ids'])\n",
    "json_test_ids = set(document_splits['test_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assertion for correct document splits:\n",
    "(same as in model script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract document IDs from train.\n",
      "Extract document IDs from validation.\n",
      "Extract document IDs from test.\n",
      "Validation successful: Datasets contain the correct document IDs according to the JSON splits.\n"
     ]
    }
   ],
   "source": [
    "def extract_doc_ids_from_dataset(dataset):\n",
    "    return {sample['metadata']['doc_id'] for sample in dataset}\n",
    "\n",
    "print(\"Extract document IDs from train.\")\n",
    "train_doc_ids = set(extract_doc_ids_from_dataset(dataset_dict['train']))\n",
    "print(\"Extract document IDs from validation.\")\n",
    "val_doc_ids = set(extract_doc_ids_from_dataset(dataset_dict['validation']))\n",
    "print(\"Extract document IDs from test.\")\n",
    "test_doc_ids = set(extract_doc_ids_from_dataset(dataset_dict['test']))\n",
    "\n",
    "# Validate match\n",
    "assert train_doc_ids == json_train_ids, \"Mismatch in train dataset document IDs\"\n",
    "assert val_doc_ids == json_val_ids, \"Mismatch in validation dataset document IDs\"\n",
    "assert test_doc_ids == json_test_ids, \"Mismatch in test dataset document IDs\"\n",
    "\n",
    "print(\"Validation successful: Datasets contain the correct document IDs according to the JSON splits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create mapping between doc_id, page_index and sample in dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9220347527143fba32a1cd8a5fe8865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset_index_map = {}\n",
    "for idx, sample in enumerate(tqdm(dataset_dict['train'])):\n",
    "    doc_id = sample['metadata']['doc_id']\n",
    "    page_id = sample['metadata']['page_id']\n",
    "    if (doc_id, page_id) not in train_dataset_index_map:\n",
    "        train_dataset_index_map[(doc_id, page_id)] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print individual samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token           Attention Mask  BIO Label  Detailed Labels               \n",
      "----------------------------------------------------------------------\n",
      "<s>             1               -100       [-1, -1, -1, -1]              \n",
      "A               1               0          [-1, -1, -1, -1]              \n",
      "Ġ108            1               0          [-1, -1, -1, -1]              \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "SR              1               0          [-1, -1, -1, -1]              \n",
      "O               1               0          [-1, -1, -1, -1]              \n",
      ".               1               0          [-1, -1, -1, -1]              \n",
      "Ġ6              1               0          [-1, -1, -1, -1]              \n",
      "Ġ               1               0          [-1, -1, -1, -1]              \n",
      "Ġ               1               0          [-1, -1, -1, -1]              \n",
      "Ġ               1               0          [-1, -1, -1, -1]              \n",
      "Ġ               1               0          [-1, -1, -1, -1]              \n",
      "Ġ               1               0          [-1, -1, -1, -1]              \n",
      "Ġ               1               0          [-1, -1, -1, -1]              \n",
      "Ġ               1               0          [-1, -1, -1, -1]              \n",
      "ĠProceed        1               0          [-1, -1, -1, -1]              \n",
      "s               1               0          [-1, -1, -1, -1]              \n",
      "Ġof             1               0          [-1, -1, -1, -1]              \n",
      "ĠCrime          1               0          [-1, -1, -1, -1]              \n",
      "Ġ(              1               0          [-1, -1, -1, -1]              \n",
      "Anti            1               0          [-1, -1, -1, -1]              \n",
      "-               1               0          [-1, -1, -1, -1]              \n",
      "Money           1               0          [-1, -1, -1, -1]              \n",
      "ĠL              1               0          [-1, -1, -1, -1]              \n",
      "aundering       1               0          [-1, -1, -1, -1]              \n",
      "Ġand            1               0          [-1, -1, -1, -1]              \n",
      "Ġ               1               0          [-1, -1, -1, -1]              \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "2012            1               1          [0, 5, 12, 19]                \n",
      "Ċ               1               2          [0, 5, 12, 19]                \n",
      "Ġ               1               2          [0, 5, 12, 19]                \n",
      "Ġ               1               2          [0, 5, 12, 19]                \n",
      "ĠTerror         1               2          [0, 5, 12, 19]                \n",
      "ist             1               2          [0, 5, 12, 19]                \n",
      "ĠFin            1               2          [0, 5, 12, 19]                \n",
      "ancing          1               2          [0, 5, 12, 19]                \n",
      ")               1               2          [0, 5, 12, 19]                \n",
      "ĠGuidelines     1               2          [0, 5, 12, 19]                \n",
      "Ġ               1               2          [0, 5, 12, 19]                \n",
      "Ċ               1               2          [0, 5, 12, 19]                \n",
      "Ġ               1               2          [0, 5, 12, 19]                \n",
      "Ċ               1               2          [0, 5, 12, 19]                \n",
      "Ġindividual     1               2          [0, 5, 12, 19]                \n",
      "Ġand            1               2          [0, 5, 12, 19]                \n",
      "Ġa              1               2          [0, 5, 12, 19]                \n",
      "Ġbrief          1               2          [0, 5, 12, 19]                \n",
      "Ġdescription    1               2          [0, 5, 12, 19]                \n",
      "Ġof             1               2          [0, 5, 12, 19]                \n",
      "Ġthe            1               2          [0, 5, 12, 19]                \n",
      "Ġcustomer       1               2          [0, 5, 12, 19]                \n",
      "âĢ              1               2          [0, 5, 12, 19]                \n",
      "Ļ               1               2          [0, 5, 12, 19]                \n",
      "s               1               2          [0, 5, 12, 19]                \n",
      "Ġor             1               2          [0, 5, 12, 19]                \n",
      "Ġkey            1               2          [0, 5, 12, 19]                \n",
      "Ġstaff          1               2          [0, 5, 12, 19]                \n",
      "âĢ              1               2          [0, 5, 12, 19]                \n",
      "Ļ               1               2          [0, 5, 12, 19]                \n",
      "s               1               2          [0, 5, 12, 19]                \n",
      "Ċ               1               2          [0, 5, 12, 19]                \n",
      "knowledge       1               2          [0, 5, 12, 19]                \n",
      "Ġof             1               2          [0, 5, 12, 19]                \n",
      "Ġthe            1               2          [0, 5, 12, 19]                \n",
      "Ġindividual     1               2          [0, 5, 12, 19]                \n",
      ".               1               2          [0, 5, 12, 19]                \n",
      "Ġ               1               2          [0, 5, 12, 19]                \n",
      "Ċ               1               2          [0, 5, 12, 19]                \n",
      "(               1               2          [0, 5, 12, 19]                \n",
      "6               1               2          [0, 5, 12, 19]                \n",
      ")               1               2          [0, 5, 12, 19]                \n",
      "ĠWhere          1               2          [0, 5, 12, 19]                \n",
      "Ġa              1               2          [0, 5, 12, 19]                \n",
      "Ġpersonal       1               2          [0, 5, 12, 19]                \n",
      "Ġaccount        1               2          [0, 5, 12, 19]                \n",
      "Ġche            1               2          [0, 5, 12, 19]                \n",
      "que             1               2          [0, 5, 12, 19]                \n",
      "Ġis             1               2          [0, 5, 12, 19]                \n",
      "Ġtend           1               2          [0, 5, 12, 19]                \n",
      "ered            1               2          [0, 5, 12, 19]                \n",
      "Ġto             1               2          [0, 5, 12, 19]                \n",
      "Ġopen           1               2          [0, 5, 12, 19]                \n",
      "Ġan             1               2          [0, 5, 12, 19]                \n",
      "Ġaccount        1               2          [0, 5, 12, 19]                \n",
      ",               1               2          [0, 5, 12, 19]                \n",
      "Ġthe            1               2          [0, 5, 12, 19]                \n",
      "Ġsignature      1               2          [0, 5, 12, 19]                \n",
      "Ċ               1               2          [0, 5, 12, 19]                \n",
      "on              1               2          [0, 5, 12, 19]                \n",
      "Ġthe            1               2          [0, 5, 12, 19]                \n",
      "Ġche            1               2          [0, 5, 12, 19]                \n",
      "que             1               2          [0, 5, 12, 19]                \n",
      "Ġshall          1               2          [0, 5, 12, 19]                \n",
      "Ġbe             1               2          [0, 5, 12, 19]                \n",
      "Ġcompared       1               2          [0, 5, 12, 19]                \n",
      "Ġwith           1               2          [0, 5, 12, 19]                \n",
      "Ġthe            1               2          [0, 5, 12, 19]                \n",
      "Ġspecimen       1               2          [0, 5, 12, 19]                \n",
      "Ġsignature      1               2          [0, 5, 12, 19]                \n",
      "Ġsubmitted      1               2          [0, 5, 12, 19]                \n",
      "Ġunder          1               2          [0, 5, 12, 19]                \n",
      "Ġsubsection     1               2          [0, 5, 12, 19]                \n",
      "Ċ               1               2          [0, 5, 12, 19]                \n",
      "(               1               2          [0, 5, 12, 19]                \n",
      "5               1               2          [0, 5, 12, 19]                \n",
      ")               1               2          [0, 5, 12, 19]                \n",
      "Ġ(              1               2          [0, 5, 12, 19]                \n",
      "b               1               2          [0, 5, 12, 19]                \n",
      ").              1               2          [0, 5, 12, 19]                \n",
      "Ġ               1               2          [0, 5, 12, 19]                \n",
      "Ċ               1               2          [0, 5, 12, 19]                \n",
      "(               1               2          [0, 5, 12, 19]                \n",
      "7               1               2          [0, 5, 12, 19]                \n",
      ")               1               2          [0, 5, 12, 19]                \n",
      "ĠAn             1               2          [0, 5, 12, 19]                \n",
      "Ġentity         1               2          [0, 5, 12, 19]                \n",
      "Ġor             1               2          [0, 5, 12, 19]                \n",
      "Ġa              1               2          [0, 5, 12, 19]                \n",
      "Ġprofessional   1               2          [0, 5, 12, 19]                \n",
      "Ġthat           1               2          [0, 5, 12, 19]                \n",
      "Ġfails          1               2          [0, 5, 12, 19]                \n",
      "Ġto             1               2          [0, 5, 12, 19]                \n",
      "Ġcomply         1               2          [0, 5, 12, 19]                \n",
      "Ġwith           1               2          [0, 5, 12, 19]                \n",
      "Ġthe            1               2          [0, 5, 12, 19]                \n",
      "Ġrequirements   1               2          [0, 5, 12, 19]                \n",
      "Ġof             1               2          [0, 5, 12, 19]                \n",
      "Ġthis           1               2          [0, 5, 12, 19]                \n",
      "Ċ               1               2          [0, 5, 12, 19]                \n",
      "section         1               2          [0, 5, 12, 19]                \n",
      "Ġcommits        1               2          [0, 5, 12, 19]                \n",
      "Ġan             1               2          [0, 5, 12, 19]                \n",
      "Ġoffence        1               2          [0, 5, 12, 19]                \n",
      "Ġand            1               2          [0, 5, 12, 19]                \n",
      "Ġis             1               2          [0, 5, 12, 19]                \n",
      "Ġliable         1               2          [0, 5, 12, 19]                \n",
      "Ġto             1               2          [0, 5, 12, 19]                \n",
      "Ġbe             1               2          [0, 5, 12, 19]                \n",
      "Ġproceeded      1               2          [0, 5, 12, 19]                \n",
      "Ġagainst        1               2          [0, 5, 12, 19]                \n",
      "Ġunder          1               2          [0, 5, 12, 19]                \n",
      "Ġsection        1               2          [0, 5, 12, 19]                \n",
      "Ġ32             1               2          [0, 5, 12, 19]                \n",
      "(               1               2          [0, 5, 12, 19]                \n",
      "4               1               2          [0, 5, 12, 19]                \n",
      ")               1               2          [0, 5, 12, 19]                \n",
      "Ġof             1               2          [0, 5, 12, 19]                \n",
      "Ċ               1               2          [0, 5, 12, 19]                \n",
      "the             1               2          [0, 5, 12, 19]                \n",
      "ĠProceed        1               2          [0, 5, 12, 19]                \n",
      "s               1               2          [0, 5, 12, 19]                \n",
      "Ġof             1               2          [0, 5, 12, 19]                \n",
      "ĠCrime          1               2          [0, 5, 12, 19]                \n",
      "ĠAct            1               2          [0, 5, 12, 19]                \n",
      ".               1               3          [0, 5, 12, 19]                \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "ĠExplan         1               0          [-1, -1, -1, -1]              \n",
      "ation           1               0          [-1, -1, -1, -1]              \n",
      ":               1               0          [-1, -1, -1, -1]              \n",
      "Ġ               1               0          [-1, -1, -1, -1]              \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "Ġ               1               0          [-1, -1, -1, -1]              \n",
      "Ġ               1               0          [-1, -1, -1, -1]              \n",
      "Ġ               1               0          [-1, -1, -1, -1]              \n",
      "Ġ               1               0          [-1, -1, -1, -1]              \n",
      "Ġ               1               0          [-1, -1, -1, -1]              \n",
      "Ġ               1               0          [-1, -1, -1, -1]              \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "(               1               0          [-1, -1, -1, -1]              \n",
      "i               1               0          [-1, -1, -1, -1]              \n",
      ")               1               0          [-1, -1, -1, -1]              \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "The             1               0          [-1, -1, -1, -1]              \n",
      "Ġidentification 1               0          [-1, -1, -1, -1]              \n",
      "Ġand            1               0          [-1, -1, -1, -1]              \n",
      "Ġverification   1               0          [-1, -1, -1, -1]              \n",
      "Ġprocess        1               0          [-1, -1, -1, -1]              \n",
      "Ġin             1               0          [-1, -1, -1, -1]              \n",
      "Ġrelation       1               0          [-1, -1, -1, -1]              \n",
      "Ġto             1               0          [-1, -1, -1, -1]              \n",
      "Ġan             1               0          [-1, -1, -1, -1]              \n",
      "Ġindividual     1               0          [-1, -1, -1, -1]              \n",
      "Ġis             1               0          [-1, -1, -1, -1]              \n",
      "Ġa              1               0          [-1, -1, -1, -1]              \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "cru             1               0          [-1, -1, -1, -1]              \n",
      "cial            1               0          [-1, -1, -1, -1]              \n",
      "Ġaspect         1               0          [-1, -1, -1, -1]              \n",
      "Ġof             1               0          [-1, -1, -1, -1]              \n",
      "Ġthe            1               0          [-1, -1, -1, -1]              \n",
      "Ġprocess        1               0          [-1, -1, -1, -1]              \n",
      "Ġof             1               0          [-1, -1, -1, -1]              \n",
      "Ġproperly       1               0          [-1, -1, -1, -1]              \n",
      "Ġmanaging       1               0          [-1, -1, -1, -1]              \n",
      "Ġany            1               0          [-1, -1, -1, -1]              \n",
      "Ġpotential      1               0          [-1, -1, -1, -1]              \n",
      "Ġrisks          1               0          [-1, -1, -1, -1]              \n",
      ".               1               0          [-1, -1, -1, -1]              \n",
      "ĠIn             1               0          [-1, -1, -1, -1]              \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "each            1               0          [-1, -1, -1, -1]              \n",
      "Ġcase           1               0          [-1, -1, -1, -1]              \n",
      "Ġof             1               0          [-1, -1, -1, -1]              \n",
      "Ġan             1               0          [-1, -1, -1, -1]              \n",
      "Ġapplication    1               0          [-1, -1, -1, -1]              \n",
      "Ġto             1               0          [-1, -1, -1, -1]              \n",
      "Ġestablish      1               0          [-1, -1, -1, -1]              \n",
      "Ġa              1               0          [-1, -1, -1, -1]              \n",
      "Ġbusiness       1               0          [-1, -1, -1, -1]              \n",
      "Ġrelationship   1               0          [-1, -1, -1, -1]              \n",
      ",               1               0          [-1, -1, -1, -1]              \n",
      "Ġit             1               0          [-1, -1, -1, -1]              \n",
      "Ġis             1               0          [-1, -1, -1, -1]              \n",
      "Ġa              1               0          [-1, -1, -1, -1]              \n",
      "Ġmatter         1               0          [-1, -1, -1, -1]              \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "of              1               0          [-1, -1, -1, -1]              \n",
      "Ġprud           1               0          [-1, -1, -1, -1]              \n",
      "ence            1               0          [-1, -1, -1, -1]              \n",
      "Ġand            1               0          [-1, -1, -1, -1]              \n",
      "Ġjudgment       1               0          [-1, -1, -1, -1]              \n",
      "Ġon             1               0          [-1, -1, -1, -1]              \n",
      "Ġthe            1               0          [-1, -1, -1, -1]              \n",
      "Ġpart           1               0          [-1, -1, -1, -1]              \n",
      "Ġof             1               0          [-1, -1, -1, -1]              \n",
      "Ġthe            1               0          [-1, -1, -1, -1]              \n",
      "Ġentity         1               0          [-1, -1, -1, -1]              \n",
      "Ġor             1               0          [-1, -1, -1, -1]              \n",
      "Ġprofessional   1               0          [-1, -1, -1, -1]              \n",
      "Ġwith           1               0          [-1, -1, -1, -1]              \n",
      "Ġwhich          1               0          [-1, -1, -1, -1]              \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "or              1               0          [-1, -1, -1, -1]              \n",
      "Ġwith           1               0          [-1, -1, -1, -1]              \n",
      "Ġwhom           1               0          [-1, -1, -1, -1]              \n",
      "Ġthe            1               0          [-1, -1, -1, -1]              \n",
      "Ġrelationship   1               0          [-1, -1, -1, -1]              \n",
      "Ġis             1               0          [-1, -1, -1, -1]              \n",
      "Ġsought         1               0          [-1, -1, -1, -1]              \n",
      "Ġto             1               0          [-1, -1, -1, -1]              \n",
      "Ġcarry          1               0          [-1, -1, -1, -1]              \n",
      "Ġout            1               0          [-1, -1, -1, -1]              \n",
      "Ġthe            1               0          [-1, -1, -1, -1]              \n",
      "Ġrequisite      1               0          [-1, -1, -1, -1]              \n",
      "Ġdue            1               0          [-1, -1, -1, -1]              \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "d               1               0          [-1, -1, -1, -1]              \n",
      "il              1               0          [-1, -1, -1, -1]              \n",
      "ig              1               0          [-1, -1, -1, -1]              \n",
      "ence            1               0          [-1, -1, -1, -1]              \n",
      "Ġmeasures       1               0          [-1, -1, -1, -1]              \n",
      ";               1               0          [-1, -1, -1, -1]              \n",
      "Ġa              1               0          [-1, -1, -1, -1]              \n",
      "Ġlot            1               0          [-1, -1, -1, -1]              \n",
      "Ġmay            1               0          [-1, -1, -1, -1]              \n",
      "Ġbe             1               0          [-1, -1, -1, -1]              \n",
      "Ġlearned        1               0          [-1, -1, -1, -1]              \n",
      "Ġfrom           1               0          [-1, -1, -1, -1]              \n",
      "Ġthe            1               0          [-1, -1, -1, -1]              \n",
      "Ġapplicant      1               0          [-1, -1, -1, -1]              \n",
      "Ġfor            1               0          [-1, -1, -1, -1]              \n",
      "Ġbusiness       1               0          [-1, -1, -1, -1]              \n",
      "Ġor             1               0          [-1, -1, -1, -1]              \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "custom          1               0          [-1, -1, -1, -1]              \n",
      "er              1               0          [-1, -1, -1, -1]              \n",
      ",               1               0          [-1, -1, -1, -1]              \n",
      "Ġranging        1               0          [-1, -1, -1, -1]              \n",
      "Ġfrom           1               0          [-1, -1, -1, -1]              \n",
      "Ġhis            1               0          [-1, -1, -1, -1]              \n",
      "Ġdem            1               0          [-1, -1, -1, -1]              \n",
      "ean             1               0          [-1, -1, -1, -1]              \n",
      "our             1               0          [-1, -1, -1, -1]              \n",
      ",               1               0          [-1, -1, -1, -1]              \n",
      "Ġtruth          1               0          [-1, -1, -1, -1]              \n",
      "fulness         1               0          [-1, -1, -1, -1]              \n",
      ",               1               0          [-1, -1, -1, -1]              \n",
      "Ġwillingness    1               0          [-1, -1, -1, -1]              \n",
      "Ġto             1               0          [-1, -1, -1, -1]              \n",
      "Ġanswer         1               0          [-1, -1, -1, -1]              \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "quest           1               0          [-1, -1, -1, -1]              \n",
      "ions            1               0          [-1, -1, -1, -1]              \n",
      "Ġto             1               0          [-1, -1, -1, -1]              \n",
      "Ġvolunteering   1               0          [-1, -1, -1, -1]              \n",
      "Ġinformation    1               0          [-1, -1, -1, -1]              \n",
      "Ġwhich          1               0          [-1, -1, -1, -1]              \n",
      "Ġby             1               0          [-1, -1, -1, -1]              \n",
      "Ġthe            1               0          [-1, -1, -1, -1]              \n",
      "Ġnature         1               0          [-1, -1, -1, -1]              \n",
      "Ġof             1               0          [-1, -1, -1, -1]              \n",
      "Ġthe            1               0          [-1, -1, -1, -1]              \n",
      "Ġrelationship   1               0          [-1, -1, -1, -1]              \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "s               1               0          [-1, -1, -1, -1]              \n",
      "ought           1               0          [-1, -1, -1, -1]              \n",
      "Ġmay            1               0          [-1, -1, -1, -1]              \n",
      "Ġbe             1               0          [-1, -1, -1, -1]              \n",
      "Ġconsidered     1               0          [-1, -1, -1, -1]              \n",
      "Ġobvious        1               0          [-1, -1, -1, -1]              \n",
      ".               1               0          [-1, -1, -1, -1]              \n",
      "Ġ               1               0          [-1, -1, -1, -1]              \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "(               1               0          [-1, -1, -1, -1]              \n",
      "ii              1               0          [-1, -1, -1, -1]              \n",
      ")               1               0          [-1, -1, -1, -1]              \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "It              1               0          [-1, -1, -1, -1]              \n",
      "Ġis             1               0          [-1, -1, -1, -1]              \n",
      "Ġnot            1               0          [-1, -1, -1, -1]              \n",
      "Ġunreasonable   1               0          [-1, -1, -1, -1]              \n",
      "Ġfor            1               0          [-1, -1, -1, -1]              \n",
      "Ġan             1               0          [-1, -1, -1, -1]              \n",
      "Ġentity         1               0          [-1, -1, -1, -1]              \n",
      "Ġto             1               0          [-1, -1, -1, -1]              \n",
      "Ġrely           1               0          [-1, -1, -1, -1]              \n",
      "Ġon             1               0          [-1, -1, -1, -1]              \n",
      "Ġan             1               0          [-1, -1, -1, -1]              \n",
      "Ġintroduction   1               0          [-1, -1, -1, -1]              \n",
      "Ġof             1               0          [-1, -1, -1, -1]              \n",
      "Ġan             1               0          [-1, -1, -1, -1]              \n",
      "Ġindividual     1               0          [-1, -1, -1, -1]              \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "from            1               0          [-1, -1, -1, -1]              \n",
      "Ġa              1               0          [-1, -1, -1, -1]              \n",
      "Ġwell           1               0          [-1, -1, -1, -1]              \n",
      "-               1               0          [-1, -1, -1, -1]              \n",
      "known           1               0          [-1, -1, -1, -1]              \n",
      "Ġcustomer       1               0          [-1, -1, -1, -1]              \n",
      "Ġor             1               0          [-1, -1, -1, -1]              \n",
      "Ġkey            1               0          [-1, -1, -1, -1]              \n",
      "Ġstaff          1               0          [-1, -1, -1, -1]              \n",
      ".               1               0          [-1, -1, -1, -1]              \n",
      "ĠIn             1               0          [-1, -1, -1, -1]              \n",
      "Ġthe            1               0          [-1, -1, -1, -1]              \n",
      "Ġcontext        1               0          [-1, -1, -1, -1]              \n",
      "Ġof             1               0          [-1, -1, -1, -1]              \n",
      "ĠGren           1               0          [-1, -1, -1, -1]              \n",
      "ada             1               0          [-1, -1, -1, -1]              \n",
      ",               1               0          [-1, -1, -1, -1]              \n",
      "Ġthis           1               0          [-1, -1, -1, -1]              \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "medium          1               0          [-1, -1, -1, -1]              \n",
      "Ġof             1               0          [-1, -1, -1, -1]              \n",
      "Ġintroduction   1               0          [-1, -1, -1, -1]              \n",
      "Ġshould         1               0          [-1, -1, -1, -1]              \n",
      "Ġexceptionally  1               0          [-1, -1, -1, -1]              \n",
      "Ġbe             1               0          [-1, -1, -1, -1]              \n",
      "Ġaccepted       1               0          [-1, -1, -1, -1]              \n",
      "Ġonly           1               0          [-1, -1, -1, -1]              \n",
      "Ġin             1               0          [-1, -1, -1, -1]              \n",
      "Ġrespect        1               0          [-1, -1, -1, -1]              \n",
      "Ġof             1               0          [-1, -1, -1, -1]              \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "individual      1               0          [-1, -1, -1, -1]              \n",
      "s               1               0          [-1, -1, -1, -1]              \n",
      "Ġwho            1               0          [-1, -1, -1, -1]              \n",
      "Ġare            1               0          [-1, -1, -1, -1]              \n",
      "Ġof             1               0          [-1, -1, -1, -1]              \n",
      "Ġold            1               0          [-1, -1, -1, -1]              \n",
      "Ġage            1               0          [-1, -1, -1, -1]              \n",
      "Ġ(              1               0          [-1, -1, -1, -1]              \n",
      "or              1               0          [-1, -1, -1, -1]              \n",
      "Ġretired        1               0          [-1, -1, -1, -1]              \n",
      ")               1               0          [-1, -1, -1, -1]              \n",
      "Ġand            1               0          [-1, -1, -1, -1]              \n",
      "Ġhave           1               0          [-1, -1, -1, -1]              \n",
      "Ġno             1               0          [-1, -1, -1, -1]              \n",
      "Ġform           1               0          [-1, -1, -1, -1]              \n",
      "Ġof             1               0          [-1, -1, -1, -1]              \n",
      "Ġidentification 1               0          [-1, -1, -1, -1]              \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "to              1               0          [-1, -1, -1, -1]              \n",
      "Ġenable         1               0          [-1, -1, -1, -1]              \n",
      "Ġan             1               0          [-1, -1, -1, -1]              \n",
      "Ġappropriate    1               0          [-1, -1, -1, -1]              \n",
      "Ġverification   1               0          [-1, -1, -1, -1]              \n",
      "Ġand            1               0          [-1, -1, -1, -1]              \n",
      "Ġthe            1               0          [-1, -1, -1, -1]              \n",
      "Ġbusiness       1               0          [-1, -1, -1, -1]              \n",
      "Ġrelationship   1               0          [-1, -1, -1, -1]              \n",
      "Ġsought         1               0          [-1, -1, -1, -1]              \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "does            1               0          [-1, -1, -1, -1]              \n",
      "Ġnot            1               0          [-1, -1, -1, -1]              \n",
      "Ġinvolve        1               0          [-1, -1, -1, -1]              \n",
      "Ġsignificant    1               0          [-1, -1, -1, -1]              \n",
      "Ġamounts        1               0          [-1, -1, -1, -1]              \n",
      "Ġof             1               0          [-1, -1, -1, -1]              \n",
      "Ġmoney          1               0          [-1, -1, -1, -1]              \n",
      "Ġor             1               0          [-1, -1, -1, -1]              \n",
      "Ġother          1               0          [-1, -1, -1, -1]              \n",
      "Ġproperty       1               0          [-1, -1, -1, -1]              \n",
      "Ġwhose          1               0          [-1, -1, -1, -1]              \n",
      "Ġvalue          1               0          [-1, -1, -1, -1]              \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "is              1               0          [-1, -1, -1, -1]              \n",
      "Ġnot            1               0          [-1, -1, -1, -1]              \n",
      "Ġsignificant    1               0          [-1, -1, -1, -1]              \n",
      "Ġin             1               0          [-1, -1, -1, -1]              \n",
      "Ġmonetary       1               0          [-1, -1, -1, -1]              \n",
      "Ġterms          1               0          [-1, -1, -1, -1]              \n",
      ".               1               0          [-1, -1, -1, -1]              \n",
      "ĠHowever        1               0          [-1, -1, -1, -1]              \n",
      ",               1               0          [-1, -1, -1, -1]              \n",
      "Ġreliance       1               0          [-1, -1, -1, -1]              \n",
      "Ġon             1               0          [-1, -1, -1, -1]              \n",
      "Ġa              1               0          [-1, -1, -1, -1]              \n",
      "Ġpersonal       1               0          [-1, -1, -1, -1]              \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "introdu         1               0          [-1, -1, -1, -1]              \n",
      "ction           1               0          [-1, -1, -1, -1]              \n",
      "Ġmust           1               0          [-1, -1, -1, -1]              \n",
      "Ġbe             1               0          [-1, -1, -1, -1]              \n",
      "Ġaccent         1               0          [-1, -1, -1, -1]              \n",
      "uated           1               0          [-1, -1, -1, -1]              \n",
      "Ġwith           1               0          [-1, -1, -1, -1]              \n",
      "Ġthe            1               0          [-1, -1, -1, -1]              \n",
      "Ġconditions     1               0          [-1, -1, -1, -1]              \n",
      "Ġstip           1               0          [-1, -1, -1, -1]              \n",
      "ulated          1               0          [-1, -1, -1, -1]              \n",
      "Ġin             1               0          [-1, -1, -1, -1]              \n",
      "Ġsection        1               0          [-1, -1, -1, -1]              \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "26              1               0          [-1, -1, -1, -1]              \n",
      "(               1               0          [-1, -1, -1, -1]              \n",
      "2               1               0          [-1, -1, -1, -1]              \n",
      ")               1               0          [-1, -1, -1, -1]              \n",
      "Ġand            1               0          [-1, -1, -1, -1]              \n",
      "Ġ(              1               0          [-1, -1, -1, -1]              \n",
      "5               1               0          [-1, -1, -1, -1]              \n",
      ")               1               0          [-1, -1, -1, -1]              \n",
      "Ġof             1               0          [-1, -1, -1, -1]              \n",
      "Ġthese          1               0          [-1, -1, -1, -1]              \n",
      "ĠGuidelines     1               0          [-1, -1, -1, -1]              \n",
      ";               1               0          [-1, -1, -1, -1]              \n",
      "Ġthe            1               0          [-1, -1, -1, -1]              \n",
      "Ġinformation    1               0          [-1, -1, -1, -1]              \n",
      "Ġtherein        1               0          [-1, -1, -1, -1]              \n",
      "Ġoutlined       1               0          [-1, -1, -1, -1]              \n",
      "Ġmust           1               0          [-1, -1, -1, -1]              \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "(               1               0          [-1, -1, -1, -1]              \n",
      "where           1               0          [-1, -1, -1, -1]              \n",
      "Ġavailable      1               0          [-1, -1, -1, -1]              \n",
      ")               1               0          [-1, -1, -1, -1]              \n",
      "Ġbe             1               0          [-1, -1, -1, -1]              \n",
      "Ġprovided       1               0          [-1, -1, -1, -1]              \n",
      ".               1               0          [-1, -1, -1, -1]              \n",
      "ĠWhere          1               0          [-1, -1, -1, -1]              \n",
      "Ġthe            1               0          [-1, -1, -1, -1]              \n",
      "Ġindividual     1               0          [-1, -1, -1, -1]              \n",
      "Ġholds          1               0          [-1, -1, -1, -1]              \n",
      "Ġmore           1               0          [-1, -1, -1, -1]              \n",
      "Ġthan           1               0          [-1, -1, -1, -1]              \n",
      "Ġone            1               0          [-1, -1, -1, -1]              \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "national        1               0          [-1, -1, -1, -1]              \n",
      "ity             1               0          [-1, -1, -1, -1]              \n",
      ",               1               0          [-1, -1, -1, -1]              \n",
      "Ġall            1               0          [-1, -1, -1, -1]              \n",
      "Ġof             1               0          [-1, -1, -1, -1]              \n",
      "Ġthe            1               0          [-1, -1, -1, -1]              \n",
      "Ġnational       1               0          [-1, -1, -1, -1]              \n",
      "ities           1               0          [-1, -1, -1, -1]              \n",
      "Ġhe             1               0          [-1, -1, -1, -1]              \n",
      "Ġholds          1               0          [-1, -1, -1, -1]              \n",
      "Ġmust           1               0          [-1, -1, -1, -1]              \n",
      "Ġbe             1               0          [-1, -1, -1, -1]              \n",
      "Ġprovided       1               0          [-1, -1, -1, -1]              \n",
      "Ġand            1               0          [-1, -1, -1, -1]              \n",
      "Ġrecorded       1               0          [-1, -1, -1, -1]              \n",
      ".               1               0          [-1, -1, -1, -1]              \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "It              1               0          [-1, -1, -1, -1]              \n",
      "Ġis             1               0          [-1, -1, -1, -1]              \n",
      "Ġimportant      1               0          [-1, -1, -1, -1]              \n",
      "Ġto             1               0          [-1, -1, -1, -1]              \n",
      "Ġtake           1               0          [-1, -1, -1, -1]              \n",
      "Ġstock          1               0          [-1, -1, -1, -1]              \n",
      "Ġof             1               0          [-1, -1, -1, -1]              \n",
      "Ġthe            1               0          [-1, -1, -1, -1]              \n",
      "Ġsource         1               0          [-1, -1, -1, -1]              \n",
      "Ġof             1               0          [-1, -1, -1, -1]              \n",
      "Ġany            1               0          [-1, -1, -1, -1]              \n",
      "Ġdocumentary    1               0          [-1, -1, -1, -1]              \n",
      "Ġevidence       1               0          [-1, -1, -1, -1]              \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "present         1               0          [-1, -1, -1, -1]              \n",
      "ed              1               0          [-1, -1, -1, -1]              \n",
      "Ġto             1               0          [-1, -1, -1, -1]              \n",
      "Ġestablish      1               0          [-1, -1, -1, -1]              \n",
      "Ġa              1               0          [-1, -1, -1, -1]              \n",
      "Ġbusiness       1               0          [-1, -1, -1, -1]              \n",
      "Ġrelationship   1               0          [-1, -1, -1, -1]              \n",
      ".               1               0          [-1, -1, -1, -1]              \n",
      "ĠWhere          1               0          [-1, -1, -1, -1]              \n",
      "Ġsuch           1               0          [-1, -1, -1, -1]              \n",
      "Ġevidence       1               0          [-1, -1, -1, -1]              \n",
      "Ġon             1               0          [-1, -1, -1, -1]              \n",
      "Ġthe            1               0          [-1, -1, -1, -1]              \n",
      "Ċ               1               0          [-1, -1, -1, -1]              \n",
      "face            1               0          [-1, -1, -1, -1]              \n",
      "Ġof             1               0          [-1, -1, -1, -1]              \n",
      "Ġit             1               0          [-1, -1, -1, -1]              \n",
      "Ġeman           1               0          [-1, -1, -1, -1]              \n",
      "ates            1               0          [-1, -1, -1, -1]              \n",
      "Ġfrom           1               0          [-1, -1, -1, -1]              \n",
      "Ġa              1               0          [-1, -1, -1, -1]              \n",
      "Ġgovernment     1               0          [-1, -1, -1, -1]              \n",
      "Ġor             1               0          [-1, -1, -1, -1]              \n",
      "Ġlocal          1               0          [-1, -1, -1, -1]              \n",
      "</s>            1               -100       [-1, -1, -1, -1]              \n"
     ]
    }
   ],
   "source": [
    "# We keep one example from the dataset here\n",
    "doc_id = '18753674'\n",
    "page_index = '71'\n",
    "\n",
    "target_index = train_dataset_index_map.get((doc_id, page_index)) # use above mapping for fast accesses\n",
    "if target_index is not None:\n",
    "    sample = dataset_dict['train'][target_index]\n",
    "    input_ids = tokenizer.convert_ids_to_tokens(sample['input_ids'])\n",
    "    attention_mask = sample['attention_mask']\n",
    "    bio_labels = sample['bio_labels']\n",
    "    detailed_labels = sample['detailed_labels']\n",
    "\n",
    "    # Print a header\n",
    "    print(f\"{'Token':<15} {'Attention Mask':<15} {'BIO Label':<10} {'Detailed Labels':<30}\")\n",
    "    print('-' * 70)\n",
    "\n",
    "    # Print each token and its corresponding details\n",
    "    for token, mask, bio, detail in zip(input_ids, attention_mask, bio_labels, detailed_labels):\n",
    "        detail_str = str(detail) if isinstance(detail, list) else detail\n",
    "        print(f\"{token:<15} {mask:<15} {bio:<10} {detail_str:<30}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def token_classification_collate_fn(self, batch):\n",
    "        '''\n",
    "        Custom collate function for token-level model.\n",
    "        Ensure proper/uniform padding for all elements in the batch.\n",
    "        '''\n",
    "        \n",
    "        # MAIN OBJECTIVE\n",
    "        # pad input_ids, attention_mask and bio_labels (i.e. main objective labels)\n",
    "        input_ids = pad_sequence([torch.tensor(sample['input_ids'], dtype=torch.long) for sample in batch], batch_first=True, padding_value=self.tokenizer.convert_tokens_to_ids(self.tokenizer.pad_token if hasattr(self.tokenizer, 'pad_token') else '[PAD]')) # if possible, we ensure that the padding value is the same as the one used by the tokenizer\n",
    "        attention_mask = pad_sequence([torch.tensor(sample['attention_mask'], dtype=torch.long) for sample in batch], batch_first=True, padding_value=0) # attention mask consists of 0s and 1s and is thefore padded with 0s\n",
    "        bio_labels_padded = pad_sequence([torch.tensor(sample['bio_labels'], dtype=torch.long) for sample in batch], batch_first=True, padding_value=-100)  # -100 is used as the padding value for the main objective (BIO(E) labels)\n",
    "\n",
    "        # put batch elements together in a dictionary\n",
    "        batch_dict = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"bio_labels\": bio_labels_padded}\n",
    "        \n",
    "        # AUXILIARY OBJECTIVE\n",
    "        # pad detailed_labels if available\n",
    "        if self.HIER_LABELS_LEVELS:\n",
    "            batch_detailed_labels = []\n",
    "            NUMBER_OF_LEVELS = len(self.HIER_LABELS_LEVELS)\n",
    "            \n",
    "            for sample in batch:\n",
    "                detailed_labels = sample.get('detailed_labels', [])\n",
    "\n",
    "                # filtered detailed labels for page\n",
    "                filtered_token_labels_list = []\n",
    "\n",
    "                for token_labels in detailed_labels:\n",
    "                    # Filter the token's labels based on HIER_LABELS_LEVELS\n",
    "                    # This step is necessary as the dataset contains all levels of labels, but the model only needs the specified levels\n",
    "                    filtered_token_labels = [token_labels[idx] for idx in self.HIER_LABELS_LEVELS if idx < len(token_labels)]\n",
    "                    filtered_token_labels_list.append(filtered_token_labels)\n",
    "                \n",
    "                # Pad detailed labels to ensure uniform length\n",
    "                token_detailed_labels_padded = [labels + [-1] * (NUMBER_OF_LEVELS - len(labels)) for labels in filtered_token_labels_list]\n",
    "\n",
    "                # Convert to tensor\n",
    "                if token_detailed_labels_padded:\n",
    "                    detailed_labels_tensor = torch.tensor(token_detailed_labels_padded, dtype=torch.long) \n",
    "                else:\n",
    "                    # Create placeholder tensor if there are no detailed labels\n",
    "                    detailed_labels_tensor = torch.full((1, NUMBER_OF_LEVELS), -1, dtype=torch.long)\n",
    "                \n",
    "                batch_detailed_labels.append(detailed_labels_tensor)\n",
    "            \n",
    "            detailed_labels_padded_uniform = pad_sequence(batch_detailed_labels, batch_first=True, padding_value=-1) # using -1 as padding value for detailed labels for consistency\n",
    "\n",
    "            # Add to bacth dict\n",
    "            batch_dict[\"detailed_labels\"] = detailed_labels_padded_uniform\n",
    "\n",
    "        return batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import evaluator\n",
    "importlib.reload(evaluator)\n",
    "\n",
    "\n",
    "# Definition of custom trainer and training arguments here (same as in script, but needed for working with models)\n",
    "class CustomTokenTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        bio_labels = inputs.pop(\"bio_labels\", None)\n",
    "        detailed_labels = inputs.pop(\"detailed_labels\", None)\n",
    "        outputs = model(**inputs, bio_labels=bio_labels, detailed_labels=detailed_labels)\n",
    "        loss = outputs[0]\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'{DATA_PATH}/Model/results/{unique_tag}',\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f'{DATA_PATH}/Model/logs',\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_steps=500, \n",
    "    save_total_limit=5,\n",
    "    load_best_model_at_end=True, \n",
    "    metric_for_best_model='loss',\n",
    "    greater_is_better=False, \n",
    "    report_to=\"wandb\",\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from token_level_model import TokenClassificationWithDetailedLabels\n",
    "\n",
    "# If pre-trained model is defined, load it from disk\n",
    "if PRETRAINED_MODEL:\n",
    "    model_path = f\"{DATA_PATH}/Model/results/saved_model/{PRETRAINED_MODEL}/torch_model.pth\"\n",
    "    config_path = os.path.join(os.path.dirname(model_path), \"config.json\")\n",
    "\n",
    "    # Load configuration\n",
    "    with open(config_path, 'r') as config_file:\n",
    "        loaded_config = json.load(config_file)\n",
    "\n",
    "    # Recreate model architecture based on loaded configuration\n",
    "    if DEFAULT_MODEL:\n",
    "        token_model = AutoModelForTokenClassification.from_pretrained(\n",
    "            MODEL,\n",
    "            num_labels=NUMBER_BIO_LABELS\n",
    "        )\n",
    "    else:\n",
    "        token_model = TokenClassificationWithDetailedLabels(\n",
    "            model_name_or_path=MODEL,\n",
    "            num_labels=loaded_config[\"num_labels\"],\n",
    "            num_detailed_labels_per_level=loaded_config[\"num_detailed_labels_per_level\"],\n",
    "            detailed_label_weights=loaded_config[\"detailed_label_weights\"],\n",
    "            alpha=loaded_config[\"alpha\"]\n",
    "        )\n",
    "\n",
    "    # Load weights\n",
    "    token_model.load_state_dict(torch.load(model_path))\n",
    "    token_model.eval()\n",
    "    token_model.to(\"cuda\")\n",
    "    print(f\"{loaded_config['model_architecture']} model successfully loaded.\")\n",
    "    print(f\"File path used: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate detailed labels predictions on test set using trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='123' max='123' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [123/123 01:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtlh45\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tlh45/project/code/Model/wandb/run-20240516_144541-y7i62qj1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tlh45/huggingface/runs/y7i62qj1' target=\"_blank\">deep-shadow-12</a></strong> to <a href='https://wandb.ai/tlh45/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tlh45/huggingface' target=\"_blank\">https://wandb.ai/tlh45/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tlh45/huggingface/runs/y7i62qj1' target=\"_blank\">https://wandb.ai/tlh45/huggingface/runs/y7i62qj1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Evaluation Results:\n",
      "eval_loss: 0.8003551363945007\n",
      "eval_normal_labels_metrics: {'cross_entropy_loss': 0.41854533553123474, 'precision': 0.7450200849816477, 'recall': 0.9255944872151243, 'f1': 0.8255482682840314, 'accuracy': 0.8858901939988688, 'cohen_kappa': 0.7401486005630533, 'model_pk': 0.006727384516473018, 'model_pk_value_segeval': 0.10541919016688624, 'model_windowdiff': 0.6402827258188656, 'model_windowdiff_value_segeval': 0.15752762437901768, 'random_baseline_accuracy': 0.5002875396186099, 'random_baseline_precision': 0.2920038226533108, 'random_baseline_recall': 0.5005517425931951, 'random_baseline_f1': 0.3688398105627545, 'random_baseline_pk': 0.375576440068596, 'random_pk_value_segeval': 0.4999439034742256, 'random_baseline_windowdiff': 63.23811077902266, 'random_windowdiff_value_segeval': 0.4999439034742256}\n",
      "eval_hierarchical_labels_metrics: {'level_1': {'cross_entropy_loss': 1.4253383874893188, 'precision': 0.5951899009837123, 'recall': 0.5891009238091965, 'f1': 0.5831115955409264, 'accuracy': 0.6878188566116044, 'cohen_kappa': 0.6573163077173356}, 'level_2': {'cross_entropy_loss': 2.106858253479004, 'precision': 0.35524099903525974, 'recall': 0.35379738511008846, 'f1': 0.3276761857894182, 'accuracy': 0.5437424597977005, 'cohen_kappa': 0.5190937839586733}}\n",
      "eval_runtime: 1034.9177\n",
      "eval_samples_per_second: 15.091\n",
      "eval_steps_per_second: 0.119\n"
     ]
    }
   ],
   "source": [
    "trainer = CustomTokenTrainer(\n",
    "            model=token_model, \n",
    "            args=training_args,\n",
    "            train_dataset=dataset_dict['train'],\n",
    "            eval_dataset=dataset_dict['validation'],\n",
    "            data_collator=token_classification_collate_fn,\n",
    "            compute_metrics=evaluator.compute_metrics_wrapper(index_to_label, HIER_LABELS_LEVELS, mapping_dicts, calc_windowdiff=True),\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=EARLY_STOPPING_PATIENCE)]\n",
    "        )\n",
    "\n",
    "\n",
    "test_results = trainer.evaluate(dataset_dict[\"test\"])\n",
    "\n",
    "print(\"Test Set Evaluation Results:\")\n",
    "for key, value in test_results.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse consecuitve tokens in a confusion matrix (Section 6.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49eb14e4bf24459785b95c9dfcabcc76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating dataset:   0%|          | 0/15618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37df5916f3194dd3a1fa1e7466d4f21f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Making Predictions:   0%|          | 0/31 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Create TensorDataset for efficient loading\n",
    "def create_dataset(dataset):\n",
    "    # List to hold data\n",
    "    input_ids_list = []\n",
    "    attention_masks_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for item in tqdm(dataset, desc='Creating dataset'):\n",
    "        input_ids_list.append(item['input_ids'])\n",
    "        attention_masks_list.append(item['attention_mask'])\n",
    "        labels_list.append(item['bio_labels'])\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids_list)\n",
    "    attention_masks = torch.tensor(attention_masks_list)\n",
    "    labels = torch.tensor(labels_list)\n",
    "    \n",
    "    return TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# DataLoader for batch processing\n",
    "def create_data_loader(dataset, batch_size=512):\n",
    "    return DataLoader(create_dataset(dataset), batch_size=batch_size)\n",
    "\n",
    "def get_consecutive_label_pairs(model, data_loader, label_map, device='cuda'):\n",
    "    all_true_label_pairs = []\n",
    "    all_pred_label_pairs = []\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(data_loader, total=len(data_loader), desc=\"Making Predictions\", unit=\"batch\"):\n",
    "        \n",
    "        input_ids, attention_mask, true_labels = tuple(t.to(device) for t in batch) # move to device\n",
    "        \n",
    "        # Make predictions using loaded model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs[0]\n",
    "        predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "        for i in range(input_ids.size(0)):  # iterate over batch\n",
    "            true_label_seq = true_labels[i].cpu().numpy()\n",
    "            pred_label_seq = predictions[i].cpu().numpy()\n",
    "            \n",
    "            # Remove padding and special tokens\n",
    "            mask = true_label_seq != -100\n",
    "            true_label_seq = true_label_seq[mask]\n",
    "            pred_label_seq = pred_label_seq[mask]\n",
    "            \n",
    "            # Get pairs by zipping the sequence with itself offset by one\n",
    "            true_label_pairs = list(zip(true_label_seq[:-1], true_label_seq[1:]))\n",
    "            pred_label_pairs = list(zip(pred_label_seq[:-1], pred_label_seq[1:]))\n",
    "            \n",
    "            # Convert to labels and store\n",
    "            true_label_pairs = [(label_map[i], label_map[j]) for i, j in true_label_pairs]\n",
    "            pred_label_pairs = [(label_map[i], label_map[j]) for i, j in pred_label_pairs]\n",
    "            \n",
    "            # Write to lsts\n",
    "            all_true_label_pairs.extend(true_label_pairs)\n",
    "            all_pred_label_pairs.extend(pred_label_pairs)\n",
    "    \n",
    "    return all_true_label_pairs, all_pred_label_pairs\n",
    "\n",
    "# Get true and predicted label pairs\n",
    "test_data_loader = create_data_loader(dataset_dict['test'])\n",
    "true_label_pairs, pred_label_pairs = get_consecutive_label_pairs(token_model, test_data_loader, index_to_label, device='cuda')\n",
    "\n",
    "# Write both lists to files\n",
    "save_path = \"Evaluation/token-level-model-CM-final-\"\n",
    "if save_path:\n",
    "        with open(save_path + 'true_label_pairs.pkl', 'wb') as f:\n",
    "            pickle.dump(true_label_pairs, f)\n",
    "        with open(save_path + 'pred_label_pairs.pkl', 'wb') as f:\n",
    "            pickle.dump(pred_label_pairs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lists with label pairs from files (see above)\n",
    "load_path = \"Evaluation/token-level-model-CM-final-\"\n",
    "if load_path:\n",
    "    with open(load_path + 'true_label_pairs.pkl', 'rb') as f:\n",
    "        true_label_pairs = pickle.load(f)\n",
    "    with open(load_path + 'pred_label_pairs.pkl', 'rb') as f:\n",
    "        pred_label_pairs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[      0       0       0       0       2       0       0       0       0\n",
      "        0       0       0       0       0       0]\n",
      " [      6    2925     101       4    2589       2      31       0       8\n",
      "        0      71      23     172       0    1327]\n",
      " [      0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0]\n",
      " [      0       2       0      26     535      61       4       0       0\n",
      "        0       2      59      11       2      60]\n",
      " [      1    1368      36     289 1805881     844    4918       0     189\n",
      "        0     637    1057    5205       9  181068]\n",
      " [      0       2       0       1    2022    3677     142       0       1\n",
      "        8      18       0      62      62     812]\n",
      " [      0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0]\n",
      " [      0       1       0       0      28       0       2       0       1\n",
      "        0      19       1       0       0       4]\n",
      " [      0       0       0       0      56       0       7       0       3\n",
      "        0      40       0       0       0       4]\n",
      " [      0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0]\n",
      " [      0       0       0       0    1433      27     527       0      64\n",
      "        0    3621       0      24       0     936]\n",
      " [      2      25       1     143    1673      16      19       0       0\n",
      "        0       7    2778     259       0    1383]\n",
      " [      0       0       0       0       0       0       0       0       0\n",
      "        0       0       0       0       0       0]\n",
      " [      0       0       0       0       2       0       0       0       0\n",
      "        0       0       0       0       0       0]\n",
      " [      1    1035     149      93  505792    1086    5485       0      51\n",
      "        5    1126    1097    5738      70 4376607]]\n",
      "Labels:\n",
      " ['B-B', 'B-I', 'B-O', 'I-B', 'I-I', 'I-E', 'I-O', 'E-B', 'E-I', 'E-E', 'E-O', 'O-B', 'O-I', 'O-E', 'O-O']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Flatten list of pairs\n",
    "flat_true_label_pairs = [f\"{i}-{j}\" for i, j in true_label_pairs]\n",
    "flat_pred_label_pairs = [f\"{i}-{j}\" for i, j in pred_label_pairs]\n",
    "\n",
    "# Define labels and their order for cm\n",
    "ordered_labels = [\n",
    "    'B-B', 'B-I', 'B-O',\n",
    "    'I-B', 'I-I', 'I-E', 'I-O',\n",
    "    'E-B', 'E-I', 'E-E', 'E-O',\n",
    "    'O-B', 'O-I', 'O-E', 'O-O'\n",
    "]\n",
    "\n",
    "# Create confusion matrix with reordered labels\n",
    "cm = confusion_matrix(flat_true_label_pairs, flat_pred_label_pairs, labels=ordered_labels, normalize=None) #'true') (unnormalised for report)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Labels:\\n\", ordered_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{table}[htbp]\n",
      "\\centering\n",
      "\\footnotesize\n",
      "\\caption{Confusion matrix of consecutively predicted labels. Values are highlighted in a row-wise normalised manner to account for the significant class imbalance.}\n",
      "\\label{tab:confusion_matrix} \n",
      "\\setlength{\\tabcolsep}{4.5pt} % reduce column spacing\n",
      "\\begin{tabular}{@{}lccccccccccccccc@{}}\n",
      "% \\toprule\n",
      "& \\multicolumn{15}{c}{\\textbf{Predictions}} \\\\\n",
      "\\cmidrule(lr){2-16}\n",
      "\\textbf{Gold} & B-B & B-I & B-O & I-B & I-I & I-E & I-O & E-B & E-I & E-E & E-O & O-B & O-I & O-E & O-O \\\\\n",
      "\\midrule\n",
      "B-B & \\textbf{\\cellcolor[RGB]{255,255,255}0} & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{127,255,127}2 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 \\\\\n",
      "B-I & \\cellcolor[RGB]{255,255,255}6 & \\textbf{\\cellcolor[RGB]{204,255,204}2.9k} & \\cellcolor[RGB]{254,255,254}101 & \\cellcolor[RGB]{255,255,255}4 & \\cellcolor[RGB]{210,255,210}2.6k & \\cellcolor[RGB]{255,255,255}2 & \\cellcolor[RGB]{255,255,255}31 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}8 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{254,255,254}71 & \\cellcolor[RGB]{255,255,255}23 & \\cellcolor[RGB]{252,255,252}172 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{232,255,232}1.3k \\\\\n",
      "B-O & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\textbf{\\cellcolor[RGB]{255,255,255}0} & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 \\\\\n",
      "I-B & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}2 & \\cellcolor[RGB]{255,255,255}0 & \\textbf{\\cellcolor[RGB]{251,255,251}26} & \\cellcolor[RGB]{166,255,166}535 & \\cellcolor[RGB]{245,255,245}61 & \\cellcolor[RGB]{255,255,255}4 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}2 & \\cellcolor[RGB]{246,255,246}59 & \\cellcolor[RGB]{254,255,254}11 & \\cellcolor[RGB]{255,255,255}2 & \\cellcolor[RGB]{245,255,245}60 \\\\\n",
      "I-I & \\cellcolor[RGB]{255,255,255}1 & \\cellcolor[RGB]{255,255,255}1.4k & \\cellcolor[RGB]{255,255,255}36 & \\cellcolor[RGB]{255,255,255}289 & \\textbf{\\cellcolor[RGB]{140,255,140}1.8m} & \\cellcolor[RGB]{255,255,255}844 & \\cellcolor[RGB]{255,255,255}4.9k & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}189 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}637 & \\cellcolor[RGB]{255,255,255}1.1k & \\cellcolor[RGB]{255,255,255}5.2k & \\cellcolor[RGB]{255,255,255}9 & \\cellcolor[RGB]{244,255,244}181.1k \\\\\n",
      "I-E & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}2 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}1 & \\cellcolor[RGB]{217,255,217}2.0k & \\textbf{\\cellcolor[RGB]{186,255,186}3.7k} & \\cellcolor[RGB]{253,255,253}142 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}1 & \\cellcolor[RGB]{255,255,255}8 & \\cellcolor[RGB]{255,255,255}18 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{254,255,254}62 & \\cellcolor[RGB]{254,255,254}62 & \\cellcolor[RGB]{240,255,240}812 \\\\\n",
      "I-O & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\textbf{\\cellcolor[RGB]{255,255,255}0} & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 \\\\\n",
      "E-B & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{253,255,253}1 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{191,255,191}28 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{251,255,251}2 & \\textbf{\\cellcolor[RGB]{255,255,255}0} & \\cellcolor[RGB]{253,255,253}1 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{212,255,212}19 & \\cellcolor[RGB]{253,255,253}1 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{246,255,246}4 \\\\\n",
      "E-I & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{190,255,190}56 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{247,255,247}7 & \\cellcolor[RGB]{255,255,255}0 & \\textbf{\\cellcolor[RGB]{252,255,252}3} & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{209,255,209}40 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{251,255,251}4 \\\\\n",
      "E-E & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\textbf{\\cellcolor[RGB]{255,255,255}0} & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 \\\\\n",
      "E-O & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{228,255,228}1.4k & \\cellcolor[RGB]{255,255,255}27 & \\cellcolor[RGB]{245,255,245}527 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{254,255,254}64 & \\cellcolor[RGB]{255,255,255}0 & \\textbf{\\cellcolor[RGB]{186,255,186}3.6k} & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}24 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{237,255,237}936 \\\\\n",
      "O-B & \\cellcolor[RGB]{255,255,255}2 & \\cellcolor[RGB]{255,255,255}25 & \\cellcolor[RGB]{255,255,255}1 & \\cellcolor[RGB]{253,255,253}143 & \\cellcolor[RGB]{222,255,222}1.7k & \\cellcolor[RGB]{255,255,255}16 & \\cellcolor[RGB]{255,255,255}19 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}7 & \\textbf{\\cellcolor[RGB]{199,255,199}2.8k} & \\cellcolor[RGB]{250,255,250}259 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{227,255,227}1.4k \\\\\n",
      "O-I & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\textbf{\\cellcolor[RGB]{255,255,255}0} & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 \\\\\n",
      "O-E & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{127,255,127}2 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}0 & \\textbf{\\cellcolor[RGB]{255,255,255}0} & \\cellcolor[RGB]{255,255,255}0 \\\\\n",
      "O-O & \\cellcolor[RGB]{255,255,255}1 & \\cellcolor[RGB]{255,255,255}1.0k & \\cellcolor[RGB]{255,255,255}149 & \\cellcolor[RGB]{255,255,255}93 & \\cellcolor[RGB]{242,255,242}505.8k & \\cellcolor[RGB]{255,255,255}1.1k & \\cellcolor[RGB]{255,255,255}5.5k & \\cellcolor[RGB]{255,255,255}0 & \\cellcolor[RGB]{255,255,255}51 & \\cellcolor[RGB]{255,255,255}5 & \\cellcolor[RGB]{255,255,255}1.1k & \\cellcolor[RGB]{255,255,255}1.1k & \\cellcolor[RGB]{255,255,255}5.7k & \\cellcolor[RGB]{255,255,255}70 & \\textbf{\\cellcolor[RGB]{141,255,141}4.4m} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\end{table}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3132492/989701291.py:19: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_cm_formatted = df_cm.applymap(format_number)\n"
     ]
    }
   ],
   "source": [
    "def format_number(x):\n",
    "    '''\n",
    "    Helper function to format numbers for the confusion matrix to make output more readable\n",
    "    '''\n",
    "    if x >= 1_000_000:\n",
    "        return f\"{x/1_000_000:.1f}m\"\n",
    "    elif x >= 1_000:\n",
    "        return f\"{x/1_000:.1f}k\"\n",
    "    else:\n",
    "        return f\"{x:,}\"\n",
    "\n",
    "# Normalise cm by row\n",
    "row_sums = cm.sum(axis=1, keepdims=True)\n",
    "cm_normalised = np.divide(cm, row_sums, where=row_sums!=0)\n",
    "cm_normalised = np.nan_to_num(cm_normalised)  # Replace NaN with 0\n",
    "\n",
    "# Create DataFrame\n",
    "df_cm = pd.DataFrame(cm, index=ordered_labels, columns=ordered_labels)\n",
    "df_cm_normalized = pd.DataFrame(cm_normalised, index=ordered_labels, columns=ordered_labels)\n",
    "\n",
    "# Format numbers in df\n",
    "df_cm_formatted = df_cm.applymap(format_number)\n",
    "\n",
    "# Heatmap coloring with shades of green\n",
    "def color_cell(value, max_value):\n",
    "    normalized_value = value / max_value\n",
    "    red_intensity = 255 - int(normalized_value * 128)\n",
    "    return f'\\\\cellcolor[RGB]{{{red_intensity},255,{red_intensity}}}'\n",
    "\n",
    "max_value = df_cm_normalized.values.max()\n",
    "for i in range(len(ordered_labels)):\n",
    "    for j in range(len(ordered_labels)):\n",
    "        color = color_cell(df_cm_normalized.iloc[i, j], max_value)\n",
    "        df_cm_formatted.iloc[i, j] = color + df_cm_formatted.iloc[i, j]\n",
    "\n",
    "# Make diagonal elements bold\n",
    "for label in ordered_labels:\n",
    "    df_cm_formatted.at[label, label] = '\\\\textbf{' + df_cm_formatted.at[label, label] + '}'\n",
    "\n",
    "# Convert to LaTeX code\n",
    "latex_code = df_cm_formatted.to_latex(escape=False)\n",
    "\n",
    "latex_preamble = fr'''\n",
    "\\begin{{table}}[htbp]\n",
    "\\centering\n",
    "\\footnotesize\n",
    "\\caption{{Confusion matrix of consecutively predicted labels. Values are highlighted in a row-wise normalised manner to account for the significant class imbalance.}}\n",
    "\\label{{tab:confusion_matrix}}\n",
    "\\begin{{tabular}}{{@{{}}l{\"c\"*len(ordered_labels)}@{{}}}}\n",
    "% \\toprule\n",
    "& \\multicolumn{{{len(ordered_labels)}}}{{c}}{{\\textbf{{Predictions}}}} \\\\\n",
    "\\cmidrule(lr){{2-{len(ordered_labels)+1}}}\n",
    "\\textbf{{Gold}}\n",
    "'''.rstrip()\n",
    "\n",
    "latex_postamble = r'''\n",
    "\\end{table}\n",
    "'''\n",
    "\n",
    "# Merge latex code together\n",
    "latex_code = latex_preamble + latex_code.split('\\n', 2)[-1] + latex_postamble\n",
    "latex_code = latex_code.replace('\\label{tab:confusion_matrix}', '\\\\label{tab:confusion_matrix} \\n\\\\setlength{\\\\tabcolsep}{4.5pt} % reduce column spacing')\n",
    "\n",
    "print(latex_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to predict tags for a page from the data_dict\n",
    "(data_dict not available here due to confidentiality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 \n",
      "CONSULTATION PAPER ON DRAFT RTS UNDER ARTICLE 45(6) OF DIRECTIVE (EU) 2015/849 \n",
      " \n",
      " \n",
      " \n",
      " C. Baseline scenario \n",
      " \n",
      "In the baseline scenario, Directive (EU) 2015/849 would be transposed without accompanying \n",
      "draft RTS under Article 45(6). This means that Member States and credit and financial institutions \n",
      "may adopt divergent views about the way credit and financial institutions should address the risk \n",
      "associated with business in  third  countries where the implementation of local law does not \n",
      "permit the application of group-wide policies and procedures. \n",
      " \n",
      " D. Options considered \n",
      " \n",
      "Option 1: The draft RTS could require credit and financial institutions to close down all \n",
      "relationships and withdraw entirely from business in the third country. \n",
      " \n",
      "Option 2: The draft RTS could set out minimum  actions and additional measures credit and \n",
      "financial institutions have to apply in all cases, irrespective of the risk and the type of legal \n",
      "impediment. \n",
      " \n",
      "Option 3: The draft RTS could distinguish between different situations where the implementation \n",
      "of a third country’s law does not permit the application of group-wide policies and procedures. \n",
      " \n",
      " E. Preferred option \n",
      " \n",
      "The advantage of Option 1 is that this would result in a harmonised approach. \n",
      " \n",
      "The disadvantage of Option 1 is that this approach is unlikely to be proportionate or \n",
      "commensurate to the ML/TF risk associated with doing business in those third countries, as in \n",
      "many cases, alternative solutions can be found to manage those risks effectively. \n",
      " \n",
      "The advantage of Option2 would be a harmonised approach and a level playing field across the \n",
      "Union’s financial sector. \n",
      " \n",
      "The disadvantage of Option2 would be that requiring credit and financial institutions to apply the \n",
      "same measures in all cases is unlikely to be proportionate or effective, as measures are not \n",
      "targeted to address specific risks. \n",
      " \n",
      "The advantage of Option 3 is that by identifying different legal impediments, it is possible to \n",
      "propose targeted measures to address the resultant risk. By providing targeted minimum actions \n",
      "and a set of targeted, additional measures that can be adjusted on a risk-sensitive basis, credit \n",
      "and financial institutions’ approaches to managing risk will be more effective and proportionate. \n",
      " \n",
      "The disadvantage of Option 3 is that by taking a differentiated approach, the draft RTS may \n",
      "appear more complex, and may lead to a greater variety of private sector practices than other \n",
      "approaches, as the greater emphasis on the risk-based approach means that not everyone will \n",
      "come to the same view. \n",
      " \n",
      "The ESAs’ preferred option is Option 3 because in spite of appearing more complex than Options \n",
      "1 and 2, it is both risk-based and proportionate and most likely to lead to effective outcomes. \n",
      "\n",
      "Predicted tags: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I', 'O', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'E', 'O', 'O', 'O', 'O', 'O', 'I', 'O', 'O', 'O', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'E', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "def predict_page(block_text):\n",
    "    inputs = tokenizer(block_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs.pop('token_type_ids', None) \n",
    "\n",
    "    # Move tensors to same device as the model\n",
    "    inputs = {k: v.to(token_model.device) for k, v in inputs.items()}\n",
    "\n",
    "    # Get predictions                        \n",
    "    with torch.no_grad():\n",
    "        outputs = token_model(**inputs)\n",
    "\n",
    "    # Process outputs for token classification\n",
    "    logits = outputs[0]\n",
    "    predicted_labels = torch.argmax(logits, dim=2)\n",
    "\n",
    "    # Convert predicted labels to tags (e.g., 'B', 'I', 'O')\n",
    "    predicted_tags = [index_to_label[label.item()] for label in predicted_labels[0]]\n",
    "\n",
    "    return predicted_tags\n",
    "\n",
    "doc_id = \"27605974\"\n",
    "page_index = \"18\"\n",
    "page_text = data_dict[doc_id][page_index]['full_text']\n",
    "print(page_text)\n",
    "\n",
    "predicted_tags = predict_page(page_text)\n",
    "print(f\"Predicted tags: {predicted_tags}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for better visualisation of model predictions for a page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OP              : O (0.9969) --> ['definitions']\n",
      "IN              : O (0.9972) --> ['definitions']\n",
      "ION             : O (0.9975) --> ['definitions']\n",
      "ĠON             : O (0.9974) --> ['definitions']\n",
      "ĠAM             : O (0.9977) --> ['definitions']\n",
      "L               : O (0.9972) --> ['definitions']\n",
      "/               : O (0.9977) --> ['definitions']\n",
      "C               : O (0.9970) --> ['definitions']\n",
      "FT              : O (0.9967) --> ['definitions']\n",
      "ĠAND            : O (0.9979) --> ['definitions']\n",
      "ĠC              : O (0.9979) --> ['definitions']\n",
      "UST             : O (0.9976) --> ['definitions']\n",
      "OM              : O (0.9978) --> ['definitions']\n",
      "ERS             : O (0.9979) --> ['definitions']\n",
      "ĠWHO            : O (0.9981) --> ['definitions']\n",
      "ĠARE            : O (0.9981) --> ['definitions']\n",
      "ĠAS             : O (0.9981) --> ['definitions']\n",
      "YL              : O (0.9978) --> ['definitions']\n",
      "UM              : O (0.9978) --> ['definitions']\n",
      "ĠSEE            : O (0.9975) --> ['definitions']\n",
      "K               : O (0.9976) --> ['definitions']\n",
      "ERS             : O (0.9978) --> ['definitions']\n",
      "Ġ               : O (0.9979) --> ['definitions']\n",
      "Ċ               : O (0.9982) --> ['definitions']\n",
      "Ġ               : O (0.9976) --> ['definitions']\n",
      "Ċ               : O (0.9980) --> ['definitions']\n",
      "Ġ               : O (0.9975) --> ['definitions']\n",
      "Ċ               : O (0.9974) --> ['definitions']\n",
      "4               : O (0.9924) --> ['definitions']\n",
      "Ġ               : O (0.9937) --> ['definitions']\n",
      "Ċ               : O (0.9942) --> ['definitions']\n",
      "Def             : O (0.9800) --> ['definitions']\n",
      "initions        : O (0.9839) --> ['definitions']\n",
      "Ġ               : O (0.9883) --> ['definitions']\n",
      "Ċ               : O (0.9897) --> ['definitions']\n",
      "Unless          : O (0.9794) --> ['definitions']\n",
      "Ġotherwise      : O (0.9840) --> ['definitions']\n",
      "Ġspecified      : O (0.9775) --> ['definitions']\n",
      ",               : O (0.9784) --> ['definitions']\n",
      "Ġthe            : O (0.9760) --> ['definitions']\n",
      "Ġterms          : O (0.9608) --> ['definitions']\n",
      "Ġused           : O (0.9743) --> ['definitions']\n",
      "Ġin             : O (0.9779) --> ['definitions']\n",
      "Ġthis           : O (0.9802) --> ['definitions']\n",
      "ĠOpinion        : O (0.9689) --> ['definitions']\n",
      "Ġhave           : O (0.9777) --> ['definitions']\n",
      "Ġthe            : O (0.9782) --> ['definitions']\n",
      "Ġsame           : O (0.9812) --> ['definitions']\n",
      "Ġmeaning        : O (0.9662) --> ['definitions']\n",
      "Ġas             : O (0.9791) --> ['definitions']\n",
      "Ġthose          : O (0.9774) --> ['definitions']\n",
      "Ġ               : O (0.9797) --> ['definitions']\n",
      "Ċ               : O (0.9827) --> ['definitions']\n",
      "defined         : O (0.9717) --> ['definitions']\n",
      "Ġin             : O (0.9813) --> ['definitions']\n",
      "ĠDirective      : O (0.9742) --> ['definitions']\n",
      "Ġ(              : O (0.9810) --> ['definitions']\n",
      "EU              : O (0.9626) --> ['definitions']\n",
      ")               : O (0.9840) --> ['definitions']\n",
      "Ġ2015           : O (0.9791) --> ['definitions']\n",
      "/               : O (0.9810) --> ['definitions']\n",
      "8               : O (0.9734) --> ['definitions']\n",
      "49              : O (0.9731) --> ['definitions']\n",
      ".               : I (0.7318) --> ['definitions']\n",
      "ĠIn             : O (0.9789) --> ['definitions']\n",
      "Ġaddition       : O (0.9779) --> ['definitions']\n",
      ",               : O (0.9742) --> ['definitions']\n",
      "Ġfor            : O (0.9680) --> ['definitions']\n",
      "Ġthe            : O (0.9743) --> ['definitions']\n",
      "Ġpurpose        : O (0.9686) --> ['definitions']\n",
      "Ġof             : O (0.9716) --> ['definitions']\n",
      "Ġthis           : O (0.9774) --> ['definitions']\n",
      "ĠOpinion        : O (0.9709) --> ['definitions']\n",
      ",               : O (0.9702) --> ['definitions']\n",
      "Ġthe            : O (0.9750) --> ['definitions']\n",
      "Ġfollowing      : O (0.9715) --> ['definitions']\n",
      "Ġ               : O (0.9690) --> ['definitions']\n",
      "Ċ               : O (0.9727) --> ['definitions']\n",
      "def             : O (0.9602) --> ['definitions']\n",
      "initions        : O (0.9354) --> ['definitions']\n",
      "Ġapply          : O (0.9652) --> ['definitions']\n",
      ":               : O (0.9506) --> ['definitions']\n",
      "Ġ               : I (0.6508) --> ['definitions']\n",
      "Ċ               : I (0.6613) --> ['definitions']\n",
      "Ġ               : I (0.8385) --> ['definitions']\n",
      "Ġ               : I (0.8469) --> ['definitions']\n",
      "Ċ               : I (0.8476) --> ['definitions']\n",
      "âĢ¢             : I (0.8646) --> ['definitions']\n",
      "Ġ               : I (0.8764) --> ['definitions']\n",
      "Ċ               : I (0.8786) --> ['definitions']\n",
      "As              : I (0.8727) --> ['definitions']\n",
      "ylum            : I (0.8709) --> ['definitions']\n",
      "Ġseeker         : I (0.8574) --> ['definitions']\n",
      "Ġmeans          : I (0.8735) --> ['definitions']\n",
      "Ġpersons        : I (0.8986) --> ['definitions']\n",
      "Ġseeking        : I (0.9014) --> ['definitions']\n",
      "Ġasylum         : I (0.8909) --> ['definitions']\n",
      "Ġunder          : I (0.9081) --> ['definitions']\n",
      "Ġthe            : I (0.8978) --> ['definitions']\n",
      "ĠGeneva         : I (0.8908) --> ['definitions']\n",
      "ĠConvention     : I (0.8890) --> ['definitions']\n",
      "Ġof             : I (0.8847) --> ['definitions']\n",
      "Ġ28             : I (0.8851) --> ['definitions']\n",
      "ĠJuly           : I (0.8731) --> ['definitions']\n",
      "Ġ               : I (0.8828) --> ['definitions']\n",
      "Ċ               : I (0.8938) --> ['definitions']\n",
      "19              : I (0.8979) --> ['definitions']\n",
      "51              : I (0.9041) --> ['definitions']\n",
      "ĠRel            : I (0.9014) --> ['definitions']\n",
      "ating           : I (0.9106) --> ['definitions']\n",
      "Ġto             : I (0.9110) --> ['definitions']\n",
      "Ġthe            : I (0.9073) --> ['definitions']\n",
      "ĠStatus         : I (0.9087) --> ['definitions']\n",
      "Ġof             : I (0.9049) --> ['definitions']\n",
      "ĠRefugees       : I (0.8926) --> ['definitions']\n",
      ",               : I (0.8928) --> ['definitions']\n",
      "Ġthe            : I (0.9023) --> ['definitions']\n",
      "ĠProtocol       : I (0.8942) --> ['definitions']\n",
      "Ġthereto        : I (0.9052) --> ['definitions']\n",
      "Ġof             : I (0.9026) --> ['definitions']\n",
      "Ġ31             : I (0.8938) --> ['definitions']\n",
      "ĠJanuary        : I (0.9030) --> ['definitions']\n",
      "Ġ1967           : I (0.9002) --> ['definitions']\n",
      "Ġand            : I (0.8858) --> ['definitions']\n",
      "Ġ               : I (0.9004) --> ['definitions']\n",
      "Ċ               : I (0.8938) --> ['definitions']\n",
      "other           : I (0.9009) --> ['definitions']\n",
      "Ġrelevant       : I (0.8938) --> ['definitions']\n",
      "Ġinternational  : I (0.8965) --> ['definitions']\n",
      "Ġtreaties       : I (0.8949) --> ['definitions']\n",
      ".               : I (0.8601) --> ['definitions']\n",
      "7               : I (0.8560) --> ['definitions']\n",
      "Ġ               : I (0.8449) --> ['definitions']\n",
      "Ġ               : I (0.8591) --> ['definitions']\n",
      "Ġ               : I (0.8663) --> ['definitions']\n",
      "Ċ               : I (0.8718) --> ['definitions']\n",
      "Ġ               : I (0.8958) --> ['definitions']\n",
      "Ċ               : I (0.8896) --> ['definitions']\n",
      "âĢ¢             : I (0.8996) --> ['definitions']\n",
      "Ġ               : I (0.9091) --> ['definitions']\n",
      "Ċ               : I (0.9064) --> ['definitions']\n",
      "Higher          : I (0.9172) --> ['definitions']\n",
      "-               : I (0.9199) --> ['definitions']\n",
      "risk            : I (0.9121) --> ['definitions']\n",
      "Ġthird          : I (0.9219) --> ['definitions']\n",
      "Ġcountry        : I (0.9200) --> ['definitions']\n",
      "Ġor             : I (0.9181) --> ['definitions']\n",
      "Ġterritory      : I (0.9233) --> ['definitions']\n",
      "Ġmeans          : I (0.9141) --> ['definitions']\n",
      "Ġany            : I (0.9269) --> ['definitions']\n",
      "Ġcountry        : I (0.9257) --> ['definitions']\n",
      "Ġor             : I (0.9323) --> ['definitions']\n",
      "Ġterritory      : I (0.9302) --> ['definitions']\n",
      "Ġthat           : I (0.9322) --> ['definitions']\n",
      "Ġis             : I (0.9318) --> ['definitions']\n",
      "Ġassociated     : I (0.9326) --> ['definitions']\n",
      "Ġwith           : I (0.9304) --> ['definitions']\n",
      "Ġ               : I (0.9274) --> ['definitions']\n",
      "Ċ               : I (0.9261) --> ['definitions']\n",
      "significant     : I (0.9224) --> ['definitions']\n",
      "ĠML             : I (0.9226) --> ['definitions']\n",
      "/               : I (0.9286) --> ['definitions']\n",
      "TF              : I (0.9196) --> ['definitions']\n",
      "Ġrisk           : I (0.9160) --> ['definitions']\n",
      "Ġas             : I (0.9342) --> ['definitions']\n",
      "Ġa              : I (0.9293) --> ['definitions']\n",
      "Ġresult         : I (0.9333) --> ['definitions']\n",
      "Ġof             : I (0.9328) --> ['definitions']\n",
      "Ġthe            : I (0.9244) --> ['definitions']\n",
      "Ġprevalence     : I (0.9239) --> ['definitions']\n",
      "Ġof             : I (0.9238) --> ['definitions']\n",
      "Ġgroups         : I (0.9261) --> ['definitions']\n",
      "Ġcommitting     : I (0.9313) --> ['definitions']\n",
      "Ġterrorist      : I (0.9158) --> ['definitions']\n",
      "Ġ               : I (0.9361) --> ['definitions']\n",
      "Ċ               : I (0.9262) --> ['definitions']\n",
      "off             : I (0.9128) --> ['definitions']\n",
      "ences           : I (0.9263) --> ['definitions']\n",
      ",               : I (0.9377) --> ['definitions']\n",
      "Ġterrorist      : I (0.9221) --> ['definitions']\n",
      "Ġfinance        : I (0.9314) --> ['definitions']\n",
      "Ġand            : I (0.9317) --> ['definitions']\n",
      "Ġpredicate      : I (0.9348) --> ['definitions']\n",
      "Ġoffences       : I (0.9361) --> ['definitions']\n",
      "Ġto             : I (0.9329) --> ['definitions']\n",
      "Ġmoney          : I (0.9193) --> ['definitions']\n",
      "Ġlaundering     : I (0.9291) --> ['definitions']\n",
      ".               : I (0.7318) --> ['definitions']\n",
      "ĠThis           : I (0.9099) --> ['definitions']\n",
      "Ġterm           : I (0.9061) --> ['definitions']\n",
      "Ġis             : I (0.9129) --> ['definitions']\n",
      "Ġ               : I (0.9175) --> ['definitions']\n",
      "Ċ               : I (0.9103) --> ['definitions']\n",
      "different       : I (0.9127) --> ['definitions']\n",
      "Ġfrom           : I (0.9161) --> ['definitions']\n",
      "Ġthe            : I (0.9160) --> ['definitions']\n",
      "Ġterm           : I (0.9137) --> ['definitions']\n",
      "ĠâĢ             : I (0.9208) --> ['definitions']\n",
      "ĺ               : I (0.9188) --> ['definitions']\n",
      "high            : I (0.9235) --> ['definitions']\n",
      "Ġrisk           : I (0.9205) --> ['definitions']\n",
      "Ġthird          : I (0.9271) --> ['definitions']\n",
      "Ġcountries      : I (0.9244) --> ['definitions']\n",
      "âĢ              : I (0.9101) --> ['definitions']\n",
      "Ļ               : I (0.9109) --> ['definitions']\n",
      "Ġwith           : I (0.9316) --> ['definitions']\n",
      "Ġstrategic      : I (0.9189) --> ['definitions']\n",
      "Ġdeficiencies   : I (0.9281) --> ['definitions']\n",
      "Ġin             : I (0.9332) --> ['definitions']\n",
      "Ġtheir          : I (0.9325) --> ['definitions']\n",
      "Ġ               : I (0.9245) --> ['definitions']\n",
      "Ċ               : I (0.9173) --> ['definitions']\n",
      "AM              : I (0.9263) --> ['definitions']\n",
      "L               : I (0.9270) --> ['definitions']\n",
      "/               : I (0.9303) --> ['definitions']\n",
      "C               : I (0.9225) --> ['definitions']\n",
      "FT              : I (0.9245) --> ['definitions']\n",
      "Ġregime         : I (0.9326) --> ['definitions']\n",
      "Ġtargeted       : I (0.9410) --> ['definitions']\n",
      "Ġby             : I (0.9291) --> ['definitions']\n",
      "ĠArticle        : I (0.9254) --> ['definitions']\n",
      "Ġ9              : I (0.9221) --> ['definitions']\n",
      "Ġof             : I (0.9240) --> ['definitions']\n",
      "ĠDirective      : I (0.9185) --> ['definitions']\n",
      "Ġ(              : I (0.9244) --> ['definitions']\n",
      "EU              : I (0.9174) --> ['definitions']\n",
      ")               : I (0.9126) --> ['definitions']\n",
      "Ġ2015           : I (0.9164) --> ['definitions']\n",
      "/               : I (0.9020) --> ['definitions']\n",
      "8               : I (0.9078) --> ['definitions']\n",
      "49              : I (0.9060) --> ['definitions']\n",
      ".               : I (0.8862) --> ['definitions']\n",
      "8               : I (0.8815) --> ['definitions']\n",
      "Ġ               : O (0.9325) --> ['cdd']\n",
      "Ġ               : O (0.9418) --> ['cdd']\n",
      "Ċ               : O (0.9325) --> ['cdd']\n",
      "ĠLegal          : O (0.9024) --> ['definitions']\n",
      "Ġobligations    : O (0.8907) --> ['definitions']\n",
      "Ġ               : O (0.9017) --> ['definitions']\n",
      "Ċ               : O (0.8997) --> ['definitions']\n",
      "Inst            : O (0.8693) --> ['definitions']\n",
      "it              : O (0.8553) --> ['definitions']\n",
      "utions          : O (0.8416) --> ['definitions']\n",
      "âĢ              : O (0.8807) --> ['definitions']\n",
      "Ļ               : O (0.8863) --> ['definitions']\n",
      "ĠAM             : O (0.8475) --> ['definitions']\n",
      "L               : O (0.8322) --> ['definitions']\n",
      "/               : O (0.8370) --> ['definitions']\n",
      "C               : O (0.8212) --> ['definitions']\n",
      "FT              : O (0.8076) --> ['definitions']\n",
      "Ġobligations    : O (0.8317) --> ['definitions']\n",
      "Ġare            : O (0.9024) --> ['definitions']\n",
      "Ġset            : O (0.8950) --> ['definitions']\n",
      "Ġout            : O (0.9017) --> ['definitions']\n",
      "Ġin             : O (0.8928) --> ['definitions']\n",
      "Ġnational       : O (0.8578) --> ['definitions']\n",
      "Ġlaw            : O (0.8586) --> ['definitions']\n",
      "Ġtrans          : O (0.8750) --> ['definitions']\n",
      "posing          : O (0.8783) --> ['definitions']\n",
      "Ġthe            : O (0.8936) --> ['definitions']\n",
      "Ġapplicable     : O (0.8601) --> ['definitions']\n",
      "ĠEU             : O (0.8357) --> ['definitions']\n",
      "Ġ               : O (0.8598) --> ['definitions']\n",
      "Ċ               : O (0.8929) --> ['definitions']\n",
      "Direct          : O (0.8664) --> ['definitions']\n",
      "ive             : O (0.8741) --> ['definitions']\n",
      "Ġon             : O (0.8712) --> ['definitions']\n",
      "Ġthe            : O (0.8788) --> ['definitions']\n",
      "Ġprevention     : O (0.8150) --> ['definitions']\n",
      "Ġof             : O (0.8090) --> ['definitions']\n",
      "Ġthe            : O (0.8420) --> ['definitions']\n",
      "Ġuse            : O (0.8169) --> ['definitions']\n",
      "Ġof             : O (0.8133) --> ['definitions']\n",
      "Ġthe            : O (0.8215) --> ['definitions']\n",
      "Ġfinancial      : O (0.7791) --> ['definitions']\n",
      "Ġsystem         : O (0.7969) --> ['definitions']\n",
      "Ġfor            : O (0.8074) --> ['definitions']\n",
      "Ġthe            : O (0.8491) --> ['definitions']\n",
      "Ġpurposes       : O (0.8180) --> ['definitions']\n",
      "Ġof             : O (0.8169) --> ['definitions']\n",
      "Ġmoney          : O (0.8253) --> ['definitions']\n",
      "Ġ               : O (0.8069) --> ['definitions']\n",
      "Ċ               : O (0.7941) --> ['definitions']\n",
      "l               : O (0.8119) --> ['definitions']\n",
      "aundering       : O (0.8068) --> ['definitions']\n",
      "Ġor             : O (0.8306) --> ['definitions']\n",
      "Ġterrorist      : O (0.8173) --> ['definitions']\n",
      "Ġfinancing      : O (0.7753) --> ['definitions']\n",
      "Ġ(              : O (0.8904) --> ['definitions']\n",
      "AM              : O (0.8947) --> ['definitions']\n",
      "L               : O (0.8519) --> ['definitions']\n",
      "/               : O (0.8791) --> ['definitions']\n",
      "C               : O (0.8626) --> ['definitions']\n",
      "FT              : O (0.8209) --> ['definitions']\n",
      "ĠDirective      : O (0.8528) --> ['definitions']\n",
      ").              : O (0.9023) --> ['definitions']\n",
      "9               : O (0.8884) --> ['definitions']\n",
      "ĠThese          : O (0.9057) --> ['definitions']\n",
      "Ġtexts          : O (0.8798) --> ['definitions']\n",
      "Ġlay            : O (0.8946) --> ['definitions']\n",
      "Ġdown           : O (0.8865) --> ['definitions']\n",
      "Ġmeasures       : O (0.8550) --> ['definitions']\n",
      "Ġthat           : O (0.8460) --> ['definitions']\n",
      "Ġ               : O (0.8503) --> ['definitions']\n",
      "Ċ               : O (0.8588) --> ['definitions']\n",
      "inst            : O (0.7906) --> ['definitions']\n",
      "it              : O (0.7819) --> ['definitions']\n",
      "utions          : O (0.7620) --> ['definitions']\n",
      "Ġwithin         : O (0.7914) --> ['definitions']\n",
      "Ġtheir          : O (0.8067) --> ['definitions']\n",
      "Ġscope          : O (0.8107) --> ['definitions']\n",
      "Ġmust           : O (0.8167) --> ['definitions']\n",
      "Ġtake           : O (0.8419) --> ['definitions']\n",
      "Ġto             : O (0.8360) --> ['definitions']\n",
      "Ġprevent        : O (0.7968) --> ['definitions']\n",
      "Ġand            : O (0.8082) --> ['definitions']\n",
      "Ġdetect         : O (0.8273) --> ['definitions']\n",
      "ĠML             : O (0.8290) --> ['definitions']\n",
      "/               : O (0.8359) --> ['definitions']\n",
      "TF              : O (0.8238) --> ['definitions']\n",
      ".               : O (0.9394) --> ['other']\n",
      "Ġ               : O (0.8254) --> ['cdd']\n",
      "Ġ               : O (0.8111) --> ['cdd']\n",
      "Ċ               : O (0.7326) --> ['cdd']\n",
      "ĠThe            : O (0.5359) --> ['cdd']\n",
      "ĠAM             : I (0.4850) --> ['cdd']\n",
      "L               : I (0.5486) --> ['cdd']\n",
      "/               : I (0.5535) --> ['cdd']\n",
      "C               : I (0.5137) --> ['cdd']\n",
      "FT              : I (0.5506) --> ['cdd']\n",
      "ĠDirect         : I (0.6125) --> ['cdd']\n",
      "ives            : I (0.6168) --> ['cdd']\n",
      "Ġmake           : I (0.7534) --> ['cdd']\n",
      "Ġthe            : I (0.8059) --> ['cdd']\n",
      "Ġsuccessful     : I (0.8239) --> ['cdd']\n",
      "Ġapplication    : I (0.8231) --> ['cdd']\n",
      "Ġof             : I (0.8276) --> ['cdd']\n",
      "Ġrisk           : I (0.7585) --> ['cdd']\n",
      "-               : I (0.8352) --> ['cdd']\n",
      "sensitive       : I (0.8149) --> ['cdd']\n",
      "ĠCD             : I (0.7555) --> ['cdd']\n",
      "D               : I (0.7489) --> ['cdd']\n",
      "Ġcontrols       : I (0.7941) --> ['cdd']\n",
      "Ġto             : I (0.8257) --> ['cdd']\n",
      "Ġall            : I (0.8012) --> ['cdd']\n",
      "Ġ               : I (0.8180) --> ['cdd']\n",
      "Ċ               : I (0.8085) --> ['cdd']\n",
      "custom          : I (0.7731) --> ['cdd']\n",
      "ers             : I (0.8030) --> ['cdd']\n",
      "Ġand            : I (0.8194) --> ['cdd']\n",
      "Ġtheir          : I (0.8110) --> ['cdd']\n",
      "Ġbeneficial     : I (0.8205) --> ['cdd']\n",
      "Ġowners         : I (0.8101) --> ['cdd']\n",
      "Ġa              : I (0.7959) --> ['cdd']\n",
      "Ġcondition      : I (0.7886) --> ['cdd']\n",
      "Ġfor            : I (0.8189) --> ['cdd']\n",
      "Ġthe            : I (0.8088) --> ['cdd']\n",
      "Ġestablishment  : I (0.8301) --> ['cdd']\n",
      "Ġof             : I (0.8458) --> ['cdd']\n",
      "Ġa              : I (0.8310) --> ['cdd']\n",
      "Ġbusiness       : I (0.8191) --> ['cdd']\n",
      "Ġ               : I (0.8408) --> ['cdd']\n",
      "Ċ               : I (0.8221) --> ['cdd']\n",
      "relations       : I (0.8208) --> ['cdd']\n",
      "hip             : I (0.8195) --> ['cdd']\n",
      ".               : I (0.8114) --> ['cdd']\n",
      "ĠWhere          : I (0.8296) --> ['cdd']\n",
      "Ġinstitutions   : I (0.8468) --> ['cdd']\n",
      "Ġcannot         : I (0.8680) --> ['cdd']\n",
      "Ġcomply         : I (0.8608) --> ['cdd']\n",
      "Ġwith           : I (0.8508) --> ['cdd']\n",
      "Ġthese          : I (0.8332) --> ['cdd']\n",
      "Ġobligations    : I (0.8250) --> ['cdd']\n",
      ",               : I (0.8539) --> ['cdd']\n",
      "Ġthey           : I (0.8488) --> ['cdd']\n",
      "Ġcannot         : I (0.8318) --> ['cdd']\n",
      "Ġenter          : I (0.8529) --> ['cdd']\n",
      "Ġinto           : I (0.8577) --> ['cdd']\n",
      ",               : I (0.8494) --> ['cdd']\n",
      "Ġor             : I (0.8601) --> ['cdd']\n",
      "Ġ               : I (0.8622) --> ['cdd']\n",
      "Ċ               : I (0.8455) --> ['cdd']\n",
      "m               : I (0.8553) --> ['cdd']\n",
      "aintain         : I (0.8480) --> ['cdd']\n",
      ",               : I (0.8623) --> ['cdd']\n",
      "Ġa              : I (0.8593) --> ['cdd']\n",
      "Ġbusiness       : I (0.8491) --> ['cdd']\n",
      "Ġrelationship   : I (0.8517) --> ['cdd']\n",
      ".               : I (0.7318) --> ['definitions']\n",
      "ĠThe            : I (0.6991) --> ['cdd']\n",
      "ĠAM             : I (0.6791) --> ['cdd']\n",
      "L               : I (0.7283) --> ['cdd']\n",
      "/               : I (0.7397) --> ['cdd']\n",
      "C               : I (0.7492) --> ['cdd']\n",
      "FT              : I (0.7392) --> ['cdd']\n",
      "ĠDirect         : I (0.7269) --> ['cdd']\n",
      "ives            : I (0.7606) --> ['cdd']\n",
      "Ġalso           : I (0.7977) --> ['cdd']\n",
      "Ġrequire        : I (0.7876) --> ['cdd']\n",
      "Ġinstitutions   : I (0.8220) --> ['cdd']\n",
      "Ġto             : I (0.8403) --> ['cdd']\n",
      "Ġtake           : I (0.8493) --> ['cdd']\n",
      "Ġ               : I (0.8474) --> ['cdd']\n",
      "Ċ               : I (0.8258) --> ['cdd']\n",
      "adequ           : I (0.8391) --> ['cdd']\n",
      "ate             : I (0.8591) --> ['cdd']\n",
      "Ġmeasures       : I (0.8507) --> ['cdd']\n",
      "Ġto             : I (0.8618) --> ['cdd']\n",
      "Ġmanage         : I (0.8261) --> ['cdd']\n",
      "ĠML             : I (0.8009) --> ['cdd']\n",
      "/               : I (0.8178) --> ['cdd']\n",
      "TF              : I (0.8144) --> ['cdd']\n",
      "Ġrisk           : I (0.8311) --> ['cdd']\n",
      ",               : I (0.8335) --> ['cdd']\n",
      "Ġwhich          : I (0.8572) --> ['cdd']\n",
      "Ġinclude        : I (0.8530) --> ['cdd']\n",
      "Ġmeasures       : I (0.8520) --> ['cdd']\n",
      "Ġthat           : I (0.8615) --> ['cdd']\n",
      "Ġenable         : I (0.8424) --> ['cdd']\n",
      "Ġinstitutions   : I (0.8359) --> ['cdd']\n",
      "Ġto             : I (0.8573) --> ['cdd']\n",
      "Ġ               : I (0.8392) --> ['cdd']\n",
      "Ċ               : I (0.8443) --> ['cdd']\n",
      "ident           : I (0.8510) --> ['cdd']\n",
      "ify             : I (0.8434) --> ['cdd']\n",
      "Ġand            : I (0.8472) --> ['cdd']\n",
      "Ġreport         : I (0.8292) --> ['cdd']\n",
      "Ġsuspicious     : I (0.8298) --> ['cdd']\n",
      "Ġtransactions   : I (0.8437) --> ['cdd']\n",
      ".               : O (0.7907) --> ['cdd']\n",
      "Ġ               : O (0.8913) --> ['cdd']\n",
      "Ċ               : O (0.8758) --> ['cdd']\n",
      "ĠThese          : I (0.5081) --> ['cdd']\n",
      "Ġobligations    : I (0.6766) --> ['cdd']\n",
      "Ġapply          : I (0.7521) --> ['cdd']\n",
      "Ġalso           : I (0.7558) --> ['cdd']\n",
      "Ġin             : I (0.7916) --> ['cdd']\n",
      "Ġsituations     : I (0.7931) --> ['cdd']\n",
      "Ġwhere          : I (0.7884) --> ['cdd']\n",
      "ĠDirective      : I (0.7640) --> ['cdd']\n",
      "Ġ2014           : I (0.7492) --> ['cdd']\n",
      "/               : I (0.7557) --> ['cdd']\n",
      "92              : I (0.7438) --> ['cdd']\n",
      "/               : I (0.7516) --> ['cdd']\n",
      "EU              : I (0.7825) --> ['cdd']\n",
      "Ġon             : I (0.7628) --> ['cdd']\n",
      "Ġpayment        : I (0.7663) --> ['cdd']\n",
      "Ġaccounts       : I (0.7554) --> ['cdd']\n",
      "Ġ               : I (0.7344) --> ['cdd']\n",
      "Ċ               : I (0.7368) --> ['cdd']\n",
      "(               : I (0.7456) --> ['cdd']\n",
      "P               : I (0.7525) --> ['cdd']\n",
      "AD              : I (0.7505) --> ['cdd']\n",
      ")               : I (0.7724) --> ['cdd']\n",
      "Ġ10             : I (0.7628) --> ['cdd']\n",
      "Ġcreates        : I (0.8123) --> ['cdd']\n",
      "Ġa              : I (0.8292) --> ['cdd']\n",
      "Ġright          : I (0.8382) --> ['cdd']\n",
      "Ġfor            : I (0.8514) --> ['cdd']\n",
      "Ġall            : I (0.8566) --> ['cdd']\n",
      "Ġconsumers      : I (0.8434) --> ['cdd']\n",
      "Ġwho            : I (0.8618) --> ['cdd']\n",
      "Ġare            : I (0.8724) --> ['cdd']\n",
      "Ġlegally        : I (0.8696) --> ['cdd']\n",
      "Ġresident       : I (0.8586) --> ['cdd']\n",
      "Ġin             : I (0.8592) --> ['cdd']\n",
      "Ġthe            : I (0.8453) --> ['cdd']\n",
      "ĠEuropean       : I (0.8517) --> ['cdd']\n",
      "ĠUnion          : I (0.8538) --> ['cdd']\n",
      ",               : I (0.8757) --> ['cdd']\n",
      "Ġincluding      : I (0.8701) --> ['cdd']\n",
      "Ġ               : I (0.8603) --> ['cdd']\n",
      "Ċ               : I (0.8370) --> ['cdd']\n",
      "as              : I (0.8524) --> ['cdd']\n",
      "ylum            : I (0.8171) --> ['cdd']\n",
      "Ġseekers        : I (0.8466) --> ['cdd']\n",
      ",               : I (0.8522) --> ['cdd']\n",
      "Ġto             : I (0.8489) --> ['cdd']\n",
      "Ġobtain         : I (0.8451) --> ['cdd']\n",
      "Ġa              : I (0.8447) --> ['cdd']\n",
      "Ġbasic          : I (0.8349) --> ['cdd']\n",
      "Ġpayment        : I (0.8314) --> ['cdd']\n",
      "Ġaccount        : I (0.8388) --> ['cdd']\n",
      "Ġand            : I (0.8459) --> ['cdd']\n",
      "Ġprotects       : I (0.8352) --> ['cdd']\n",
      "Ġthem           : I (0.8603) --> ['cdd']\n",
      "Ġfrom           : I (0.8282) --> ['cdd']\n",
      "Ġdiscrimination : I (0.8516) --> ['cdd']\n",
      "Ġon             : I (0.8629) --> ['cdd']\n",
      "Ġthe            : I (0.8513) --> ['cdd']\n",
      "Ġ               : I (0.8467) --> ['cdd']\n",
      "Ċ               : I (0.8356) --> ['cdd']\n",
      "bas             : I (0.8444) --> ['cdd']\n",
      "is              : I (0.8535) --> ['cdd']\n",
      "Ġof             : I (0.8620) --> ['cdd']\n",
      "Ġtheir          : I (0.8599) --> ['cdd']\n",
      "Ġnationality    : I (0.8560) --> ['cdd']\n",
      ",               : I (0.8757) --> ['cdd']\n",
      "Ġplace          : I (0.8491) --> ['cdd']\n",
      "Ġof             : I (0.8579) --> ['cdd']\n",
      "Ġresidence      : I (0.8512) --> ['cdd']\n",
      "Ġor             : I (0.8845) --> ['cdd']\n",
      "Ġany            : I (0.8785) --> ['cdd']\n",
      "Ġground         : I (0.8454) --> ['cdd']\n",
      "Ġreferred       : I (0.8576) --> ['cdd']\n",
      "Ġto             : I (0.8541) --> ['cdd']\n",
      "Ġin             : I (0.8372) --> ['cdd']\n",
      "ĠArticle        : I (0.8289) --> ['cdd']\n",
      "Ġ21             : I (0.8314) --> ['cdd']\n",
      "Ġof             : I (0.8110) --> ['cdd']\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import softmax\n",
    "from detailed_labels_handler import decode_detailed_labels\n",
    "\n",
    "def display_token_predictions(model, tokenizer, text):\n",
    "    # Tokenize input text and prepare inputs for model (for simplicity, we truncate the text here)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs.pop('token_type_ids', None) \n",
    "    input_ids = inputs[\"input_ids\"][0]\n",
    "\n",
    "    # Move tensors to the same device as the model\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "   # Process outputs for token classification\n",
    "    logits = outputs[0]\n",
    "    predictions = torch.argmax(logits, dim=2)[0]\n",
    "    probabilities = softmax(logits, dim=2)[0]\n",
    "    \n",
    "    # Auxiliary objective: detailed labels\n",
    "    if HIER_LABELS_LEVELS:\n",
    "        detailed_logits = outputs[1]\n",
    "        detailed_predictions = []\n",
    "        if detailed_logits is not None:\n",
    "            encoded_detailed_labels_per_token = [[] for _ in range(len(input_ids))] # initialise list of lists to store detailed labels per token\n",
    "            for level_logits in detailed_logits:\n",
    "                level_probabilities = softmax(level_logits, dim=-1)\n",
    "                predicted_detailed_labels = torch.argmax(level_probabilities, dim=-1).squeeze()\n",
    "                if predicted_detailed_labels.dim() == 0:\n",
    "                    # in case of only one senetnce --> scalar\n",
    "                    encoded_detailed_labels_per_token[0].append(predicted_detailed_labels.item())\n",
    "                else:\n",
    "                    # Otherwise, iterate as normal\n",
    "                    for token_idx, label in enumerate(predicted_detailed_labels):\n",
    "                        encoded_detailed_labels_per_token[token_idx].append(label.item())\n",
    "\n",
    "            for encoded_labels in encoded_detailed_labels_per_token:\n",
    "                decoded_label = decode_detailed_labels([encoded_labels], mapping_dicts, HIER_LABELS_LEVELS)\n",
    "                detailed_predictions.append(decoded_label)\n",
    "    else: # placeholder\n",
    "        detailed_predictions = [\"N/A\"]*len(input_ids)\n",
    "\n",
    "    # Convert input ids to tokens and predicted labels to tags\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    predicted_tags = [index_to_label[label.item()] for label in predictions]\n",
    "\n",
    "    # Display tokens with their corresponding tags and probabilities\n",
    "    for token, tag, prob, detailed_tag in zip(tokens, predicted_tags, probabilities, detailed_predictions):\n",
    "        prob_value = prob[torch.argmax(prob)].item()  # Get highest probability value\n",
    "        if token not in [cls_token, sep_token, pad_token]:  # Skip special tokens as defined in code cells above\n",
    "            print(f\"{token:15} : {tag} ({prob_value:.4f}) --> {detailed_tag}\")\n",
    "\n",
    "\n",
    "# Get prediction for page from data_dict below\n",
    "doc_id = \"27416407\"\n",
    "page_index = \"3\" \n",
    "page_text = data_dict[doc_id][page_index]['full_text']\n",
    "display_token_predictions(token_model, tokenizer, page_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create snippets / post-processing algorithm (Section 5.4)\n",
    "\n",
    "![](../Misc/post-processing.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(tags):\n",
    "    '''\n",
    "    Majority vote function to determine the detailed label for a snippet\n",
    "    '''\n",
    "    if not tags:\n",
    "        return \"N/A\"  # Default label if no tags are present\n",
    "    \n",
    "    # Flatten into a single list\n",
    "    flat_tags = [item for sublist in tags for item in sublist if item is not None]\n",
    "    if not flat_tags:\n",
    "        return \"N/A\"  # Return default if flattened list is empty\n",
    "\n",
    "    # Return most frequent tag\n",
    "    return max(set(flat_tags), key=flat_tags.count)\n",
    "\n",
    "def create_snippets(tokens, tags, tokenizer, merged_detailed_tags=None, min_snippet_length=10, ignore_o_threshold = 4):\n",
    "    '''\n",
    "    Post-processing code to create snippets from NER (i.e. BIO(E)) tags\n",
    "    '''\n",
    "    snippets = []\n",
    "    current_snippet = []\n",
    "    current_detailed_tags = []\n",
    "    \n",
    "    # Sequentially process each token and its corresponding tag\n",
    "    idx = 0\n",
    "    while idx < len(tokens):\n",
    "        \n",
    "        # Get token and tag\n",
    "        token = tokens[idx]\n",
    "        tag = tags[idx]\n",
    "        detailed_tag = merged_detailed_tags[idx] if merged_detailed_tags else None\n",
    "\n",
    "        # Decide what to do based on the tag\n",
    "        # \"B\": Begin a new snippet\n",
    "        # \"I\": Continue the current snippet\n",
    "        # \"O\": End the current snippet\n",
    "        # \"E\": End the current snippet\n",
    "        if tag == 'I' and token not in [cls_token, sep_token]:\n",
    "            current_snippet.append(token)\n",
    "            if detailed_tag:\n",
    "                current_detailed_tags.append(detailed_tag)\n",
    "        elif tag == 'O':\n",
    "            o_count = 1\n",
    "            while idx + o_count < len(tags) and tags[idx + o_count] == 'O':\n",
    "                o_count += 1\n",
    "            \n",
    "            if current_snippet and o_count <= ignore_o_threshold:\n",
    "                # Append all 'O' sequence at once if below threshold\n",
    "                current_snippet.extend(tokens[idx:idx + o_count])\n",
    "                if detailed_tag:\n",
    "                    current_detailed_tags.extend(merged_detailed_tags[idx:idx + o_count])\n",
    "                idx += o_count - 1  # Skip 'O' sequence to the end\n",
    "            else:\n",
    "                # create snippet\n",
    "                if len(current_snippet) >= min_snippet_length:\n",
    "                    snippet_text = tokenizer.convert_tokens_to_string(current_snippet)\n",
    "                    snippets.append((snippet_text.strip(), majority_vote(current_detailed_tags)))\n",
    "                current_snippet = []\n",
    "                current_detailed_tags = []\n",
    "        elif tag == 'B':\n",
    "            # create snippet\n",
    "            if current_snippet and len(current_snippet) >= min_snippet_length:\n",
    "                snippet_text = tokenizer.convert_tokens_to_string(current_snippet)\n",
    "                snippets.append((snippet_text.strip(), majority_vote(current_detailed_tags)))\n",
    "            current_snippet = [token]\n",
    "            current_detailed_tags = [detailed_tag] if detailed_tag else []\n",
    "        elif tag == 'E':\n",
    "            if current_snippet:\n",
    "                current_snippet.append(token)\n",
    "                if detailed_tag:\n",
    "                    current_detailed_tags.append(detailed_tag)\n",
    "                if len(current_snippet) >= min_snippet_length:\n",
    "                    snippet_text = tokenizer.convert_tokens_to_string(current_snippet)\n",
    "                    snippets.append((snippet_text.strip(), majority_vote(current_detailed_tags)))\n",
    "                current_snippet = []\n",
    "                current_detailed_tags = []\n",
    "        idx += 1\n",
    "\n",
    "    # Check if there is a current snippet at the end\n",
    "    if current_snippet and len(current_snippet) >= min_snippet_length:\n",
    "        snippet_text = tokenizer.convert_tokens_to_string(current_snippet)\n",
    "        snippets.append((snippet_text.strip(), majority_vote(current_detailed_tags)))\n",
    "    return snippets\n",
    "\n",
    "def display_token_predictions_and_create_snippets(model, tokenizer, text, window_size=512, stride=256, min_snippet_length=10, ignore_o_threshold=4, verbose=True):\n",
    "    '''\n",
    "    Get token predictions and create snippets from the predictions using the custom post-processing algorithm.\n",
    "    '''\n",
    "    adjusted_window_size = window_size - 2  # Adjust for special tokens\n",
    "\n",
    "    # Tokenize text with stride handling to get multiple windows (including overlapping windows)\n",
    "    tokenized_text = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=adjusted_window_size, return_overflowing_tokens=True, stride=stride, add_special_tokens=True)\n",
    "\n",
    "    # Setup inputs dict for model\n",
    "    inputs = {\"input_ids\": [], \"attention_mask\": []}\n",
    "    for window in tokenized_text.encodings:\n",
    "        inputs[\"input_ids\"].append(torch.tensor([window.ids]).to(model.device))\n",
    "        inputs[\"attention_mask\"].append(torch.tensor([window.attention_mask]).to(model.device))\n",
    "\n",
    "    # Convert list of tensors to a single tensor for batch processing\n",
    "    batch_input_ids = torch.cat(inputs[\"input_ids\"], dim=0)\n",
    "    batch_attention_mask = torch.cat(inputs[\"attention_mask\"], dim=0)\n",
    "    batch_inputs = {\"input_ids\": batch_input_ids, \"attention_mask\": batch_attention_mask}\n",
    "\n",
    "    # Model prediction in one go (significantly more efficient due to batch handling)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch_inputs)\n",
    "    logits = outputs[0]\n",
    "    predicted_labels = torch.argmax(logits, dim=2)\n",
    "    \n",
    "    # Auxiliary Objective: detailed labels\n",
    "    detailed_predictions = []\n",
    "    if len(outputs) > 1 and outputs[1] is not None:\n",
    "        detailed_logits = outputs[1]\n",
    "        encoded_detailed_labels_per_token = [[] for _ in range(len(tokenized_text.input_ids[0]))]  # Adjust to token count\n",
    "\n",
    "        for level_logits in detailed_logits:\n",
    "            level_probabilities = torch.nn.functional.softmax(level_logits, dim=-1)\n",
    "            predicted_detailed_labels = torch.argmax(level_probabilities, dim=-1)\n",
    "            \n",
    "            # Predicted_detailed_labels shape is [batch_size, num_tokens]\n",
    "            # Batch Handling:\n",
    "            for token_idx in range(predicted_detailed_labels.shape[1]):  # Iterate over tokens\n",
    "                labels_for_token = predicted_detailed_labels[:, token_idx]  # All batch predictions for this token\n",
    "                encoded_detailed_labels_per_token[token_idx].extend(labels_for_token.tolist())\n",
    "        # Decode using heloer function\n",
    "        for encoded_labels in encoded_detailed_labels_per_token:\n",
    "            decoded_label = detailed_labels_handler.decode_detailed_labels([encoded_labels], mapping_dicts, HIER_LABELS_LEVELS)\n",
    "            detailed_predictions.append(decoded_label)\n",
    "    else:\n",
    "        detailed_predictions = [\"N/A\"] * len(tokenized_text.input_ids[0])\n",
    "        \n",
    "    # Merge tokens and tags from all windows back together --> this is the same as the original text for the page  \n",
    "    merged_tokens = []\n",
    "    merged_tags = []\n",
    "    merged_detailed_tags = []\n",
    "    for window_idx, window in enumerate(tokenized_text.encodings):\n",
    "        start_index = 1 if window_idx == 0 else stride + 1\n",
    "        end_index = -1 if window_idx < len(tokenized_text.encodings) - 1 else None\n",
    "        \n",
    "        tokens = tokenizer.convert_ids_to_tokens(batch_input_ids[window_idx])\n",
    "        window_tags = [index_to_label[label.item()] for label in predicted_labels[window_idx][start_index:end_index]]\n",
    "        window_detailed_tags = detailed_predictions[start_index:end_index]\n",
    "        window_tokens = [tok for tok in tokens[start_index:end_index] if tok not in [cls_token, sep_token, pad_token]]\n",
    "        \n",
    "        merged_tokens.extend(window_tokens)\n",
    "        merged_tags.extend(window_tags)\n",
    "        merged_detailed_tags.extend(window_detailed_tags)\n",
    "    \n",
    "    if verbose:\n",
    "        for token, tag, detailed_tag in zip(merged_tokens, merged_tags, merged_detailed_tags):\n",
    "            print(f\"{token:15} : {tag} --> {detailed_tag}\")\n",
    "\n",
    "    # Create snippets for page\n",
    "    snippets = create_snippets(merged_tokens, merged_tags, tokenizer, merged_detailed_tags=merged_detailed_tags,  min_snippet_length=min_snippet_length, ignore_o_threshold=ignore_o_threshold)\n",
    "    \n",
    "    if verbose:\n",
    "        for idx, (snippet, detailed_tag) in enumerate(snippets, 1):\n",
    "            print(f'---------------------- Snippet {idx} | Label: {detailed_tag} ----------------------')\n",
    "            print(snippet + \"\\n\")\n",
    "    \n",
    "    return snippets\n",
    "\n",
    "# Use above function with exemplary page from data_dict\n",
    "# Note: This is how the snippet identifier system would be used in practice! (We would only want to pass the text of a page and get the snippets)\n",
    "doc_id = \"20829680\" \n",
    "page_index = \"34\" \n",
    "page_text = data_dict[doc_id][page_index]['full_text']\n",
    "snippets = display_token_predictions_and_create_snippets(token_model, tokenizer, page_text, ignore_o_threshold=7, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of how custom PDFs could be fed into the model to identify FRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EXAMPLE1_finregulation.pdf', 'tlh45_Proposal.pdf', 'EXAMPLE2_CELEX_32018L0843_EN_TXT.pdf', 'EXAMPLE3_uksi_20170692_en.pdf', 'EXAMPLE0_wipo_financial_regulations.pdf']\n",
      "Ġ(              : I --> ['str']\n",
      "18              : I --> ['str']\n",
      ")               : I --> ['str']\n",
      "Ġin             : I --> ['str']\n",
      "ĠArticle        : I --> ['str']\n",
      "Ġ32             : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "Ġfollowing      : I --> ['str']\n",
      "Ġparagraph      : I --> ['str']\n",
      "Ġis             : I --> ['str']\n",
      "Ġadded          : I --> ['str']\n",
      ":               : I --> ['str']\n",
      "ĠâĢ             : I --> ['str']\n",
      "ĺ               : I --> ['str']\n",
      "9               : I --> ['str']\n",
      ".               : I --> ['str']\n",
      "Ġ               : I --> ['str']\n",
      "Ċ               : I --> ['str']\n",
      "Without         : I --> ['str']\n",
      "Ġprejudice      : I --> ['str']\n",
      "Ġto             : I --> ['str']\n",
      "ĠArticle        : I --> ['str']\n",
      "Ġ34             : I --> ['str']\n",
      "(               : I --> ['str']\n",
      "2               : I --> ['str']\n",
      "),              : I --> ['str']\n",
      "Ġin             : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "Ġcontext        : I --> ['str']\n",
      "Ġof             : I --> ['str']\n",
      "Ġits            : I --> ['str']\n",
      "Ġfunctions      : I --> ['str']\n",
      ",               : I --> ['str']\n",
      "Ġeach           : I --> ['str']\n",
      "ĠFI             : I --> ['str']\n",
      "U               : I --> ['str']\n",
      "Ġshall          : I --> ['str']\n",
      "Ġbe             : I --> ['str']\n",
      "Ġable           : I --> ['str']\n",
      "Ġto             : I --> ['str']\n",
      "Ġrequest        : I --> ['str']\n",
      ",               : I --> ['str']\n",
      "Ġobtain         : I --> ['str']\n",
      "Ġand            : I --> ['str']\n",
      "Ġ               : I --> ['str']\n",
      "Ċ               : I --> ['str']\n",
      "use             : I --> ['str']\n",
      "Ġinformation    : I --> ['str']\n",
      "Ġfrom           : I --> ['str']\n",
      "Ġany            : I --> ['str']\n",
      "Ġobliged        : I --> ['str']\n",
      "Ġentity         : I --> ['str']\n",
      "Ġfor            : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "Ġpurpose        : I --> ['str']\n",
      "Ġset            : I --> ['str']\n",
      "Ġin             : I --> ['str']\n",
      "Ġparagraph      : I --> ['str']\n",
      "Ġ1              : I --> ['str']\n",
      "Ġof             : I --> ['str']\n",
      "Ġthis           : I --> ['str']\n",
      "ĠArticle        : I --> ['str']\n",
      ",               : I --> ['str']\n",
      "Ġeven           : I --> ['str']\n",
      "Ġif             : I --> ['str']\n",
      "Ġno             : I --> ['str']\n",
      "Ġprior          : I --> ['str']\n",
      "Ġreport         : I --> ['str']\n",
      "Ġis             : I --> ['str']\n",
      "Ġ               : I --> ['str']\n",
      "Ċ               : I --> ['str']\n",
      "f               : I --> ['str']\n",
      "iled            : I --> ['str']\n",
      "Ġpursuant       : I --> ['str']\n",
      "Ġto             : I --> ['str']\n",
      "ĠArticle        : I --> ['str']\n",
      "Ġ33             : I --> ['str']\n",
      "(               : I --> ['str']\n",
      "1               : I --> ['str']\n",
      ")(              : I --> ['str']\n",
      "a               : I --> ['str']\n",
      ")               : I --> ['str']\n",
      "Ġor             : I --> ['str']\n",
      "Ġ34             : I --> ['str']\n",
      "(               : I --> ['str']\n",
      "1               : I --> ['str']\n",
      ").              : I --> ['str']\n",
      "âĢ              : I --> ['str']\n",
      "Ļ               : I --> ['str']\n",
      ";               : I --> ['str']\n",
      "Ġ(              : I --> ['str']\n",
      "19              : I --> ['str']\n",
      ")               : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "Ġfollowing      : I --> ['str']\n",
      "ĠArticle        : I --> ['str']\n",
      "Ġis             : I --> ['str']\n",
      "Ġinserted       : I --> ['str']\n",
      ":               : I --> ['str']\n",
      "ĠâĢ             : I --> ['str']\n",
      "ĺ               : I --> ['str']\n",
      "Article         : I --> ['str']\n",
      "Ġ32             : I --> ['str']\n",
      "a               : I --> ['str']\n",
      "Ġ1              : I --> ['str']\n",
      ".               : I --> ['str']\n",
      "Ġ               : I --> ['str']\n",
      "Ċ               : I --> ['str']\n",
      "Member          : I --> ['str']\n",
      "ĠStates         : I --> ['str']\n",
      "Ġshall          : I --> ['str']\n",
      "Ġput            : I --> ['str']\n",
      "Ġin             : I --> ['str']\n",
      "Ġplace          : I --> ['str']\n",
      "Ġcentral        : I --> ['str']\n",
      "ised            : I --> ['str']\n",
      "Ġautomated      : I --> ['str']\n",
      "Ġmechanisms     : I --> ['str']\n",
      ",               : I --> ['str']\n",
      "Ġsuch           : I --> ['str']\n",
      "Ġas             : I --> ['str']\n",
      "Ġcentral        : I --> ['str']\n",
      "Ġregist         : I --> ['str']\n",
      "ries            : I --> ['str']\n",
      "Ġor             : I --> ['str']\n",
      "Ġcentral        : I --> ['str']\n",
      "Ġ               : I --> ['str']\n",
      "Ċ               : I --> ['str']\n",
      "elect           : I --> ['str']\n",
      "ronic           : I --> ['str']\n",
      "Ġdata           : I --> ['str']\n",
      "Ġretrieval      : I --> ['str']\n",
      "Ġsystems        : I --> ['str']\n",
      ",               : I --> ['str']\n",
      "Ġwhich          : I --> ['str']\n",
      "Ġallow          : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "Ġidentification : I --> ['str']\n",
      ",               : I --> ['str']\n",
      "Ġin             : I --> ['str']\n",
      "Ġa              : I --> ['str']\n",
      "Ġtimely         : I --> ['str']\n",
      "Ġmanner         : I --> ['str']\n",
      ",               : I --> ['str']\n",
      "Ġof             : I --> ['str']\n",
      "Ġany            : I --> ['str']\n",
      "Ġnatural        : I --> ['str']\n",
      "Ġor             : I --> ['str']\n",
      "Ġlegal          : I --> ['str']\n",
      "Ġ               : I --> ['str']\n",
      "Ċ               : I --> ['str']\n",
      "pers            : I --> ['str']\n",
      "ons             : I --> ['str']\n",
      "Ġholding        : I --> ['str']\n",
      "Ġor             : I --> ['str']\n",
      "Ġcontrolling    : I --> ['str']\n",
      "Ġpayment        : I --> ['str']\n",
      "Ġaccounts       : I --> ['str']\n",
      "Ġand            : I --> ['str']\n",
      "Ġbank           : I --> ['str']\n",
      "Ġaccounts       : I --> ['str']\n",
      "Ġidentified     : I --> ['str']\n",
      "Ġby             : I --> ['str']\n",
      "ĠIB             : I --> ['str']\n",
      "AN              : I --> ['str']\n",
      ",               : I --> ['str']\n",
      "Ġas             : I --> ['str']\n",
      "Ġdefined        : I --> ['str']\n",
      "Ġby             : I --> ['str']\n",
      "ĠRegulation     : I --> ['str']\n",
      "Ġ               : I --> ['str']\n",
      "Ċ               : I --> ['str']\n",
      "(               : I --> ['str']\n",
      "EU              : I --> ['str']\n",
      ")               : I --> ['str']\n",
      "ĠNo             : I --> ['str']\n",
      "Ġ260            : I --> ['str']\n",
      "/               : I --> ['str']\n",
      "2012            : I --> ['str']\n",
      "Ġof             : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "ĠEuropean       : I --> ['str']\n",
      "ĠParliament     : I --> ['str']\n",
      "Ġand            : I --> ['str']\n",
      "Ġof             : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "ĠCouncil        : I --> ['str']\n",
      "Ġ(*             : I --> ['str']\n",
      "),              : I --> ['str']\n",
      "Ġand            : I --> ['str']\n",
      "Ġsafe           : I --> ['str']\n",
      "-               : I --> ['str']\n",
      "dep             : I --> ['str']\n",
      "osit            : I --> ['str']\n",
      "Ġboxes          : I --> ['str']\n",
      "Ġheld           : I --> ['str']\n",
      "Ġby             : I --> ['str']\n",
      "Ġa              : I --> ['str']\n",
      "Ġcredit         : I --> ['str']\n",
      "Ġ               : I --> ['str']\n",
      "Ċ               : I --> ['str']\n",
      "inst            : I --> ['str']\n",
      "itution         : I --> ['str']\n",
      "Ġwithin         : I --> ['str']\n",
      "Ġtheir          : I --> ['str']\n",
      "Ġterritory      : I --> ['str']\n",
      ".               : I --> ['str']\n",
      "ĠMember         : I --> ['str']\n",
      "ĠStates         : I --> ['str']\n",
      "Ġshall          : I --> ['str']\n",
      "Ġnotify         : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "ĠCommission     : I --> ['str']\n",
      "Ġof             : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "Ġcharacteristics : I --> ['str']\n",
      "Ġof             : I --> ['str']\n",
      "Ġthose          : I --> ['str']\n",
      "Ġnational       : I --> ['str']\n",
      "Ġ               : I --> ['str']\n",
      "Ċ               : I --> ['str']\n",
      "me              : I --> ['str']\n",
      "chan            : I --> ['str']\n",
      "isms            : I --> ['str']\n",
      ".               : I --> ['str']\n",
      "Ġ2              : I --> ['str']\n",
      ".               : I --> ['str']\n",
      "Ġ               : I --> ['str']\n",
      "Ċ               : I --> ['str']\n",
      "Member          : I --> ['str']\n",
      "ĠStates         : I --> ['str']\n",
      "Ġshall          : I --> ['str']\n",
      "Ġensure         : I --> ['str']\n",
      "Ġthat           : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "Ġinformation    : I --> ['str']\n",
      "Ġheld           : I --> ['str']\n",
      "Ġin             : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "Ġcentral        : I --> ['str']\n",
      "ised            : I --> ['str']\n",
      "Ġmechanisms     : I --> ['str']\n",
      "Ġreferred       : I --> ['str']\n",
      "Ġto             : I --> ['str']\n",
      "Ġin             : I --> ['str']\n",
      "Ġparagraph      : I --> ['str']\n",
      "Ġ1              : I --> ['str']\n",
      "Ġ               : I --> ['str']\n",
      "Ċ               : I --> ['str']\n",
      "of              : I --> ['str']\n",
      "Ġthis           : I --> ['str']\n",
      "ĠArticle        : I --> ['str']\n",
      "Ġis             : I --> ['str']\n",
      "Ġdirectly       : I --> ['str']\n",
      "Ġaccessible     : I --> ['str']\n",
      "Ġin             : I --> ['str']\n",
      "Ġan             : I --> ['str']\n",
      "Ġimmediate      : I --> ['str']\n",
      "Ġand            : I --> ['str']\n",
      "Ġunf            : I --> ['str']\n",
      "il              : I --> ['str']\n",
      "tered           : I --> ['str']\n",
      "Ġmanner         : I --> ['str']\n",
      "Ġto             : I --> ['str']\n",
      "Ġnational       : I --> ['str']\n",
      "ĠFI             : I --> ['str']\n",
      "Us              : I --> ['str']\n",
      ".               : I --> ['str']\n",
      "ĠThe            : I --> ['str']\n",
      "Ġinformation    : I --> ['str']\n",
      "Ġshall          : I --> ['str']\n",
      "Ġ               : I --> ['str']\n",
      "Ċ               : I --> ['str']\n",
      "also            : I --> ['str']\n",
      "Ġbe             : I --> ['str']\n",
      "Ġaccessible     : I --> ['str']\n",
      "Ġto             : I --> ['str']\n",
      "Ġnational       : I --> ['str']\n",
      "Ġcompetent      : I --> ['str']\n",
      "Ġauthorities    : I --> ['str']\n",
      "Ġfor            : I --> ['str']\n",
      "Ġfulfilling     : I --> ['str']\n",
      "Ġtheir          : I --> ['str']\n",
      "Ġobligations    : I --> ['str']\n",
      "Ġunder          : I --> ['str']\n",
      "Ġthis           : I --> ['str']\n",
      "ĠDirective      : I --> ['str']\n",
      ".               : I --> ['str']\n",
      "ĠMember         : I --> ['str']\n",
      "Ġ               : I --> ['str']\n",
      "Ċ               : I --> ['str']\n",
      "States          : I --> ['str']\n",
      "Ġshall          : I --> ['str']\n",
      "Ġensure         : I --> ['str']\n",
      "Ġthat           : I --> ['str']\n",
      "Ġany            : I --> ['str']\n",
      "ĠFI             : I --> ['str']\n",
      "U               : I --> ['str']\n",
      "Ġis             : I --> ['str']\n",
      "Ġable           : I --> ['str']\n",
      "Ġto             : I --> ['str']\n",
      "Ġprovide        : I --> ['str']\n",
      "Ġinformation    : I --> ['str']\n",
      "Ġheld           : I --> ['str']\n",
      "Ġin             : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "Ġcentral        : I --> ['str']\n",
      "ised            : I --> ['str']\n",
      "Ġmechanisms     : I --> ['str']\n",
      "Ġreferred       : I --> ['str']\n",
      "Ġto             : I --> ['str']\n",
      "Ġin             : I --> ['str']\n",
      "Ġ               : I --> ['str']\n",
      "Ċ               : I --> ['str']\n",
      "paragraph       : I --> ['str']\n",
      "Ġ1              : I --> ['str']\n",
      "Ġof             : I --> ['str']\n",
      "Ġthis           : I --> ['str']\n",
      "ĠArticle        : I --> ['str']\n",
      "Ġto             : I --> ['str']\n",
      "Ġany            : I --> ['str']\n",
      "Ġother          : I --> ['str']\n",
      "ĠFI             : I --> ['str']\n",
      "Us              : I --> ['str']\n",
      "Ġin             : I --> ['str']\n",
      "Ġa              : I --> ['str']\n",
      "Ġtimely         : I --> ['str']\n",
      "Ġmanner         : I --> ['str']\n",
      "Ġin             : I --> ['str']\n",
      "Ġaccordance     : I --> ['str']\n",
      "Ġwith           : I --> ['str']\n",
      "ĠArticle        : I --> ['str']\n",
      "Ġ53             : I --> ['str']\n",
      ".               : I --> ['str']\n",
      "Ġ3              : I --> ['str']\n",
      ".               : I --> ['str']\n",
      "Ġ               : I --> ['str']\n",
      "Ċ               : I --> ['str']\n",
      "The             : I --> ['str']\n",
      "Ġfollowing      : I --> ['str']\n",
      "Ġinformation    : I --> ['str']\n",
      "Ġshall          : I --> ['str']\n",
      "Ġbe             : I --> ['str']\n",
      "Ġaccessible     : I --> ['str']\n",
      "Ġand            : I --> ['str']\n",
      "Ġsearch         : I --> ['str']\n",
      "able            : I --> ['str']\n",
      "Ġthrough        : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "Ġcentral        : I --> ['str']\n",
      "ised            : I --> ['str']\n",
      "Ġmechanisms     : I --> ['str']\n",
      "Ġreferred       : I --> ['str']\n",
      "Ġto             : I --> ['str']\n",
      "Ġin             : I --> ['str']\n",
      "Ġ               : I --> ['str']\n",
      "Ċ               : I --> ['str']\n",
      "paragraph       : I --> ['str']\n",
      "Ġ1              : I --> ['str']\n",
      ":               : I --> ['str']\n",
      "ĠâĢĶ            : I --> ['str']\n",
      "Ġfor            : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "Ġcustomer       : I --> ['str']\n",
      "-               : I --> ['str']\n",
      "account         : I --> ['str']\n",
      "Ġholder         : I --> ['str']\n",
      "Ġand            : I --> ['str']\n",
      "Ġany            : I --> ['str']\n",
      "Ġperson         : I --> ['str']\n",
      "Ġpur            : I --> ['str']\n",
      "porting         : I --> ['str']\n",
      "Ġto             : I --> ['str']\n",
      "Ġact            : I --> ['str']\n",
      "Ġon             : I --> ['str']\n",
      "Ġbehalf         : I --> ['str']\n",
      "Ġof             : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "Ġcustomer       : I --> ['str']\n",
      ":               : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "Ġname           : I --> ['str']\n",
      ",               : I --> ['str']\n",
      "Ġ               : I --> ['str']\n",
      "Ċ               : I --> ['str']\n",
      "com             : I --> ['str']\n",
      "ple             : I --> ['str']\n",
      "mented          : I --> ['str']\n",
      "Ġby             : I --> ['str']\n",
      "Ġeither         : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "Ġother          : I --> ['str']\n",
      "Ġidentification : I --> ['str']\n",
      "Ġdata           : I --> ['str']\n",
      "Ġrequired       : I --> ['str']\n",
      "Ġunder          : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "Ġnational       : I --> ['str']\n",
      "Ġprovisions     : I --> ['str']\n",
      "Ġtrans          : I --> ['str']\n",
      "posing          : I --> ['str']\n",
      "Ġ               : I --> ['str']\n",
      "Ċ               : I --> ['str']\n",
      "point           : I --> ['str']\n",
      "Ġ(              : I --> ['str']\n",
      "a               : I --> ['str']\n",
      ")               : I --> ['str']\n",
      "Ġof             : I --> ['str']\n",
      "ĠArticle        : I --> ['str']\n",
      "Ġ13             : I --> ['str']\n",
      "(               : I --> ['str']\n",
      "1               : I --> ['str']\n",
      ")               : I --> ['str']\n",
      "Ġor             : I --> ['str']\n",
      "Ġa              : I --> ['str']\n",
      "Ġunique         : I --> ['str']\n",
      "Ġidentification : I --> ['str']\n",
      "Ġnumber         : I --> ['str']\n",
      ";               : I --> ['str']\n",
      "ĠâĢĶ            : I --> ['str']\n",
      "Ġfor            : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "Ġbeneficial     : I --> ['str']\n",
      "Ġowner          : I --> ['str']\n",
      "Ġof             : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "Ġcustomer       : I --> ['str']\n",
      "-               : I --> ['str']\n",
      "account         : I --> ['str']\n",
      "Ġholder         : I --> ['str']\n",
      ":               : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "Ġname           : I --> ['str']\n",
      ",               : I --> ['str']\n",
      "Ġcomple         : I --> ['str']\n",
      "mented          : I --> ['str']\n",
      "Ġby             : I --> ['str']\n",
      "Ġeither         : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "Ġother          : I --> ['str']\n",
      "Ġident          : I --> ['str']\n",
      "i               : I --> ['str']\n",
      "ÂŃ              : I --> ['str']\n",
      "Ċ               : I --> ['str']\n",
      "f               : I --> ['str']\n",
      "ication         : I --> ['str']\n",
      "Ġdata           : I --> ['str']\n",
      "Ġrequired       : I --> ['str']\n",
      "Ġunder          : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "Ġnational       : I --> ['str']\n",
      "Ġprovisions     : I --> ['str']\n",
      "Ġtrans          : I --> ['str']\n",
      "posing          : I --> ['str']\n",
      "Ġpoint          : I --> ['str']\n",
      "Ġ(              : I --> ['str']\n",
      "b               : I --> ['str']\n",
      ")               : I --> ['str']\n",
      "Ġof             : I --> ['str']\n",
      "ĠArticle        : I --> ['str']\n",
      "Ġ13             : I --> ['str']\n",
      "(               : I --> ['str']\n",
      "1               : I --> ['str']\n",
      ")               : I --> ['str']\n",
      "Ġor             : I --> ['str']\n",
      "Ġa              : I --> ['str']\n",
      "Ġunique         : I --> ['str']\n",
      "Ġident          : I --> ['str']\n",
      "i               : I --> ['str']\n",
      "ÂŃ              : I --> ['str']\n",
      "Ċ               : I --> ['str']\n",
      "f               : I --> ['str']\n",
      "ication         : I --> ['str']\n",
      "Ġnumber         : I --> ['str']\n",
      ";               : I --> ['str']\n",
      "ĠâĢĶ            : I --> ['str']\n",
      "Ġfor            : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "Ġbank           : I --> ['str']\n",
      "Ġor             : I --> ['str']\n",
      "Ġpayment        : I --> ['str']\n",
      "Ġaccount        : I --> ['str']\n",
      ":               : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "ĠIB             : I --> ['str']\n",
      "AN              : I --> ['str']\n",
      "Ġnumber         : I --> ['str']\n",
      "Ġand            : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "Ġdate           : I --> ['str']\n",
      "Ġof             : I --> ['str']\n",
      "Ġaccount        : I --> ['str']\n",
      "Ġopening        : I --> ['str']\n",
      "Ġand            : I --> ['str']\n",
      "Ġclosing        : I --> ['str']\n",
      ";               : I --> ['str']\n",
      "ĠâĢĶ            : I --> ['str']\n",
      "Ġfor            : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "Ġsafe           : I --> ['str']\n",
      "-               : I --> ['str']\n",
      "dep             : I --> ['str']\n",
      "osit            : I --> ['str']\n",
      "Ġbox            : I --> ['str']\n",
      ":               : I --> ['str']\n",
      "Ġname           : I --> ['str']\n",
      "Ġof             : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "Ġles            : I --> ['str']\n",
      "see             : I --> ['str']\n",
      "Ġcomple         : I --> ['str']\n",
      "mented          : I --> ['str']\n",
      "Ġby             : I --> ['str']\n",
      "Ġeither         : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "Ġother          : I --> ['str']\n",
      "Ġidentification : I --> ['str']\n",
      "Ġdata           : I --> ['str']\n",
      "Ġrequired       : I --> ['str']\n",
      "Ġunder          : I --> ['str']\n",
      "Ġ               : I --> ['str']\n",
      "Ċ               : I --> ['str']\n",
      "the             : I --> ['str']\n",
      "Ġnational       : I --> ['str']\n",
      "Ġprovisions     : I --> ['str']\n",
      "Ġtrans          : I --> ['str']\n",
      "posing          : I --> ['str']\n",
      "ĠArticle        : I --> ['str']\n",
      "Ġ13             : I --> ['str']\n",
      "(               : I --> ['str']\n",
      "1               : I --> ['str']\n",
      ")               : I --> ['str']\n",
      "Ġor             : I --> ['str']\n",
      "Ġa              : I --> ['str']\n",
      "Ġunique         : I --> ['str']\n",
      "Ġidentification : I --> ['str']\n",
      "Ġnumber         : I --> ['str']\n",
      "Ġand            : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "Ġduration       : I --> ['str']\n",
      "Ġof             : I --> ['str']\n",
      "Ġthe            : I --> ['str']\n",
      "Ġlease          : I --> ['str']\n",
      "Ġ               : I --> ['str']\n",
      "Ċ               : I --> ['str']\n",
      "period          : I --> ['str']\n",
      ".               : I --> ['str']\n",
      "Ġ4              : I --> ['str']\n",
      ".               : I --> ['str']\n",
      "Ġ               : I --> ['str']\n",
      "Ċ               : I --> ['str']\n",
      "Member          : I --> ['str']\n",
      "ĠStates         : I --> ['str']\n",
      "Ġmay            : O --> ['str']\n",
      "Ġconsider       : O --> ['str']\n",
      "Ġrequiring      : O --> ['str']\n",
      "Ġother          : O --> ['str']\n",
      "Ġinformation    : I --> ['str']\n",
      "Ġdeemed         : I --> ['str']\n",
      "Ġessential      : I --> ['str']\n",
      "Ġfor            : I --> ['str']\n",
      "ĠFI             : I --> ['str']\n",
      "Us              : I --> ['str']\n",
      "Ġand            : I --> ['str']\n",
      "Ġcompetent      : O --> ['str']\n",
      "Ġauthorities    : O --> ['str']\n",
      "Ġ               : O --> ['str']\n",
      "Ċ               : O --> ['str']\n",
      "for             : O --> ['str']\n",
      "Ġfulfilling     : O --> ['str']\n",
      "Ġtheir          : I --> ['str']\n",
      "Ġobligations    : O --> ['str']\n",
      "Ġunder          : O --> ['str']\n",
      "Ġthis           : O --> ['str']\n",
      "ĠDirective      : I --> ['str']\n",
      "Ġto             : O --> ['str']\n",
      "Ġbe             : O --> ['str']\n",
      "Ġaccessible     : I --> ['str']\n",
      "Ġand            : O --> ['str']\n",
      "Ġsearch         : O --> ['str']\n",
      "able            : O --> ['str']\n",
      "Ġthrough        : O --> ['str']\n",
      "Ġthe            : O --> ['str']\n",
      "Ġcentral        : I --> ['str']\n",
      "ised            : O --> ['str']\n",
      "Ġmech           : O --> ['str']\n",
      "ÂŃ              : O --> ['str']\n",
      "Ċ               : O --> ['str']\n",
      "an              : O --> ['str']\n",
      "isms            : O --> ['str']\n",
      ".               : O --> ['str']\n",
      "Ġ5              : O --> ['str']\n",
      ".               : I --> ['str']\n",
      "Ġ               : O --> ['str']\n",
      "Ċ               : O --> ['str']\n",
      "By              : O --> ['str']\n",
      "Ġ26             : O --> ['str']\n",
      "ĠJune           : O --> ['str']\n",
      "Ġ2020           : O --> ['str']\n",
      ",               : O --> ['str']\n",
      "Ġthe            : O --> ['str']\n",
      "ĠCommission     : O --> ['str']\n",
      "Ġshall          : O --> ['str']\n",
      "Ġsubmit         : O --> ['str']\n",
      "Ġa              : O --> ['str']\n",
      "Ġreport         : O --> ['str']\n",
      "Ġto             : O --> ['str']\n",
      "Ġthe            : O --> ['str']\n",
      "ĠEuropean       : O --> ['str']\n",
      "ĠParliament     : O --> ['str']\n",
      "Ġand            : O --> ['str']\n",
      "Ġto             : O --> ['str']\n",
      "Ġthe            : O --> ['str']\n",
      "ĠCouncil        : O --> ['str']\n",
      "Ġ               : O --> ['str']\n",
      "Ċ               : O --> ['str']\n",
      "ass             : O --> ['str']\n",
      "essing          : O --> ['str']\n",
      "Ġthe            : O --> ['str']\n",
      "Ġconditions     : O --> ['str']\n",
      "Ġand            : O --> ['str']\n",
      "Ġthe            : O --> ['str']\n",
      "Ġtechnical      : O --> ['str']\n",
      "Ġspecifications : O --> ['str']\n",
      "Ġand            : O --> ['str']\n",
      "Ġprocedures     : O --> ['str']\n",
      "Ġfor            : O --> ['str']\n",
      "Ġensuring       : O --> ['str']\n",
      "Ġsecure         : O --> ['str']\n",
      "Ġand            : O --> ['str']\n",
      "Ġefficient      : O --> ['str']\n",
      "Ġinter          : O --> ['str']\n",
      "ÂŃ              : O --> ['str']\n",
      "Ċ               : O --> ['str']\n",
      "connection      : O --> ['str']\n",
      "Ġof             : O --> ['str']\n",
      "Ġthe            : O --> ['str']\n",
      "Ġcentral        : O --> ['str']\n",
      "ised            : O --> ['str']\n",
      "Ġautomated      : O --> ['str']\n",
      "Ġmechanisms     : O --> ['str']\n",
      ".               : I --> ['str']\n",
      "ĠWhere          : O --> ['str']\n",
      "Ġappropriate    : O --> ['str']\n",
      ",               : O --> ['str']\n",
      "Ġthat           : O --> ['str']\n",
      "Ġreport         : O --> ['str']\n",
      "Ġshall          : O --> ['str']\n",
      "Ġbe             : O --> ['str']\n",
      "Ġaccompanied    : O --> ['str']\n",
      "Ġby             : O --> ['str']\n",
      "Ġa              : O --> ['str']\n",
      "Ġ               : O --> ['str']\n",
      "Ċ               : O --> ['str']\n",
      "leg             : O --> ['str']\n",
      "isl             : O --> ['str']\n",
      "ative           : O --> ['str']\n",
      "Ġproposal       : O --> ['str']\n",
      ".               : I --> ['str']\n",
      "Ġ               : O --> ['str']\n",
      "________        : O --> ['str']\n",
      "___             : O --> ['str']\n",
      "Ġ               : O --> ['str']\n",
      "Ċ               : O --> ['str']\n",
      "(               : O --> ['str']\n",
      "*)              : O --> ['str']\n",
      "ĠRegulation     : O --> ['str']\n",
      "Ġ(              : O --> ['str']\n",
      "EU              : O --> ['str']\n",
      ")               : O --> ['str']\n",
      "ĠNo             : O --> ['str']\n",
      "Ġ260            : O --> ['str']\n",
      "/               : O --> ['str']\n",
      "2012            : O --> ['str']\n",
      "Ġof             : O --> ['str']\n",
      "Ġthe            : O --> ['str']\n",
      "ĠEuropean       : O --> ['str']\n",
      "ĠParliament     : O --> ['str']\n",
      "Ġand            : O --> ['str']\n",
      "Ġof             : O --> ['str']\n",
      "Ġthe            : O --> ['str']\n",
      "ĠCouncil        : O --> ['str']\n",
      "Ġof             : O --> ['str']\n",
      "Ġ14             : O --> ['str']\n",
      "ĠMarch          : O --> ['str']\n",
      "Ġ2012           : O --> ['str']\n",
      "Ġestablishing   : O --> ['str']\n",
      "Ġ               : O --> ['str']\n",
      "Ċ               : O --> ['str']\n",
      "technical       : O --> ['str']\n",
      "Ġand            : O --> ['str']\n",
      "Ġbusiness       : O --> ['str']\n",
      "Ġrequirements   : O --> ['str']\n",
      "Ġfor            : O --> ['str']\n",
      "Ġcredit         : O --> ['str']\n",
      "Ġtransfers      : O --> ['str']\n",
      "Ġand            : O --> ['str']\n",
      "Ġdirect         : O --> ['str']\n",
      "Ġdeb            : O --> ['str']\n",
      "its             : O --> ['str']\n",
      "Ġin             : O --> ['str']\n",
      "Ġeuro           : O --> ['str']\n",
      "Ġand            : O --> ['str']\n",
      "Ġam             : O --> ['str']\n",
      "ending          : O --> ['str']\n",
      "ĠRegulation     : O --> ['str']\n",
      "Ġ(              : O --> ['str']\n",
      "EC              : O --> ['str']\n",
      ")               : O --> ['str']\n",
      "Ġ               : O --> ['str']\n",
      "Ċ               : O --> ['str']\n",
      "No              : O --> ['str']\n",
      "Ġ9              : O --> ['str']\n",
      "24              : O --> ['str']\n",
      "/               : O --> ['str']\n",
      "2009            : O --> ['str']\n",
      "Ġ(              : O --> ['str']\n",
      "O               : O --> ['str']\n",
      "J               : O --> ['str']\n",
      "ĠL              : O --> ['str']\n",
      "Ġ94             : O --> ['str']\n",
      ",               : O --> ['str']\n",
      "Ġ30             : O --> ['str']\n",
      ".               : O --> ['str']\n",
      "3               : O --> ['str']\n",
      ".               : O --> ['str']\n",
      "2012            : O --> ['str']\n",
      ",               : O --> ['str']\n",
      "Ġp              : O --> ['str']\n",
      ".               : O --> ['str']\n",
      "Ġ22             : O --> ['str']\n",
      ").              : O --> ['str']\n",
      "âĢ              : O --> ['str']\n",
      "Ļ               : O --> ['str']\n",
      ";               : O --> ['str']\n",
      "Ġ(              : O --> ['str']\n",
      "20              : O --> ['str']\n",
      ")               : O --> ['str']\n",
      "Ġthe            : O --> ['str']\n",
      "Ġfollowing      : O --> ['str']\n",
      "ĠArticle        : O --> ['str']\n",
      "Ġis             : O --> ['str']\n",
      "Ġinserted       : O --> ['str']\n",
      ":               : O --> ['str']\n",
      "ĠâĢ             : O --> ['str']\n",
      "ĺ               : O --> ['str']\n",
      "Article         : O --> ['str']\n",
      "Ġ32             : O --> ['str']\n",
      "b               : O --> ['str']\n",
      "Ġ1              : O --> ['str']\n",
      ".               : O --> ['str']\n",
      "Ġ               : O --> ['str']\n",
      "Ċ               : O --> ['str']\n",
      "Member          : O --> ['str']\n",
      "ĠStates         : O --> ['str']\n",
      "Ġshall          : O --> ['str']\n",
      "Ġprovide        : O --> ['str']\n",
      "ĠFI             : O --> ['str']\n",
      "Us              : O --> ['str']\n",
      "Ġand            : O --> ['str']\n",
      "Ġcompetent      : O --> ['str']\n",
      "Ġauthorities    : O --> ['str']\n",
      "Ġwith           : O --> ['str']\n",
      "Ġaccess         : O --> ['str']\n",
      "Ġto             : O --> ['str']\n",
      "Ġinformation    : O --> ['str']\n",
      "Ġwhich          : O --> ['str']\n",
      "Ġallows         : O --> ['str']\n",
      "Ġthe            : O --> ['str']\n",
      "Ġ               : O --> ['str']\n",
      "Ċ               : O --> ['str']\n",
      "ident           : O --> ['str']\n",
      "ification       : O --> ['str']\n",
      "Ġin             : O --> ['str']\n",
      "Ġa              : O --> ['str']\n",
      "Ġtimely         : O --> ['str']\n",
      "Ġmanner         : O --> ['str']\n",
      "Ġof             : O --> ['str']\n",
      "Ġany            : O --> ['str']\n",
      "Ġnatural        : O --> ['str']\n",
      "Ġor             : O --> ['str']\n",
      "Ġlegal          : O --> ['str']\n",
      "Ġpersons        : O --> ['str']\n",
      "Ġowning         : O --> ['str']\n",
      "Ġreal           : O --> ['str']\n",
      "Ġestate         : O --> ['str']\n",
      ",               : O --> ['str']\n",
      "Ġincluding      : O --> ['str']\n",
      "Ġthrough        : O --> ['str']\n",
      "Ġregisters      : O --> ['str']\n",
      "Ġor             : O --> ['str']\n",
      "Ġ               : O --> ['str']\n",
      "Ċ               : O --> ['str']\n",
      "elect           : O --> ['str']\n",
      "ronic           : O --> ['str']\n",
      "Ġdata           : O --> ['str']\n",
      "Ġretrieval      : O --> ['str']\n",
      "Ġsystems        : O --> ['str']\n",
      "Ġwhere          : O --> ['str']\n",
      "Ġsuch           : O --> ['str']\n",
      "Ġregisters      : O --> ['str']\n",
      "Ġor             : O --> ['str']\n",
      "Ġsystems        : O --> ['str']\n",
      "Ġare            : O --> ['str']\n",
      "Ġavailable      : O --> ['str']\n",
      ".               : O --> ['str']\n",
      "Ġ2              : O --> ['str']\n",
      ".               : O --> ['str']\n",
      "Ġ               : O --> ['str']\n",
      "Ċ               : O --> ['str']\n",
      "By              : O --> ['str']\n",
      "Ġ31             : O --> ['str']\n",
      "ĠDecember       : O --> ['str']\n",
      "Ġ2020           : O --> ['str']\n",
      ",               : O --> ['str']\n",
      "Ġthe            : O --> ['str']\n",
      "ĠCommission     : O --> ['str']\n",
      "Ġshall          : O --> ['str']\n",
      "Ġsubmit         : O --> ['str']\n",
      "Ġa              : O --> ['str']\n",
      "Ġreport         : O --> ['str']\n",
      "Ġto             : O --> ['str']\n",
      "Ġthe            : O --> ['str']\n",
      "ĠEuropean       : O --> ['str']\n",
      "ĠParliament     : O --> ['str']\n",
      "Ġand            : O --> ['str']\n",
      "Ġto             : O --> ['str']\n",
      "Ġthe            : O --> ['str']\n",
      "ĠCouncil        : O --> ['str']\n",
      "Ġ               : O --> ['str']\n",
      "Ċ               : O --> ['str']\n",
      "ass             : O --> ['str']\n",
      "essing          : O --> ['str']\n",
      "Ġthe            : O --> ['str']\n",
      "Ġnecessity      : O --> ['str']\n",
      "Ġand            : O --> ['str']\n",
      "Ġproportion     : O --> ['str']\n",
      "ality           : O --> ['str']\n",
      "Ġof             : O --> ['str']\n",
      "Ġharmon         : O --> ['str']\n",
      "ising           : O --> ['str']\n",
      "Ġthe            : O --> ['str']\n",
      "Ġinformation    : O --> ['str']\n",
      "Ġincluded       : O --> ['str']\n",
      "Ġin             : O --> ['str']\n",
      "Ġthe            : O --> ['str']\n",
      "Ġregisters      : O --> ['str']\n",
      "Ġand            : O --> ['str']\n",
      "Ġassessing      : O --> ['str']\n",
      "Ġ               : O --> ['str']\n",
      "Ċ               : O --> ['str']\n",
      "the             : O --> ['str']\n",
      "Ġneed           : O --> ['str']\n",
      "Ġfor            : O --> ['str']\n",
      "Ġthe            : O --> ['str']\n",
      "Ġinter          : O --> ['str']\n",
      "connection      : O --> ['str']\n",
      "Ġof             : O --> ['str']\n",
      "Ġthose          : O --> ['str']\n",
      "Ġregisters      : O --> ['str']\n",
      ".               : O --> ['str']\n",
      "ĠWhere          : O --> ['str']\n",
      "Ġappropriate    : O --> ['str']\n",
      ",               : O --> ['str']\n",
      "Ġthat           : O --> ['str']\n",
      "Ġreport         : O --> ['str']\n",
      "Ġshall          : O --> ['str']\n",
      "Ġbe             : O --> ['str']\n",
      "Ġaccompanied    : O --> ['str']\n",
      "Ġby             : O --> ['str']\n",
      "Ġa              : O --> ['str']\n",
      "Ġ               : O --> ['str']\n",
      "Ċ               : O --> ['str']\n",
      "leg             : O --> ['str']\n",
      "isl             : O --> ['str']\n",
      "ative           : O --> ['str']\n",
      "Ġproposal       : O --> ['str']\n",
      ".               : O --> ['str']\n",
      "âĢ              : O --> ['str']\n",
      "Ļ               : O --> ['str']\n",
      ";               : O --> ['str']\n",
      "ĠEN             : O --> ['str']\n",
      "Ġ               : O --> ['str']\n",
      "Ċ               : O --> ['str']\n",
      "L               : O --> ['str']\n",
      "Ġ156            : O --> ['str']\n",
      "/               : O --> ['str']\n",
      "64              : O --> ['str']\n",
      "Ġ               : O --> ['str']\n",
      "Ċ               : O --> ['str']\n",
      "Official        : O --> ['str']\n",
      "ĠJournal        : O --> ['str']\n",
      "Ġof             : O --> ['str']\n",
      "Ġthe            : O --> ['str']\n",
      "ĠEuropean       : O --> ['str']\n",
      "ĠUnion          : O --> ['str']\n",
      "Ġ               : O --> ['str']\n",
      "Ċ               : O --> ['str']\n",
      "19              : O --> ['str']\n",
      ".               : O --> ['str']\n",
      "6               : O --> ['str']\n",
      ".               : O --> ['str']\n",
      "2018            : O --> ['str']\n",
      "---------------------- Snippet 1 | Label: str ----------------------\n",
      "(18) in Article 32 the following paragraph is added: ‘9. \n",
      "Without prejudice to Article 34(2), in the context of its functions, each FIU shall be able to request, obtain and \n",
      "use information from any obliged entity for the purpose set in paragraph 1 of this Article, even if no prior report is \n",
      "filed pursuant to Article 33(1)(a) or 34(1).’; (19) the following Article is inserted: ‘Article 32a 1. \n",
      "Member States shall put in place centralised automated mechanisms, such as central registries or central \n",
      "electronic data retrieval systems, which allow the identification, in a timely manner, of any natural or legal \n",
      "persons holding or controlling payment accounts and bank accounts identified by IBAN, as defined by Regulation \n",
      "(EU) No 260/2012 of the European Parliament and of the Council (*), and safe-deposit boxes held by a credit \n",
      "institution within their territory. Member States shall notify the Commission of the characteristics of those national \n",
      "mechanisms. 2. \n",
      "Member States shall ensure that the information held in the centralised mechanisms referred to in paragraph 1 \n",
      "of this Article is directly accessible in an immediate and unfiltered manner to national FIUs. The information shall \n",
      "also be accessible to national competent authorities for fulfilling their obligations under this Directive. Member \n",
      "States shall ensure that any FIU is able to provide information held in the centralised mechanisms referred to in \n",
      "paragraph 1 of this Article to any other FIUs in a timely manner in accordance with Article 53. 3. \n",
      "The following information shall be accessible and searchable through the centralised mechanisms referred to in \n",
      "paragraph 1: — for the customer-account holder and any person purporting to act on behalf of the customer: the name, \n",
      "complemented by either the other identification data required under the national provisions transposing \n",
      "point (a) of Article 13(1) or a unique identification number; — for the beneficial owner of the customer-account holder: the name, complemented by either the other identi­\n",
      "fication data required under the national provisions transposing point (b) of Article 13(1) or a unique identi­\n",
      "fication number; — for the bank or payment account: the IBAN number and the date of account opening and closing; — for the safe-deposit box: name of the lessee complemented by either the other identification data required under \n",
      "the national provisions transposing Article 13(1) or a unique identification number and the duration of the lease \n",
      "period. 4. \n",
      "Member States may consider requiring other information deemed essential for FIUs and competent authorities \n",
      "for fulfilling their obligations under this Directive to be accessible and searchable through the central\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "from collections import defaultdict\n",
    "\n",
    "def default_page():\n",
    "    return {'blocks': [], 'full_text': \"\"}\n",
    "\n",
    "def create_adjusted_dict(pdf_path: str, doc_id: str):\n",
    "    '''\n",
    "    Read custom pdf using PyMuPDF (alias fitz) and create a dictionary with the same structure as the data_dict\n",
    "    '''\n",
    "    pdf_data_dict = defaultdict(lambda: defaultdict(default_page))\n",
    "\n",
    "    # Open PDF file\n",
    "    doc = fitz.open(pdf_path)\n",
    "    # Process each page\n",
    "    for page_index, page in enumerate(doc):\n",
    "        # Extract text blocks from the page\n",
    "        blocks = page.get_text(\"blocks\")\n",
    "        # Process each block\n",
    "        block_texts = []\n",
    "        for block in blocks:\n",
    "            block_text = block[4].strip()  # Extract the text from the block\n",
    "            block_texts.append(block_text)\n",
    "            pdf_data_dict[doc_id][str(page_index)]['blocks'].append({'text': block_text})\n",
    "        full_page_text = ' '.join([block_text for block_text in block_texts if block_text is not None])\n",
    "        pdf_data_dict[doc_id][str(page_index)]['full_text'] = full_page_text\n",
    "    return pdf_data_dict\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "page_index = 21\n",
    "file_index = 2 # easy indexing in example pdfs folder\n",
    "\n",
    "pdf_files = [file for file in os.listdir(\"../Example_PDFs\") if file.endswith('.pdf')]\n",
    "print(pdf_files)\n",
    "pdf_path = os.path.join(\"../Example_PDFs\", pdf_files[file_index])\n",
    "\n",
    "# create dict for local file\n",
    "adjusted_dict = create_adjusted_dict(pdf_path, 'test_pdf')\n",
    "page_text = adjusted_dict[\"test_pdf\"][str(page_index)]['full_text']\n",
    "snippets = display_token_predictions_and_create_snippets(token_model, tokenizer, page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['22396093', '22371945', '19180743', '22683853', '20518933', '19534497', '20575785', '19507185', '26369135', '19586509', '19073734', '17459693', '20545737', '20529447', '22243253', '19592316', '20202989', '22057835', '21450084', '20753115']\n"
     ]
    }
   ],
   "source": [
    "def get_sample_doc_page_ids_from_test_set(test_dataset, num_samples=20):\n",
    "    '''\n",
    "    Helper function to exract IDs from dataset\n",
    "    '''\n",
    "    sample_doc_page_ids = []\n",
    "    sample_doc_ids = set()\n",
    "\n",
    "    for data_point in test_dataset:\n",
    "        metadata = data_point['metadata']\n",
    "        doc_id = metadata['doc_id']\n",
    "        page_id = metadata['page_id']\n",
    "        \n",
    "        if doc_id not in sample_doc_ids:\n",
    "            sample_doc_ids.add(doc_id)\n",
    "            sample_doc_page_ids.append((doc_id, page_id))\n",
    "        \n",
    "        if len(sample_doc_ids) >= num_samples:\n",
    "            break\n",
    "\n",
    "    return sample_doc_page_ids, list(sample_doc_ids)\n",
    "\n",
    "sample_doc_page_ids, sample_doc_ids = get_sample_doc_page_ids_from_test_set(test_dataset, 20)\n",
    "print(sample_doc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct Predictions on Test Pages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect Documents and pages from test dataset for easy access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26ca832e860e41a0b86881650cf5832e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "test_doc_pages_overview = defaultdict(list)\n",
    "for sample in tqdm(dataset_dict['test']):\n",
    "    doc_id = sample['metadata']['doc_id']\n",
    "    page_index = sample['metadata']['page_id']\n",
    "    if page_index not in test_doc_pages_overview[doc_id]:\n",
    "        test_doc_pages_overview[doc_id].append(page_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to predict and process snippets in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from functools import partial\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "\n",
    "import importlib\n",
    "importlib.reload(detailed_labels_handler)\n",
    "\n",
    "def process_chunk(data, batch_input_ids, predicted_labels, detailed_predictions, tokenizer, stride, min_snippet_length, ignore_o_threshold):\n",
    "    '''\n",
    "    Worker function to process a chunk of data and create snippets\n",
    "    '''\n",
    "    index, encodings = data\n",
    "    merged_tokens = []\n",
    "    merged_tags = []\n",
    "    merged_detailed_tags = []\n",
    "    current_idx = index\n",
    "\n",
    "    # Merge tokens and tags from all windows back together\n",
    "    for window_idx, window in enumerate(encodings):\n",
    "        start_index = 1 if window_idx == 0 else stride + 1\n",
    "        end_index = -1 if window_idx < len(encodings) - 1 else None\n",
    "\n",
    "        tokens = tokenizer.convert_ids_to_tokens(batch_input_ids[current_idx])\n",
    "        window_tags = [index_to_label[label.item()] for label in predicted_labels[current_idx][start_index:end_index]]\n",
    "        window_tokens = [tok for tok in tokens[start_index:end_index] if tok not in [cls_token, sep_token, pad_token]]\n",
    "        window_detailed_tags = detailed_predictions[start_index:end_index] \n",
    "\n",
    "        merged_tokens.extend(window_tokens)\n",
    "        merged_tags.extend(window_tags)\n",
    "        merged_detailed_tags.extend(window_detailed_tags)\n",
    "        current_idx += 1\n",
    "\n",
    "    # Create snippets for page (i.e. merged windows)\n",
    "    snippets = create_snippets(merged_tokens, merged_tags, tokenizer, merged_detailed_tags=merged_detailed_tags, min_snippet_length=min_snippet_length, ignore_o_threshold=ignore_o_threshold)\n",
    "    return snippets\n",
    "\n",
    "def create_snippets_multiprocessing(all_encodings, batch_input_ids, predicted_labels\n",
    "                                    , detailed_predictions, tokenizer, stride\n",
    "                                    , min_snippet_length, ignore_o_threshold, workers, chunksize):\n",
    "    '''\n",
    "    Create snippets from the predictions using the custom post-processing algorithm with multiprocessing\n",
    "    '''\n",
    "    \n",
    "    # Partial function to pass arguments to process_chunk (easier handling for multiprocessing)\n",
    "    partial_process_chunk = partial(\n",
    "        process_chunk, \n",
    "        batch_input_ids=batch_input_ids, \n",
    "        predicted_labels=predicted_labels,\n",
    "        detailed_predictions=detailed_predictions,\n",
    "        tokenizer=tokenizer, \n",
    "        stride=stride, \n",
    "        min_snippet_length=min_snippet_length, \n",
    "        ignore_o_threshold=ignore_o_threshold\n",
    "    )\n",
    "\n",
    "    # Creating index and encodings pair for proper order (very important for correct snippet creation)\n",
    "    data = []\n",
    "    encding_idx = 0\n",
    "    current_idx = 0\n",
    "    while encding_idx < len(all_encodings):\n",
    "        encodings = all_encodings[encding_idx]\n",
    "        num_windows = len(encodings)\n",
    "        data.append((current_idx, encodings))\n",
    "        current_idx += num_windows\n",
    "        encding_idx += 1\n",
    "    \n",
    "    results = process_map(partial_process_chunk, data, max_workers=workers, chunksize=chunksize, desc=\"Creating snippets\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def batch_predict(model, tokenizer, texts, metadata, window_size=512, stride=256, sub_batch_size=256):\n",
    "    '''\n",
    "    Process multiple texts in batches and create predictions\n",
    "    Window size and stride/overap are used to handle long texts.\n",
    "    '''\n",
    "    \n",
    "    adjusted_window_size = window_size - 2 # account for special tokens\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Tokenize all texts with stride handling to get multiple windows for each text, considering overlap\n",
    "    # Prepare inputs for model\n",
    "    all_inputs = {\"input_ids\": [], \"attention_mask\": [], \"metadata\": []}\n",
    "    all_encodings = []\n",
    "    max_length = 0 \n",
    "    for text, meta in tqdm(zip(texts, metadata), desc=\"Tokenizing\", total=len(texts)):\n",
    "        tokenized_text = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=adjusted_window_size, return_overflowing_tokens=True, stride=stride, add_special_tokens=True)\n",
    "        all_encodings.append(tokenized_text.encodings)\n",
    "        for window in tokenized_text.encodings:\n",
    "            tensor_ids = torch.tensor([window.ids]).to(model.device)\n",
    "            all_inputs[\"input_ids\"].append(tensor_ids)\n",
    "            all_inputs[\"attention_mask\"].append(torch.tensor([window.attention_mask]).to(model.device))\n",
    "            all_inputs[\"metadata\"].append(meta)\n",
    "            max_length = max(max_length, tensor_ids.size(1))\n",
    "    \n",
    "    # Apply padding to ensure uniform length within batch\n",
    "    padded_input_ids = [F.pad(tensor, (0, max_length - tensor.shape[1]), 'constant', 0) for tensor in all_inputs[\"input_ids\"]] # pad difference between max_length and current length with 0\n",
    "    padded_attention_mask = [F.pad(tensor, (0, max_length - tensor.shape[1]), 'constant', 0) for tensor in all_inputs[\"attention_mask\"]] # pad difference between max_length and current length with 0\n",
    "    \n",
    "    # Convert list of tensors to a single tensor for batch processing\n",
    "    batch_input_ids = torch.cat(padded_input_ids, dim=0)\n",
    "    batch_attention_mask = torch.cat(padded_attention_mask, dim=0)\n",
    "    batch_inputs = {\"input_ids\": batch_input_ids, \"attention_mask\": batch_attention_mask}\n",
    "\n",
    "    print(\"Number of samples:\", len(padded_attention_mask))\n",
    "    \n",
    "    # Model predictions in one go\n",
    "    model.eval()\n",
    "    \n",
    "    # Calculate bacthes as number of samples divided by sub_batch_size + 1 if there is a remainder\n",
    "    total_batches = len(batch_input_ids) // sub_batch_size + (len(batch_input_ids) % sub_batch_size != 0)\n",
    "    \n",
    "    predicted_labels = [] \n",
    "    detailed_predictions = [] \n",
    "    batch_metadata = []\n",
    "    with tqdm(total=total_batches, desc=\"Predicting\") as pbar:\n",
    "        for i in range(0, len(batch_input_ids), sub_batch_size):\n",
    "            \n",
    "            # Prepare sub-batch\n",
    "            sub_input_ids = batch_input_ids[i:i+sub_batch_size]\n",
    "            sub_attention_mask = batch_attention_mask[i:i+sub_batch_size]\n",
    "            sub_metadata = all_inputs[\"metadata\"][i:i+sub_batch_size]\n",
    "            batch_inputs = {\"input_ids\": sub_input_ids, \"attention_mask\": sub_attention_mask}\n",
    "\n",
    "            # Make predictions with token-level model\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch_inputs)\n",
    "            logits = outputs[0]\n",
    "            labels = torch.argmax(logits, dim=2)\n",
    "            predicted_labels.extend(labels)\n",
    "            batch_metadata.extend(sub_metadata) \n",
    "            \n",
    "            # Get detailed labels (auxiliary objective)\n",
    "            if len(outputs) > 1 and outputs[1] is not None:\n",
    "                    detailed_logits = outputs[1]\n",
    "                    level_predictions = [torch.argmax(level_logits, dim=2) for level_logits in detailed_logits]\n",
    "                    merged_predictions = torch.stack(level_predictions, dim=2)\n",
    "                    detailed_predictions.extend(merged_predictions)\n",
    "\n",
    "            pbar.update()\n",
    "    \n",
    "    # Decode detailed labels using helper function\n",
    "    decoded_detailed_labels = [\n",
    "        detailed_labels_handler.decode_detailed_labels(sample_predictions.tolist(), mapping_dicts, HIER_LABELS_LEVELS) \n",
    "        for sample_predictions in detailed_predictions\n",
    "    ]\n",
    "    \n",
    "    # Result with metadata (used for debugging)\n",
    "    results_with_metadata = list(zip(batch_metadata, predicted_labels, decoded_detailed_labels))\n",
    "\n",
    "    return all_encodings, batch_input_ids, predicted_labels, decoded_detailed_labels, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "doc_count = 0\n",
    "page_count = 0\n",
    "\n",
    "# Step 1: Collect all texts and metadata for batch processing\n",
    "all_texts = []\n",
    "metadata = []\n",
    "for doc_id in tqdm(test_doc_pages_overview):\n",
    "    for page_index in test_doc_pages_overview[doc_id]:\n",
    "        page_text = data_dict[doc_id][page_index]['full_text']\n",
    "        all_texts.append(page_text)\n",
    "        metadata.append((doc_id, page_index))\n",
    "        page_count += 1\n",
    "    doc_count += 1\n",
    "    #if doc_count >= 10:\n",
    "        #break\n",
    "\n",
    "# Step 2: Process all texts in batches\n",
    "stride = OVERLAP\n",
    "sub_batch_size = 512 if MODEL == \"roberta-large\" else 2048 # batch size needs to be smaller for larger model to not run out of memory\n",
    "\n",
    "start_time = time.time()  # Start timing\n",
    "all_encodings, batch_input_ids, predicted_labels, detailed_predictions, tokenizer = batch_predict(token_model\n",
    "                                                                                            , tokenizer\n",
    "                                                                                            , all_texts\n",
    "                                                                                            , metadata\n",
    "                                                                                            , window_size=512\n",
    "                                                                                            , stride = stride\n",
    "                                                                                            , sub_batch_size=sub_batch_size)\n",
    "end_time = time.time()  # End timing\n",
    "\n",
    "# Move to cpu for subsequent processing and snippet creation\n",
    "batch_input_ids = batch_input_ids.to(\"cpu\")\n",
    "predicted_labels = [label.to(\"cpu\") for label in predicted_labels]\n",
    "\n",
    "# Create snippets using multiprocessing\n",
    "snippets_collection = create_snippets_multiprocessing(all_encodings\n",
    "                                            , batch_input_ids\n",
    "                                            , predicted_labels\n",
    "                                            , detailed_predictions\n",
    "                                            , tokenizer\n",
    "                                            , stride = stride\n",
    "                                            , min_snippet_length = 10\n",
    "                                            , ignore_o_threshold = 4\n",
    "                                            # , multiprocessing = True\n",
    "                                            , workers = 8\n",
    "                                            , chunksize = 512)\n",
    "\n",
    "# Step 3: Assign results back to the original data_dict\n",
    "for ((doc_id, page_index), snippets) in tqdm(zip(metadata, snippets_collection), desc=\"Assign results\", total=len(metadata)):\n",
    "    snippet_texts = []\n",
    "    detailed_tags = []\n",
    "    for snippet_text, detailed_tag in snippets:\n",
    "        if snippet_text.strip():\n",
    "            snippet_texts.append(snippet_text)\n",
    "            detailed_tags.append(detailed_tag)\n",
    "    predicted_snippets = []\n",
    "\n",
    "    if snippet_texts:\n",
    "        tokenized_snippets = tokenizer(snippet_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        for snippet_text, detailed_tag, snippet_tokenized_ids in zip(snippet_texts, detailed_tags, tokenized_snippets['input_ids']):\n",
    "            snippet_tokenized_text = tokenizer.convert_ids_to_tokens(snippet_tokenized_ids)\n",
    "            dict_to_add = {'text': snippet_text, 'tokenized_text': snippet_tokenized_text, 'detailed_tag': detailed_tag}\n",
    "            predicted_snippets.append(dict_to_add)\n",
    "\n",
    "    # Write predicted snippets into data_dict, following structure of other elements\n",
    "    data_dict[doc_id][page_index]['predicted_snippets'] = predicted_snippets\n",
    "\n",
    "total_time = end_time - start_time  # Calculate total time\n",
    "\n",
    "print(f\"Total time for processing {doc_count} documents (={page_count} pages): {total_time} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate evaluation metrics (Chapter 6 in report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import evaluator\n",
    "importlib.reload(evaluator)\n",
    "\n",
    "metrics_config = {\n",
    "    'iou': True,\n",
    "    'bleu': True,\n",
    "    'jaccard': True,\n",
    "    'precision': True,\n",
    "    'recall': True,\n",
    "    'f1': True,\n",
    "    'precision_region_lvl': True,\n",
    "    'recall_region_lvl': True,\n",
    "    'f1_region_lvl': True,\n",
    "    'edit_distance': True,\n",
    "    'rouge-1-f': True,\n",
    "    'rouge-2-f': True,\n",
    "    'rouge-l-f': True,\n",
    "    'pk': False,\n",
    "    'windowdiff': False,\n",
    "    'cohen_kappa': False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b36dd7afdf7480cbcb0583a7dd2ae3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.6530978259125895\n",
      "Average Recall Score: 0.8428955629920146\n",
      "Average F1 Score: 0.7359568080237774\n",
      "Average Iou Score: 0.7685942283292141\n",
      "Average Bleu Score: 0.5939380039220407\n",
      "Average Jaccard Score: 0.7685942283292143\n",
      "Average Edit_distance Score: 148.65251626155427\n",
      "Average Precision_region_lvl Score: 0.6631284134971608\n",
      "Average Recall_region_lvl Score: 0.8922598553166202\n",
      "Average F1_region_lvl Score: 0.6980769874318282\n",
      "Average Rouge-1-f Score: 0.8588084778041937\n",
      "Average Rouge-2-f Score: 0.8334367448925841\n",
      "Average Rouge-l-f Score: 0.8574261693576304\n"
     ]
    }
   ],
   "source": [
    "aggregated_metrics_predicted_snippets = evaluator.evaluate_snippets_parallel(data_dict, \"predicted_snippets\", \"refined_regions\", metrics_config)\n",
    "aggregated_metrics_predicted_snippets['inference_time'] = total_time\n",
    "aggregated_metrics_predicted_snippets['pages'] = page_count\n",
    "aggregated_metrics_predicted_snippets['batch_size'] = sub_batch_size\n",
    "aggregated_metrics_predicted_snippets['GPU'] = torch.cuda.get_device_properties(0).name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'roberta-base'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'or6jqv'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRETRAINED_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.6530978259125895,\n",
       " 'recall': 0.8428955629920146,\n",
       " 'f1': 0.7359568080237774,\n",
       " 'iou': 0.7685942283292141,\n",
       " 'bleu': 0.5939380039220407,\n",
       " 'jaccard': 0.7685942283292143,\n",
       " 'edit_distance': 148.65251626155427,\n",
       " 'precision_region_lvl': 0.6631284134971608,\n",
       " 'recall_region_lvl': 0.8922598553166202,\n",
       " 'f1_region_lvl': 0.6980769874318282,\n",
       " 'rouge-1-f': 0.8588084778041937,\n",
       " 'rouge-2-f': 0.8334367448925841,\n",
       " 'rouge-l-f': 0.8574261693576304,\n",
       " 'inference_time': 120.92954063415527,\n",
       " 'pages': 6918,\n",
       " 'batch_size': 2048,\n",
       " 'GPU': 'NVIDIA A100-SXM4-80GB'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_metrics_predicted_snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result to json file\n",
    "with open(f'Evaluation/{DOMAIN}/token-{str(MODEL).replace(\"/\", \"-\")}_{PRETRAINED_MODEL if PRETRAINED_MODEL else unique_tag}.json', 'w') as f:\n",
    "    json.dump(aggregated_metrics_predicted_snippets, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation loop for best snippet creation parameters (see Section 5.4 in report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean data_dict predictions, if relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c245273f474959ae25ab863886a962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for doc_id, page_dict in tqdm(data_dict.items()):\n",
    "    for page_index, page_content in page_dict.items():\n",
    "        if page_index in ['title', 'doc_long_id']:\n",
    "            continue\n",
    "        data_dict[doc_id][page_index]['predicted_snippets'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f79072ef0941abb2051c1fad715a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# collect validation data in dict for easy access (same as done above for test data)\n",
    "validation_doc_pages_overview = defaultdict(list)\n",
    "for sample in tqdm(dataset_dict['test']):\n",
    "    doc_id = sample['metadata']['doc_id']\n",
    "    page_index = sample['metadata']['page_id']\n",
    "    if page_index not in validation_doc_pages_overview[doc_id]:\n",
    "        validation_doc_pages_overview[doc_id].append(page_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c57f3b0950034376b45f0a0162733f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cefc3f2b85d24b2296528d8c1995b966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 15763\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbe80a93a944638925818d1156b073e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc_count = 0\n",
    "page_count = 0\n",
    "\n",
    "# Step 1: Collect all texts and metadata for batch processing of validation data\n",
    "all_texts = []\n",
    "metadata = []\n",
    "for doc_id in tqdm(validation_doc_pages_overview):\n",
    "    for page_index in validation_doc_pages_overview[doc_id]:\n",
    "        page_text = data_dict[doc_id][page_index]['full_text']\n",
    "        all_texts.append(page_text)\n",
    "        metadata.append((doc_id, page_index))\n",
    "        page_count += 1\n",
    "    doc_count += 1\n",
    "\n",
    "# Step 2: Process all texts in batches\n",
    "stride = OVERLAP\n",
    "\n",
    "all_encodings, batch_input_ids, predicted_labels, tokenizer = batch_predict(token_model\n",
    "                                                                                            , tokenizer\n",
    "                                                                                            , all_texts\n",
    "                                                                                            , window_size=512\n",
    "                                                                                            , stride = stride\n",
    "                                                                                            , sub_batch_size=512 if MODEL == \"roberta-large\" else 2048)\n",
    "batch_input_ids = batch_input_ids.to(\"cpu\")\n",
    "predicted_labels = [label.to(\"cpu\") for label in predicted_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0 out of 25. Combination tested: ignore_o_threshold=0 and min_snippet_length=0. Currently best: {}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d5e18fd6ad4099adf9a2d9def1f379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b9ac051c8d41709e33cda15bc699f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assign results:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd899c12e5f462aa4ecf793b8c4fa89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.6534951734094638\n",
      "Average Recall Score: 0.7885198727110102\n",
      "Average F1 Score: 0.7146859283339813\n",
      "Average Iou Score: 0.7435251950611076\n",
      "Average Bleu Score: 0.5622263863505115\n",
      "Average Jaccard Score: 0.7435251950611076\n",
      "Average Precision_region_lvl Score: 0.667132528976865\n",
      "Average Recall_region_lvl Score: 0.8439876973903111\n",
      "Average F1_region_lvl Score: 0.6757403110200386\n",
      "Average Rouge-1-f Score: 0.841391720239641\n",
      "Average Rouge-2-f Score: 0.8137685035997634\n",
      "Average Rouge-l-f Score: 0.8399571872883745\n",
      "Run 1 out of 25. Combination tested: ignore_o_threshold=0 and min_snippet_length=4. Currently best: {'run_id': 0, 'ignore_o_threshold': 0, 'min_snippet_length': 0, 'f1': 0.7146859283339813}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85e0f951b514d71905a54bf05c23807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8230eaa4101458e94d0dd002f7a4cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assign results:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a55b06e4704f33ae5dade1b37d0ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.653500110222604\n",
      "Average Recall Score: 0.7891874668522366\n",
      "Average F1 Score: 0.7149629687945669\n",
      "Average Iou Score: 0.7443366934743155\n",
      "Average Bleu Score: 0.56290025531491\n",
      "Average Jaccard Score: 0.7443366934743156\n",
      "Average Precision_region_lvl Score: 0.6675567026206406\n",
      "Average Recall_region_lvl Score: 0.8450144874845837\n",
      "Average F1_region_lvl Score: 0.6765345689867835\n",
      "Average Rouge-1-f Score: 0.8420190933957421\n",
      "Average Rouge-2-f Score: 0.814592498478804\n",
      "Average Rouge-l-f Score: 0.8405830866092516\n",
      "Run 2 out of 25. Combination tested: ignore_o_threshold=0 and min_snippet_length=8. Currently best: {'run_id': 1, 'ignore_o_threshold': 0, 'min_snippet_length': 4, 'f1': 0.7149629687945669}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9903c156620b45a08353dba0340f495d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76defd63fa1d477b86fbbcffa656865a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assign results:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b4abe3beac4f138280e8905dcbd5b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.6535276534357056\n",
      "Average Recall Score: 0.7892388960119511\n",
      "Average F1 Score: 0.7150005576554852\n",
      "Average Iou Score: 0.7441568549620449\n",
      "Average Bleu Score: 0.5629956247939677\n",
      "Average Jaccard Score: 0.7441568549620449\n",
      "Average Precision_region_lvl Score: 0.6675884155465397\n",
      "Average Recall_region_lvl Score: 0.8453393543361369\n",
      "Average F1_region_lvl Score: 0.6766699911229992\n",
      "Average Rouge-1-f Score: 0.8417659431885891\n",
      "Average Rouge-2-f Score: 0.8143258393867231\n",
      "Average Rouge-l-f Score: 0.8403202528647408\n",
      "Run 3 out of 25. Combination tested: ignore_o_threshold=0 and min_snippet_length=10. Currently best: {'run_id': 2, 'ignore_o_threshold': 0, 'min_snippet_length': 8, 'f1': 0.7150005576554852}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6c5b3560014eb395c7b850cbd20784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe1ab595c144f0c8f923fd4c755bb74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assign results:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec25c72b28b24184a3227ba1058eb5cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.6535804337583346\n",
      "Average Recall Score: 0.789405684754522\n",
      "Average F1 Score: 0.7151005865321617\n",
      "Average Iou Score: 0.7443551183427839\n",
      "Average Bleu Score: 0.5632200564546811\n",
      "Average Jaccard Score: 0.7443551183427839\n",
      "Average Precision_region_lvl Score: 0.6676519339194741\n",
      "Average Recall_region_lvl Score: 0.845996735257491\n",
      "Average F1_region_lvl Score: 0.6769203592450044\n",
      "Average Rouge-1-f Score: 0.8419477171959541\n",
      "Average Rouge-2-f Score: 0.8145069442560818\n",
      "Average Rouge-l-f Score: 0.8404991846567922\n",
      "Run 4 out of 25. Combination tested: ignore_o_threshold=0 and min_snippet_length=12. Currently best: {'run_id': 3, 'ignore_o_threshold': 0, 'min_snippet_length': 10, 'f1': 0.7151005865321617}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e7876242be64ceb9b2eec0e950f418e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c41068fbdaf4519a8ddf11f7b141da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assign results:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66613da848f84811bfc9cf7b32f8ef73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.6535877685593585\n",
      "Average Recall Score: 0.7894565108666783\n",
      "Average F1 Score: 0.7151258303968845\n",
      "Average Iou Score: 0.7440688157984248\n",
      "Average Bleu Score: 0.563315423756124\n",
      "Average Jaccard Score: 0.7440688157984248\n",
      "Average Precision_region_lvl Score: 0.6676564003371623\n",
      "Average Recall_region_lvl Score: 0.8459997000611125\n",
      "Average F1_region_lvl Score: 0.6770364818095016\n",
      "Average Rouge-1-f Score: 0.8413773472792347\n",
      "Average Rouge-2-f Score: 0.8138673879262516\n",
      "Average Rouge-l-f Score: 0.839919672640271\n",
      "Run 5 out of 25. Combination tested: ignore_o_threshold=2 and min_snippet_length=0. Currently best: {'run_id': 4, 'ignore_o_threshold': 0, 'min_snippet_length': 12, 'f1': 0.7151258303968845}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4512486800146628bc4c9e4590bfdd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c1e8ea804f41d8807d2cc591d334f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assign results:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae6f71b14bd4a21a861f84e22c1003d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.6441759143375186\n",
      "Average Recall Score: 0.8093745391700305\n",
      "Average F1 Score: 0.7173876662529682\n",
      "Average Iou Score: 0.7453425172972481\n",
      "Average Bleu Score: 0.5696131841360336\n",
      "Average Jaccard Score: 0.7453425172972481\n",
      "Average Precision_region_lvl Score: 0.6581789528875815\n",
      "Average Recall_region_lvl Score: 0.866333792369093\n",
      "Average F1_region_lvl Score: 0.6799449789981434\n",
      "Average Rouge-1-f Score: 0.8408828615175408\n",
      "Average Rouge-2-f Score: 0.8138799935610037\n",
      "Average Rouge-l-f Score: 0.8395866073631703\n",
      "Run 6 out of 25. Combination tested: ignore_o_threshold=2 and min_snippet_length=4. Currently best: {'run_id': 5, 'ignore_o_threshold': 2, 'min_snippet_length': 0, 'f1': 0.7173876662529682}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbffbc5c9d7d4c66949ddebebce81b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19d01c886924ad5994053c02f0db301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assign results:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc398d94a7c84eeb99a4dbc4dae544c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.6441783504046438\n",
      "Average Recall Score: 0.8096437515380496\n",
      "Average F1 Score: 0.717494906129535\n",
      "Average Iou Score: 0.7457159004683019\n",
      "Average Bleu Score: 0.5699057382560535\n",
      "Average Jaccard Score: 0.7457159004683019\n",
      "Average Precision_region_lvl Score: 0.6583811438520701\n",
      "Average Recall_region_lvl Score: 0.866824239941157\n",
      "Average F1_region_lvl Score: 0.6802901856733803\n",
      "Average Rouge-1-f Score: 0.8411890566545775\n",
      "Average Rouge-2-f Score: 0.8141585288086293\n",
      "Average Rouge-l-f Score: 0.8398923588814001\n",
      "Run 7 out of 25. Combination tested: ignore_o_threshold=2 and min_snippet_length=8. Currently best: {'run_id': 6, 'ignore_o_threshold': 2, 'min_snippet_length': 4, 'f1': 0.717494906129535}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc14e1ddef954eae91d3e0c4043f8f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf2a4cb530d407a805d88a74061f4ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assign results:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94bf9e0b0a824177bfb88a4c5b93c10d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.6442193361560231\n",
      "Average Recall Score: 0.8099099018345771\n",
      "Average F1 Score: 0.717624824086556\n",
      "Average Iou Score: 0.7458745572991652\n",
      "Average Bleu Score: 0.5701082717970113\n",
      "Average Jaccard Score: 0.7458745572991652\n",
      "Average Precision_region_lvl Score: 0.6585209774053409\n",
      "Average Recall_region_lvl Score: 0.8672977079253992\n",
      "Average F1_region_lvl Score: 0.6805659066917427\n",
      "Average Rouge-1-f Score: 0.8412475821744897\n",
      "Average Rouge-2-f Score: 0.814218566277425\n",
      "Average Rouge-l-f Score: 0.8399484500804423\n",
      "Run 8 out of 25. Combination tested: ignore_o_threshold=2 and min_snippet_length=10. Currently best: {'run_id': 7, 'ignore_o_threshold': 2, 'min_snippet_length': 8, 'f1': 0.717624824086556}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71bd67bda17148e6a7a9c5c0118ecf30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8f95f32a5f46b4b7461cd7934d9b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assign results:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7ea5732e96435db195e9d3db2af43f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.6442193361560231\n",
      "Average Recall Score: 0.8099099018345771\n",
      "Average F1 Score: 0.717624824086556\n",
      "Average Iou Score: 0.7458745572991652\n",
      "Average Bleu Score: 0.5701082717970113\n",
      "Average Jaccard Score: 0.7458745572991652\n",
      "Average Precision_region_lvl Score: 0.6585209774053409\n",
      "Average Recall_region_lvl Score: 0.8672977079253992\n",
      "Average F1_region_lvl Score: 0.6805659066917427\n",
      "Average Rouge-1-f Score: 0.8412475821744897\n",
      "Average Rouge-2-f Score: 0.814218566277425\n",
      "Average Rouge-l-f Score: 0.8399484500804423\n",
      "Run 9 out of 25. Combination tested: ignore_o_threshold=2 and min_snippet_length=12. Currently best: {'run_id': 7, 'ignore_o_threshold': 2, 'min_snippet_length': 8, 'f1': 0.717624824086556}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb46fec5c9ed477eb291e599b190683b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5abe0592b543a9b714da42be4d7f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assign results:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82eb65fc6d01459188f2f4a7ff409993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.6442292082574644\n",
      "Average Recall Score: 0.8099223129999372\n",
      "Average F1 Score: 0.7176358210633038\n",
      "Average Iou Score: 0.7456441128140486\n",
      "Average Bleu Score: 0.5701117326955447\n",
      "Average Jaccard Score: 0.7456441128140486\n",
      "Average Precision_region_lvl Score: 0.6585475193798747\n",
      "Average Recall_region_lvl Score: 0.8672918808945806\n",
      "Average F1_region_lvl Score: 0.6806028995267532\n",
      "Average Rouge-1-f Score: 0.8407959262865686\n",
      "Average Rouge-2-f Score: 0.8137279512571438\n",
      "Average Rouge-l-f Score: 0.8394891864478369\n",
      "Run 10 out of 25. Combination tested: ignore_o_threshold=4 and min_snippet_length=0. Currently best: {'run_id': 9, 'ignore_o_threshold': 2, 'min_snippet_length': 12, 'f1': 0.7176358210633038}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e1dd763e3340e39393bc368e3f1d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eee2698c02545afab2e7fd0ce104de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assign results:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6e531cc8f946aa9b486decc31afc71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.6419695304064112\n",
      "Average Recall Score: 0.8156838613382433\n",
      "Average F1 Score: 0.7184755832751906\n",
      "Average Iou Score: 0.7451742964108466\n",
      "Average Bleu Score: 0.5710486810653063\n",
      "Average Jaccard Score: 0.7451742964108466\n",
      "Average Precision_region_lvl Score: 0.6561882605452536\n",
      "Average Recall_region_lvl Score: 0.8720900651730561\n",
      "Average F1_region_lvl Score: 0.6810806748588459\n",
      "Average Rouge-1-f Score: 0.8404688167271754\n",
      "Average Rouge-2-f Score: 0.8135940246601502\n",
      "Average Rouge-l-f Score: 0.839209938694407\n",
      "Run 11 out of 25. Combination tested: ignore_o_threshold=4 and min_snippet_length=4. Currently best: {'run_id': 10, 'ignore_o_threshold': 4, 'min_snippet_length': 0, 'f1': 0.7184755832751906}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7143ad906b0b4fcfb47cb878789a7a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64561290c4c446dcabba41d199a9c39a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assign results:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49117d0523f4a92b8555b6729ea136a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.6419719273849819\n",
      "Average Recall Score: 0.8159248115636341\n",
      "Average F1 Score: 0.7185705405425095\n",
      "Average Iou Score: 0.7454198342437712\n",
      "Average Bleu Score: 0.57124417559797\n",
      "Average Jaccard Score: 0.7454198342437712\n",
      "Average Precision_region_lvl Score: 0.6562767339609791\n",
      "Average Recall_region_lvl Score: 0.8724341805476294\n",
      "Average F1_region_lvl Score: 0.6813091208663313\n",
      "Average Rouge-1-f Score: 0.8407740885347628\n",
      "Average Rouge-2-f Score: 0.8138724144034043\n",
      "Average Rouge-l-f Score: 0.8395147797481768\n",
      "Run 12 out of 25. Combination tested: ignore_o_threshold=4 and min_snippet_length=8. Currently best: {'run_id': 11, 'ignore_o_threshold': 4, 'min_snippet_length': 4, 'f1': 0.7185705405425095}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694c70a5a9a44ae196cc3beb2d762994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c31dc852054c02843d21a55212b7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assign results:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "179aa2e9a5d9420187f6fdb8ac69e55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.6419951677328206\n",
      "Average Recall Score: 0.8159820666009053\n",
      "Average F1 Score: 0.7186073024711069\n",
      "Average Iou Score: 0.7454365247464708\n",
      "Average Bleu Score: 0.5713438639340951\n",
      "Average Jaccard Score: 0.7454365247464708\n",
      "Average Precision_region_lvl Score: 0.6563016149365688\n",
      "Average Recall_region_lvl Score: 0.8726855317307785\n",
      "Average F1_region_lvl Score: 0.6814434106302335\n",
      "Average Rouge-1-f Score: 0.8406870094376678\n",
      "Average Rouge-2-f Score: 0.8137941023408615\n",
      "Average Rouge-l-f Score: 0.8394218748136362\n",
      "Run 13 out of 25. Combination tested: ignore_o_threshold=4 and min_snippet_length=10. Currently best: {'run_id': 12, 'ignore_o_threshold': 4, 'min_snippet_length': 8, 'f1': 0.7186073024711069}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2cfda6bce949a7aa5ad0caae718fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eedc938252c4e6888bbae66e24991f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assign results:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c91b05ad5f444e1690523ef06d867081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.6419979466687059\n",
      "Average Recall Score: 0.8161975361078724\n",
      "Average F1 Score: 0.7186925874431567\n",
      "Average Iou Score: 0.7456770504596139\n",
      "Average Bleu Score: 0.5715394624023001\n",
      "Average Jaccard Score: 0.7456770504596139\n",
      "Average Precision_region_lvl Score: 0.656386247153565\n",
      "Average Recall_region_lvl Score: 0.8729778976287863\n",
      "Average F1_region_lvl Score: 0.6816645406185242\n",
      "Average Rouge-1-f Score: 0.8409363403734572\n",
      "Average Rouge-2-f Score: 0.8140532795110852\n",
      "Average Rouge-l-f Score: 0.8396707726324711\n",
      "Run 14 out of 25. Combination tested: ignore_o_threshold=4 and min_snippet_length=12. Currently best: {'run_id': 13, 'ignore_o_threshold': 4, 'min_snippet_length': 10, 'f1': 0.7186925874431567}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8fb8921d3054ccbb6aa0d47683901ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d79acaf4a945bfa4248aa42381af36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assign results:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1abfaa813a1e49619d12cbc1e0de7fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.6419941502294405\n",
      "Average Recall Score: 0.8161927095435656\n",
      "Average F1 Score: 0.7186883374719957\n",
      "Average Iou Score: 0.7455170655170201\n",
      "Average Bleu Score: 0.571533744106066\n",
      "Average Jaccard Score: 0.7455170655170201\n",
      "Average Precision_region_lvl Score: 0.6563821364198162\n",
      "Average Recall_region_lvl Score: 0.8728474791484877\n",
      "Average F1_region_lvl Score: 0.6816566127919944\n",
      "Average Rouge-1-f Score: 0.8406217057909169\n",
      "Average Rouge-2-f Score: 0.8137223426190354\n",
      "Average Rouge-l-f Score: 0.8393561380499306\n",
      "Run 15 out of 25. Combination tested: ignore_o_threshold=8 and min_snippet_length=0. Currently best: {'run_id': 13, 'ignore_o_threshold': 4, 'min_snippet_length': 10, 'f1': 0.7186925874431567}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dccafa74dda492687448193c92d0041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038632dc3b83418a8833490cfaa926a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assign results:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a628d24ab2245fe8d61a99f6ae2f925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.6385847970549449\n",
      "Average Recall Score: 0.8195262357823188\n",
      "Average F1 Score: 0.717828729325119\n",
      "Average Iou Score: 0.7429200490382156\n",
      "Average Bleu Score: 0.5707674093632182\n",
      "Average Jaccard Score: 0.7429200490382156\n",
      "Average Precision_region_lvl Score: 0.6535739077529336\n",
      "Average Recall_region_lvl Score: 0.8760401800084575\n",
      "Average F1_region_lvl Score: 0.6809781446154587\n",
      "Average Rouge-1-f Score: 0.8381857040877957\n",
      "Average Rouge-2-f Score: 0.8110093126443688\n",
      "Average Rouge-l-f Score: 0.8369408433025517\n",
      "Run 16 out of 25. Combination tested: ignore_o_threshold=8 and min_snippet_length=4. Currently best: {'run_id': 13, 'ignore_o_threshold': 4, 'min_snippet_length': 10, 'f1': 0.7186925874431567}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa82f85652ec4f7eb2aff877ce98d262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b186c83d4ad448f9bd92398fc1f97de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assign results:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b570e1535fb4a0a9361db092db76f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.6385860781393131\n",
      "Average Recall Score: 0.8197669489334863\n",
      "Average F1 Score: 0.717921862799457\n",
      "Average Iou Score: 0.7431678352697195\n",
      "Average Bleu Score: 0.5709627104442664\n",
      "Average Jaccard Score: 0.7431678352697195\n",
      "Average Precision_region_lvl Score: 0.6536606738462623\n",
      "Average Recall_region_lvl Score: 0.8763367651766931\n",
      "Average F1_region_lvl Score: 0.6812049566015791\n",
      "Average Rouge-1-f Score: 0.8384639326755831\n",
      "Average Rouge-2-f Score: 0.8112868179694823\n",
      "Average Rouge-l-f Score: 0.8372186459328419\n",
      "Run 17 out of 25. Combination tested: ignore_o_threshold=8 and min_snippet_length=8. Currently best: {'run_id': 13, 'ignore_o_threshold': 4, 'min_snippet_length': 10, 'f1': 0.7186925874431567}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c57de4fe1b2439cbc1e3a0c1712b5ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f47987042164563bc67a2130e620fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assign results:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272907ef0b134bf38aa0241499e0fddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.6385860781393131\n",
      "Average Recall Score: 0.8197669489334863\n",
      "Average F1 Score: 0.717921862799457\n",
      "Average Iou Score: 0.7430865164259257\n",
      "Average Bleu Score: 0.5709613715369265\n",
      "Average Jaccard Score: 0.7430865164259257\n",
      "Average Precision_region_lvl Score: 0.6536606738462623\n",
      "Average Recall_region_lvl Score: 0.8763367651766931\n",
      "Average F1_region_lvl Score: 0.6812049566015791\n",
      "Average Rouge-1-f Score: 0.8383001785519164\n",
      "Average Rouge-2-f Score: 0.8111362687285999\n",
      "Average Rouge-l-f Score: 0.8370548918091752\n",
      "Run 18 out of 25. Combination tested: ignore_o_threshold=8 and min_snippet_length=10. Currently best: {'run_id': 13, 'ignore_o_threshold': 4, 'min_snippet_length': 10, 'f1': 0.7186925874431567}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a369ab7c1544cfbbcacf4f476a6303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87fee0fbf2ee459eb80296b84ed9029b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assign results:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e692c5b032441bcaa2aab8e6e37260d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.6385855543116108\n",
      "Average Recall Score: 0.819791948857828\n",
      "Average F1 Score: 0.7179311185805491\n",
      "Average Iou Score: 0.7431864472280022\n",
      "Average Bleu Score: 0.5710567426030477\n",
      "Average Jaccard Score: 0.7431864472280022\n",
      "Average Precision_region_lvl Score: 0.6536480776701112\n",
      "Average Recall_region_lvl Score: 0.8764613696628377\n",
      "Average F1_region_lvl Score: 0.681279400665653\n",
      "Average Rouge-1-f Score: 0.8383909738759759\n",
      "Average Rouge-2-f Score: 0.8112322879399753\n",
      "Average Rouge-l-f Score: 0.8371454740451545\n",
      "Run 19 out of 25. Combination tested: ignore_o_threshold=8 and min_snippet_length=12. Currently best: {'run_id': 13, 'ignore_o_threshold': 4, 'min_snippet_length': 10, 'f1': 0.7186925874431567}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2de2a0e4f7548cb81e3387e4fe15e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd6c8a3768a742e196fbeb585e4c3b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assign results:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6abab0b9feed4a0596f46b6796d65125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.6385844804025012\n",
      "Average Recall Score: 0.8197905702139862\n",
      "Average F1 Score: 0.7179299112360311\n",
      "Average Iou Score: 0.7431265327762528\n",
      "Average Bleu Score: 0.5710535136615903\n",
      "Average Jaccard Score: 0.7431265327762528\n",
      "Average Precision_region_lvl Score: 0.6536467460306233\n",
      "Average Recall_region_lvl Score: 0.876432850383805\n",
      "Average F1_region_lvl Score: 0.6812768561946612\n",
      "Average Rouge-1-f Score: 0.838248377481117\n",
      "Average Rouge-2-f Score: 0.8110801851188194\n",
      "Average Rouge-l-f Score: 0.8370028776502956\n",
      "Run 20 out of 25. Combination tested: ignore_o_threshold=10 and min_snippet_length=0. Currently best: {'run_id': 13, 'ignore_o_threshold': 4, 'min_snippet_length': 10, 'f1': 0.7186925874431567}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90cba0c7d1f741728abe914c8848df8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce8d60c12e804718b1666d161e431100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assign results:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38863cbc514486a95c630fd586e88cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.6375605005881332\n",
      "Average Recall Score: 0.8202098145930294\n",
      "Average F1 Score: 0.7174427610898976\n",
      "Average Iou Score: 0.742475255026636\n",
      "Average Bleu Score: 0.5704749597757481\n",
      "Average Jaccard Score: 0.742475255026636\n",
      "Average Precision_region_lvl Score: 0.6526835007626605\n",
      "Average Recall_region_lvl Score: 0.8768379247385243\n",
      "Average F1_region_lvl Score: 0.6806971371341963\n",
      "Average Rouge-1-f Score: 0.8378928806149109\n",
      "Average Rouge-2-f Score: 0.8106548258227755\n",
      "Average Rouge-l-f Score: 0.8366832791866671\n",
      "Run 21 out of 25. Combination tested: ignore_o_threshold=10 and min_snippet_length=4. Currently best: {'run_id': 13, 'ignore_o_threshold': 4, 'min_snippet_length': 10, 'f1': 0.7186925874431567}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c799e0fec24c79a1b3d66c94ff8b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30cf6a86a44941bd958c4be6927a682b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assign results:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ef76d2a27f49e299dc9b26d9c6d1ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.6375617730650689\n",
      "Average Recall Score: 0.8204507308263628\n",
      "Average F1 Score: 0.7175357155882639\n",
      "Average Iou Score: 0.7427228890617288\n",
      "Average Bleu Score: 0.570670160788503\n",
      "Average Jaccard Score: 0.7427228890617288\n",
      "Average Precision_region_lvl Score: 0.6527699621829387\n",
      "Average Recall_region_lvl Score: 0.8771347828733057\n",
      "Average F1_region_lvl Score: 0.6809238529672007\n",
      "Average Rouge-1-f Score: 0.8381710090064712\n",
      "Average Rouge-2-f Score: 0.8109322098521418\n",
      "Average Rouge-l-f Score: 0.8369609936855233\n",
      "Run 22 out of 25. Combination tested: ignore_o_threshold=10 and min_snippet_length=8. Currently best: {'run_id': 13, 'ignore_o_threshold': 4, 'min_snippet_length': 10, 'f1': 0.7186925874431567}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f0dd35f8e64580b324eda627d7abd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd9f82b024bd4122a60210ba5c99c17a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assign results:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc985cf3700b4048a6e65e586ad34105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.6375617730650689\n",
      "Average Recall Score: 0.8204507308263628\n",
      "Average F1 Score: 0.7175357155882639\n",
      "Average Iou Score: 0.7426415702179349\n",
      "Average Bleu Score: 0.570668821881163\n",
      "Average Jaccard Score: 0.7426415702179349\n",
      "Average Precision_region_lvl Score: 0.6527699621829387\n",
      "Average Recall_region_lvl Score: 0.8771347828733057\n",
      "Average F1_region_lvl Score: 0.6809238529672007\n",
      "Average Rouge-1-f Score: 0.8380072548828046\n",
      "Average Rouge-2-f Score: 0.8107816606112592\n",
      "Average Rouge-l-f Score: 0.8367972395618565\n",
      "Run 23 out of 25. Combination tested: ignore_o_threshold=10 and min_snippet_length=10. Currently best: {'run_id': 13, 'ignore_o_threshold': 4, 'min_snippet_length': 10, 'f1': 0.7186925874431567}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac1aa31781304c7392bc4e96133e8afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b47fa761e0942c7870f6a022b7d21c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assign results:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377590cba9384d4c9e476cc8a42d7902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.6375612444781544\n",
      "Average Recall Score: 0.8204757562033803\n",
      "Average F1 Score: 0.717544951115323\n",
      "Average Iou Score: 0.742741424882741\n",
      "Average Bleu Score: 0.5707641428874527\n",
      "Average Jaccard Score: 0.742741424882741\n",
      "Average Precision_region_lvl Score: 0.6527572135920608\n",
      "Average Recall_region_lvl Score: 0.8772595239127864\n",
      "Average F1_region_lvl Score: 0.6809982489300367\n",
      "Average Rouge-1-f Score: 0.8380980000830333\n",
      "Average Rouge-2-f Score: 0.8108776191436278\n",
      "Average Rouge-l-f Score: 0.8368877777094977\n",
      "Run 24 out of 25. Combination tested: ignore_o_threshold=10 and min_snippet_length=12. Currently best: {'run_id': 13, 'ignore_o_threshold': 4, 'min_snippet_length': 10, 'f1': 0.7186925874431567}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b743f27963d4a77ae4e31da75dcf6b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130f7abc2e154103b14255d75663a8c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assign results:   0%|          | 0/6918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6b53dc63bc40c791e69bf154168cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.6375965971451114\n",
      "Average Recall Score: 0.820521251450161\n",
      "Average F1 Score: 0.7175847388657597\n",
      "Average Iou Score: 0.7426647029140002\n",
      "Average Bleu Score: 0.5707664066475714\n",
      "Average Jaccard Score: 0.7426647029140002\n",
      "Average Precision_region_lvl Score: 0.652785547472765\n",
      "Average Recall_region_lvl Score: 0.8773667119563194\n",
      "Average F1_region_lvl Score: 0.6810438606967714\n",
      "Average Rouge-1-f Score: 0.8379245295698652\n",
      "Average Rouge-2-f Score: 0.8106749944062592\n",
      "Average Rouge-l-f Score: 0.8367083031375858\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import evaluator\n",
    "importlib.reload(evaluator)\n",
    "\n",
    "metrics_config = {\n",
    "    'iou': True,\n",
    "    'bleu': True,\n",
    "    'jaccard': True,\n",
    "    'precision': True,\n",
    "    'recall': True,\n",
    "    'f1': True,\n",
    "    'precision_region_lvl': True,\n",
    "    'recall_region_lvl': True,\n",
    "    'f1_region_lvl': True,\n",
    "    'edit_distance': False,\n",
    "    'rouge-1-f': True,\n",
    "    'rouge-2-f': True,\n",
    "    'rouge-l-f': True,\n",
    "    'pk': False,\n",
    "    'windowdiff': False,\n",
    "    'cohen_kappa': False\n",
    "}\n",
    "\n",
    "\n",
    "# Test values in simple grid search\n",
    "all_scores = []\n",
    "best_run = {}\n",
    "best_f1 = 0\n",
    "run_id = 0\n",
    "ignore_o_thresholds = [0, 2, 4, 8, 10]\n",
    "min_snippet_lengths = [0, 4, 8, 10, 12]\n",
    "for ignore_o_threshold in ignore_o_thresholds:\n",
    "    for min_snippet_length in min_snippet_lengths:\n",
    "        print(f\"Run {run_id} out of {len(ignore_o_thresholds)*len(min_snippet_lengths)}. Combination tested: {ignore_o_threshold=} and {min_snippet_length=}. Currently best: {best_run}\")\n",
    "\n",
    "        # Create snippets through adjusted post-proessing algorithm using multiprocessing\n",
    "        snippets_collection = create_snippets_multiprocessing(all_encodings\n",
    "                                                    , batch_input_ids\n",
    "                                                    , predicted_labels\n",
    "                                                    , tokenizer\n",
    "                                                    , stride = stride\n",
    "                                                    , min_snippet_length = min_snippet_length\n",
    "                                                    , ignore_o_threshold = ignore_o_threshold\n",
    "                                                    # , multiprocessing = True\n",
    "                                                    , workers = 8\n",
    "                                                    , chunksize = 512)\n",
    "\n",
    "        # Step 3: Assign results back to original data_dict\n",
    "        for ((doc_id, page_index), snippets) in tqdm(zip(metadata, snippets_collection), desc=\"Assign results\", total=len(metadata)):\n",
    "            snippet_texts = []\n",
    "            detailed_tags = []\n",
    "            for snippet_text, detailed_tag in snippets:\n",
    "                if snippet_text.strip():\n",
    "                    snippet_texts.append(snippet_text)\n",
    "                    detailed_tags.append(detailed_tag)\n",
    "            predicted_snippets = []\n",
    "\n",
    "            if snippet_texts:\n",
    "                tokenized_snippets = tokenizer(snippet_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "                \n",
    "                for snippet_text, detailed_tag, snippet_tokenized_ids in zip(snippet_texts, detailed_tags, tokenized_snippets['input_ids']):\n",
    "                    snippet_tokenized_text = tokenizer.convert_ids_to_tokens(snippet_tokenized_ids)\n",
    "                    dict_to_add = {'text': snippet_text, 'tokenized_text': snippet_tokenized_text, 'detailed_tag': detailed_tag}\n",
    "                    predicted_snippets.append(dict_to_add)\n",
    "\n",
    "            data_dict[doc_id][page_index]['predicted_snippets'] = predicted_snippets\n",
    "        \n",
    "        # Get scores for the current run\n",
    "        aggregated_metrics_predicted_snippets = evaluator.evaluate_snippets_parallel(data_dict, \"predicted_snippets\", \"refined_regions\", metrics_config)\n",
    "        all_scores.append(aggregated_metrics_predicted_snippets)\n",
    "        f1_score = aggregated_metrics_predicted_snippets['f1']\n",
    "        if f1_score > best_f1:\n",
    "            best_run['run_id'] = run_id\n",
    "            best_run['ignore_o_threshold'] = ignore_o_threshold\n",
    "            best_run['min_snippet_length'] = min_snippet_length\n",
    "            best_run['f1'] = f1_score\n",
    "            best_f1 = f1_score\n",
    "        run_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save run data to file\n",
    "import json\n",
    "\n",
    "with open('Evaluation/token_model_snippet_creation_params_all_scores.json', 'w') as outfile:\n",
    "    json.dump(all_scores, outfile, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of CYBER I & CYBER II\n",
    "(samle process as done for aml test data above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data_dict from /home/tlh45/rds/hpc-work/preprocessing/data_dicts/28259v240404_CYBER/data_dict_roberta-base-06-04.pkl.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "docReader = DocReader(MODEL, tokenizer)\n",
    "create_original_snippets = True\n",
    "add_full_page = True\n",
    "\n",
    "cyber_file_path_i = config[\"general\"]['DATA_DICT_FILE_PATH']['CYBER_I']\n",
    "cyber_file_path_ii = config[\"general\"]['DATA_DICT_FILE_PATH']['CYBER_II']\n",
    "\n",
    "if True:\n",
    "    print(f\"Load data_dict from {DATA_PATH}/{cyber_file_path_i}.\")\n",
    "    with open(f'{DATA_PATH}/{cyber_file_path_i}', 'rb') as handle:\n",
    "        data_dict_cyber_i = pickle.load(handle)\n",
    "    print(\"Done.\")\n",
    "\n",
    "if True:\n",
    "    print(f\"Load data_dict from {DATA_PATH}/{cyber_file_path_ii}.\")\n",
    "    with open(f'{DATA_PATH}/{cyber_file_path_ii}', 'rb') as handle:\n",
    "        data_dict_cyber_ii = pickle.load(handle)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bde4836d1e24d83a34d50ab89de2773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collect texts and metadata:   0%|          | 0/730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0039695de83470fb93e5a72dc5cdae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 21450\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1238c51c6e45a395cc56badccec20b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b15cb06ae14b088cb57f3fdfcc3745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07b6c62e65c421d915d9b965f7a0668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 21311\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e8a710c2aee44df8ea5f96a1ad6748c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024283d129af481c9e6f6ee1ddfdf400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da48121270340de86677373a1a566cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/4513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 9977\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5fe23d276147d49ddbf5870240277e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359fa5c93778419fa3f58e2d1470a678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/4513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664f769d3a564800809cb3f818b66314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assign results:   0%|          | 0/24513 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58942e8d62d146f687a26cba636b35cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/383 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.6701474538585565\n",
      "Average Recall Score: 0.8092387605230164\n",
      "Average F1 Score: 0.7331544523751762\n",
      "Average Iou Score: 0.7443593037861259\n",
      "Average Bleu Score: 0.6040033016575665\n",
      "Average Jaccard Score: 0.7443593037861259\n",
      "Average Precision_region_lvl Score: 0.6933163250659018\n",
      "Average Recall_region_lvl Score: 0.8724206453249644\n",
      "Average F1_region_lvl Score: 0.7104174815874686\n",
      "Average Rouge-1-f Score: 0.8330772493050197\n",
      "Average Rouge-2-f Score: 0.8059755768699488\n",
      "Average Rouge-l-f Score: 0.8316925812118936\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dea8b528a30433f8eb8e248bc1751da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collect texts and metadata:   0%|          | 0/965 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de035f40c45b46288660beee4943a040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 21184\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "891478f56dbe43e78268377e2ddacadd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be850ac787b4495b7709ae66563b9b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "303e18b6bc024c389cea589803344d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 20113\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5ddae828684b22806ff63069d6b03f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cfe0ab7f2a54c679cd9d124519b2ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a2cce12232f4cebaaea1ca4dff4392d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/9058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 18048\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555839e41f66469fa0a7251ab7120465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b210b89283469a94e55181fb10cfd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating snippets:   0%|          | 0/9058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe0ff54f134488a88fe6275d7a3da8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assign results:   0%|          | 0/29058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8be668e29f4d828f4c3cc1383d3dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.7200198200254957\n",
      "Average Recall Score: 0.803640139905733\n",
      "Average F1 Score: 0.7595353873134623\n",
      "Average Iou Score: 0.7534106807910367\n",
      "Average Bleu Score: 0.6240424194086169\n",
      "Average Jaccard Score: 0.7535112439446973\n",
      "Average Precision_region_lvl Score: 0.7285348941660446\n",
      "Average Recall_region_lvl Score: 0.8608933583949359\n",
      "Average F1_region_lvl Score: 0.727774583826841\n",
      "Average Rouge-1-f Score: 0.8391584908899389\n",
      "Average Rouge-2-f Score: 0.8119460703854102\n",
      "Average Rouge-l-f Score: 0.8377474080570861\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import importlib\n",
    "import evaluator\n",
    "importlib.reload(evaluator)\n",
    "\n",
    "metrics_config = {\n",
    "    'iou': True,\n",
    "    'bleu': True,\n",
    "    'jaccard': True,\n",
    "    'precision': True,\n",
    "    'recall': True,\n",
    "    'f1': True,\n",
    "    'precision_region_lvl': True,\n",
    "    'recall_region_lvl': True,\n",
    "    'f1_region_lvl': True,\n",
    "    'edit_distance': False,\n",
    "    'rouge-1-f': True,\n",
    "    'rouge-2-f': True,\n",
    "    'rouge-l-f': True,\n",
    "    'pk': False,\n",
    "    'windowdiff': False,\n",
    "    'cohen_kappa': False\n",
    "}\n",
    "\n",
    "for theme, data_dict in zip([\"CYBER_I\", \"CYBER_II\"], [data_dict_cyber_i, data_dict_cyber_ii]):   \n",
    "    doc_count = 0\n",
    "    page_count = 0\n",
    "    \n",
    "    # Step 1: Collect all texts and metadata for batch processing\n",
    "    all_texts = []\n",
    "    metadata = []\n",
    "    for doc_id, page_dict in tqdm(data_dict.items(), desc=\"Collect texts and metadata\"):\n",
    "        for page_index, page_content in page_dict.items():\n",
    "            if page_index in ['title', 'doc_long_id']:\n",
    "                continue\n",
    "            page_text = data_dict[doc_id][page_index]['full_text']\n",
    "            all_texts.append(page_text)\n",
    "            metadata.append((doc_id, page_index))\n",
    "            page_count += 1\n",
    "        doc_count += 1\n",
    "        \n",
    "        \n",
    "    # Split metadata and all_texts into chunks\n",
    "    # This step is necessary as the evaluation of the cyber data is considerably larger than the evaluation of the AML test data only\n",
    "    text_chunk_size = 10000\n",
    "    all_texts_chunks = [all_texts[i:i + text_chunk_size] for i in range(0, len(all_texts), text_chunk_size)]\n",
    "    total_time = 0\n",
    "    full_snippets_collection = []\n",
    "    for all_texts_chunk in all_texts_chunks:\n",
    "\n",
    "        # Step 2: Process all texts in batches\n",
    "        stride = OVERLAP\n",
    "        sub_batch_size = 512 if MODEL == \"roberta-large\" else 2048\n",
    "        if True:\n",
    "            start_time = time.time()  # Start timing\n",
    "            all_encodings, batch_input_ids, predicted_labels, detailed_predictions, tokenizer = batch_predict(token_model\n",
    "                                                                                                            , tokenizer\n",
    "                                                                                                            , all_texts_chunk\n",
    "                                                                                                            , metadata\n",
    "                                                                                                            , window_size=512\n",
    "                                                                                                            , stride = stride\n",
    "                                                                                                            , sub_batch_size=sub_batch_size)\n",
    "            end_time = time.time()  # End timing\n",
    "            total_time_chunk = end_time - start_time  # Calculate total time\n",
    "            total_time += total_time_chunk\n",
    "        # del token_model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        batch_input_ids = batch_input_ids.to(\"cpu\")\n",
    "        predicted_labels = [label.to(\"cpu\") for label in predicted_labels]\n",
    "\n",
    "        snippets_collection = create_snippets_multiprocessing(all_encodings\n",
    "                                                , batch_input_ids\n",
    "                                                , predicted_labels\n",
    "                                                , detailed_predictions\n",
    "                                                , tokenizer\n",
    "                                                , stride = stride\n",
    "                                                , min_snippet_length = 10\n",
    "                                                , ignore_o_threshold = 4\n",
    "                                                # , multiprocessing = True\n",
    "                                                , workers = 8\n",
    "                                                , chunksize = 512)\n",
    "\n",
    "        full_snippets_collection.extend(snippets_collection)\n",
    "        del snippets_collection, batch_input_ids, predicted_labels, all_encodings\n",
    "        gc.collect()\n",
    "    \n",
    "    with open(f\"full_snippets_collection_{theme}_{PRETRAINED_MODEL}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(full_snippets_collection, f)\n",
    "        \n",
    "    # Step 3: Assign results back to original data_dict\n",
    "    for ((doc_id, page_index), snippets) in tqdm(zip(metadata, full_snippets_collection), desc=\"Assign results\", total=len(metadata)):\n",
    "        snippet_texts = []\n",
    "        detailed_tags = []\n",
    "        for snippet_text, detailed_tag in snippets:\n",
    "            if snippet_text.strip():\n",
    "                snippet_texts.append(snippet_text)\n",
    "                detailed_tags.append(detailed_tag)\n",
    "        predicted_snippets = []\n",
    "\n",
    "        if snippet_texts:\n",
    "            tokenized_snippets = tokenizer(snippet_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            \n",
    "            for snippet_text, detailed_tag, snippet_tokenized_ids in zip(snippet_texts, detailed_tags, tokenized_snippets['input_ids']):\n",
    "                snippet_tokenized_text = tokenizer.convert_ids_to_tokens(snippet_tokenized_ids)\n",
    "                dict_to_add = {'text': snippet_text, 'tokenized_text': snippet_tokenized_text, 'detailed_tag': detailed_tag}\n",
    "                predicted_snippets.append(dict_to_add)\n",
    "\n",
    "        data_dict[doc_id][page_index]['predicted_snippets'] = predicted_snippets\n",
    "\n",
    "        \n",
    "    aggregated_metrics_predicted_snippets = evaluator.evaluate_snippets_parallel(data_dict, \"predicted_snippets\", \"refined_regions\", metrics_config, batch_size=64)\n",
    "    aggregated_metrics_predicted_snippets['inference_time'] = total_time\n",
    "    aggregated_metrics_predicted_snippets['pages'] = page_count\n",
    "    aggregated_metrics_predicted_snippets['batch_size'] = sub_batch_size\n",
    "    aggregated_metrics_predicted_snippets['GPU'] = torch.cuda.get_device_properties(0).name\n",
    "        \n",
    "    # Save to json\n",
    "    with open(f'Evaluation/{theme}/token-{str(MODEL).replace(\"/\", \"-\")}_{PRETRAINED_MODEL if PRETRAINED_MODEL else unique_tag}.json', 'w') as f:\n",
    "        json.dump(aggregated_metrics_predicted_snippets, f, indent=4)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for sentence-level model\n",
    "(Section 5.2 of the report)\n",
    "\n",
    "Contains the following token-level model specific components:\n",
    "- Code for dataset creation\n",
    "- Post-processing algorithm\n",
    "- Evaluation code\n",
    "\n",
    "![](../Misc/sentence_level_model.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tlh45/project/code/.venv/lib64/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2024-05-29 21:52:36.124859: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-29 21:52:38.593565: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-29 21:52:43.074640: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "from multiprocessing import cpu_count, Pool, Manager\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_from_disk\n",
    "import importlib\n",
    "\n",
    "# Preprocessing\n",
    "sys.path.append('../DataPreprocessing')\n",
    "from read_into_dicts import DocReader\n",
    "\n",
    "# Model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, EarlyStoppingCallback\n",
    "\n",
    "# WANDB\n",
    "import wandb\n",
    "import random\n",
    "import string\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DICT_FILE_PATH='preprocessing/data_dicts/28245V231219_AML/data_dict_roberta-base-10-04.pkl'\n",
      "USE_MULTIPROCESSING=True\n",
      "SPLIT_BASE='blocks'\n",
      "SENTENCE_SPLITTER_MODEL='transformer'\n",
      "DATASET='Model/datasets/sentence-level/dataset_dict_1149-docs_roberta-base-model_transformer-07-05.hf'\n",
      "USE_MULTIPROCESSING=True\n",
      "PRETRAINED_MODEL='oiHQRN'\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "with open('config.yaml', 'r') as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "\n",
    "# GENERAL:\n",
    "DEBUG = config[\"general\"][\"DEBUG\"]\n",
    "MODEL = config[\"general\"][\"MODEL\"]\n",
    "DATA_SIZE = config[\"general\"][\"DATA_SIZE\"]\n",
    "DOMAIN = config[\"general\"]['DATA_DICT_FILE_PATH'][\"DOMAIN\"]\n",
    "DATA_DICT_FILE_PATH = config[\"general\"]['DATA_DICT_FILE_PATH'][DOMAIN]\n",
    "DATA_PATH = config[\"general\"][\"DATA_PATH\"]\n",
    "SPLITS_JSON = config[\"general\"][\"SPLITS_JSON\"]\n",
    "TRAIN_SIZE = config[\"general\"][\"TRAIN_SIZE\"]\n",
    "VAL_SIZE = config[\"general\"][\"VAL_SIZE\"]\n",
    "USE_MULTIPROCESSING = config[\"general\"][\"USE_MULTIPROCESSING\"]\n",
    "SPLIT_BASE = config[\"general\"][\"SPLIT_BASE\"]\n",
    "SENTENCE_SPLITTER_MODEL = config[\"general\"][\"SENTENCE_SPLITTER_MODEL\"]\n",
    "IS_HPO_RUN = config[\"hpo\"][\"IS_HPO_RUN\"]\n",
    "\n",
    "# SENTENCE LEVEL MODEL\n",
    "DATASET = config[\"sentence_level_model\"][\"DATASET\"]\n",
    "PRETRAINED_MODEL = config[\"sentence_level_model\"][\"PRETRAINED_MODEL\"]\n",
    "CHECKPOINT = config[\"sentence_level_model\"][\"CHECKPOINT\"]\n",
    "EPOCHS = config[\"sentence_level_model\"][\"EPOCHS\"]\n",
    "EARLY_STOPPING_PATIENCE = config[\"sentence_level_model\"][\"EARLY_STOPPING_PATIENCE\"]\n",
    "BATCH_SIZE = config[\"sentence_level_model\"][\"BATCH_SIZE\"]\n",
    "USE_POS_ENCODING = config[\"sentence_level_model\"][\"USE_POS_ENCODING\"]\n",
    "ALPHA = config[\"sentence_level_model\"][\"ALPHA\"]\n",
    "HIER_LABELS_LEVELS = config[\"sentence_level_model\"][\"HIER_LABELS_LEVELS\"]  # also acts as flag if detailed models shall be used (i.e. model with auxiliary objective trained)\n",
    "DETAILED_LABEL_WEIGHTS = config[\"sentence_level_model\"][\"DETAILED_LABEL_WEIGHTS\"]\n",
    "NUMBER_OF_LEVELS = len(HIER_LABELS_LEVELS)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "sentence_model = SentenceTransformer(config[\"sentence_level_model\"][\"SENTENCE_TRANSFORMER_MODEL\"], device=\"cuda\" if DATASET else \"cpu\")\n",
    "EMBEDDING_DIMENSIONS = sentence_model.get_sentence_embedding_dimension()\n",
    "SENTENCE_MODEL_IS_BINARY = config[\"sentence_level_model\"][\"SENTENCE_MODEL_IS_BINARY\"]\n",
    "SENTENCE_MODEL_ARCHITECTURE = config[\"sentence_level_model\"][\"SENTENCE_MODEL_ARCHITECTURE\"]\n",
    "\n",
    "label_to_index = {\"O\": 0, \"B\": 1, \"I\": 2} # {\"O\": 0, \"B\": 1, \"I\": 2} # 22-03 dataset with {\"B\": 0, \"I\": 1, \"O\": 2}\n",
    "index_to_label = {v: k for k, v in label_to_index.items()} # reverse of label_to_index\n",
    "\n",
    "print(f\"{DATA_DICT_FILE_PATH=}\")\n",
    "print(f\"{USE_MULTIPROCESSING=}\")\n",
    "print(f\"{SPLIT_BASE=}\")\n",
    "print(f\"{SENTENCE_SPLITTER_MODEL=}\")\n",
    "print(f\"{DATASET=}\")\n",
    "print(f\"{USE_MULTIPROCESSING=}\")\n",
    "print(f\"{PRETRAINED_MODEL=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WANDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtlh45\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tlh45/rds/hpc-work/Model/wandb/run-20240421_130955-usq9m2m1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tlh45/mphil-project/runs/usq9m2m1/workspace' target=\"_blank\">SENTENCE-Transformer-SENTENCE-BINARY-[1]-roberta-base</a></strong> to <a href='https://wandb.ai/tlh45/mphil-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tlh45/mphil-project' target=\"_blank\">https://wandb.ai/tlh45/mphil-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tlh45/mphil-project/runs/usq9m2m1/workspace' target=\"_blank\">https://wandb.ai/tlh45/mphil-project/runs/usq9m2m1/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate a random tag of 6 characters\n",
    "unique_tag = ''.join(random.choices(string.ascii_letters + string.digits, k=6))\n",
    "os.environ['WANDB_DIR'] = f\"{DATA_PATH}/Model\"\n",
    "os.environ['WANDB_CACHE_DIR'] = f\"{DATA_PATH}/Model\"\n",
    "\n",
    "wandb_init = {\n",
    "    \"project\": config[\"wandb\"][\"WANDB_PROJECT\"],    \n",
    "    \"tags\": [unique_tag, f\"MODEL={MODEL}\", f\"DATA_SIZE={DATA_SIZE}\"],\n",
    "    \"group\": config[\"wandb\"][\"WANDB_GROUP\"],\n",
    "    \"name\": f'{config[\"wandb\"][\"WANDB_GROUP\"]}-{SENTENCE_MODEL_ARCHITECTURE}-SENTENCE-{\"BINARY\" if SENTENCE_MODEL_IS_BINARY else \"BIO\"}-{HIER_LABELS_LEVELS}-{MODEL}'\n",
    "}\n",
    "wandb.login()\n",
    "wandb.init(**wandb_init)\n",
    "config_dict = {\n",
    "    \"model\": MODEL,\n",
    "    \"data_size\": DATA_SIZE,\n",
    "    \"train_size\": TRAIN_SIZE,\n",
    "    \"val_size\": VAL_SIZE,\n",
    "    \"test_size\": 1 - TRAIN_SIZE - VAL_SIZE,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"hierarchical_labels_levels\": HIER_LABELS_LEVELS\n",
    "}\n",
    "wandb.config.update(config_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Full Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data_dict from /home/tlh45/rds/hpc-work/preprocessing/data_dicts/28245V231219_AML/data_dict_roberta-base-10-04.pkl.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "docReader = DocReader(MODEL, tokenizer)\n",
    "create_original_snippets = True\n",
    "add_full_page = False\n",
    "\n",
    "if DATA_DICT_FILE_PATH:\n",
    "    print(f\"Load data_dict from {DATA_PATH}/{DATA_DICT_FILE_PATH}.\")\n",
    "    with open(f'{DATA_PATH}/{DATA_DICT_FILE_PATH}', 'rb') as handle:\n",
    "        data_dict = pickle.load(handle)\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    data_dict = docReader.preprocess_folder(preprocess=False, folder_path=f'{DATA_PATH}/28245V231219', num_workers=min(70, cpu_count()), data_size=DATA_SIZE, chunksize=1\n",
    "                                        , extract_title=True, extract_doc_long_id=True, refine_regions=True, create_original_snippets=create_original_snippets, add_full_page=add_full_page\n",
    "                                        , data_dict_folder=f\"{DATA_PATH}/preprocessing/data_dicts\", file_name_additional_suffix=\"-22-02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Cyber data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{DATA_PATH}/{config[\"general\"][\"DATA_DICT_FILE_PATH\"][\"CYBER_I\"]}', 'rb') as handle:\n",
    "    data_dict_cyber_i = pickle.load(handle)\n",
    "    \n",
    "with open(f'{DATA_PATH}/{config[\"general\"][\"DATA_DICT_FILE_PATH\"][\"CYBER_II\"]}', 'rb') as handle:\n",
    "    data_dict_cyber_ii = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom_sentence_tokenizer\n",
    "importlib.reload(custom_sentence_tokenizer)\n",
    "\n",
    "doc_id = '22206856'\n",
    "page_id = '12'\n",
    "\n",
    "print(f\"Sentence Splitter Model chosen: {SENTENCE_SPLITTER_MODEL}\")\n",
    "sentenizer = custom_sentence_tokenizer.Sentenizer(SENTENCE_SPLITTER_MODEL, f\"{DATA_PATH}/Model/punkt_tokenizer.pkl\", \"cuda\" if DATASET else \"cpu\")\n",
    "\n",
    "\n",
    "# Below shows the two different sentence splitting options\n",
    "if SPLIT_BASE != \"blocks\":\n",
    "    text = data_dict[doc_id][page_id]['full_text']\n",
    "    sentences = sentenizer.tokenize_into_sentences(text)\n",
    "    for sentence in sentences:\n",
    "        print(\"------------- SENTENCE -------------\")\n",
    "        print(sentence)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "if SPLIT_BASE == \"blocks\":\n",
    "    blocks = data_dict[doc_id][page_id]['blocks']\n",
    "    sentences = []\n",
    "    # Iterate over each block and tokenize its text into sentences\n",
    "    for block in blocks:\n",
    "        block_text = block['text'] \n",
    "        block_sentences = sentenizer.tokenize_into_sentences(block_text)\n",
    "        sentences.extend(block_sentences) \n",
    "\n",
    "    for sentence in sentences:\n",
    "        print(\"------------- SENTENCE -------------\")\n",
    "        print(sentence)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level_0 Tags: Counter({'aml': 43085})\n",
      "Unique Level_0 Tags: 1\n",
      "Level_1 Tags: Counter({'cdd': 7645, 'customeridentification': 5187, 'definitions': 4547, 'str': 3991, 'pcp': 3426, 'riskassessment': 2341, 'penalties': 2140, 'assetfreeze': 2122, 'tfs': 2092, 'other': 2057, 'competentauthority': 2010, 'recordkeeping': 1516, 'mlro': 977, 'wiretransfer': 869, 'activitybasedsanction': 774, 'intlcooperation': 755, 'bo': 505, 'sanctionrights': 98, 'internationalcooperation': 33})\n",
      "Unique Level_1 Tags: 19\n",
      "Level_2 Tags: Counter({'other': 6436, 'verification': 3261, 'firms': 2650, 'measures': 1503, 'enhanced': 1327, 'information': 1107, 'amlprogram': 1103, 'monitoring': 1064, 'firm': 748, 'regulation': 696, 'counterparty': 686, 'fiu': 638, 'prohibition': 583, 'training': 565, 'cdd': 560, 'simplified': 557, 'reporting': 507, 'lawenforcement': 504, 'grouplevel': 490, 'identificationdocs': 483, 'correspondentbank': 444, 'highlevel': 431, 'moneylaundering': 400, 'kyc': 398, 'sanctionspermit': 394, 'obligation': 374, 'applicablefirms': 372, 'procedure': 370, 'rba': 365, 'complianceofficer': 362, 'additional': 361, 'exemption': 359, 'customerrisk': 312, 'transactions': 303, 'fortfeiture': 291, 'beneficialownership': 290, 'financialinstitutions': 288, 'audit': 281, 'terroristfinancing': 277, 'dealingassets': 274, 'legalassistance': 271, 'pep': 265, 'periodicrba': 265, 'investigation': 257, 'designation': 254, 'employees': 245, 'criminalpenalties': 241, 'newtechnology': 238, 'remoteonboarding': 237, 'obligations': 222, 'dnfbp': 220, 'collaboration': 212, 'appointment': 211, 'supplyofgoods': 205, 'internalreporting': 202, 'sanctions': 187, 'immunity': 160, 'tfs': 154, 'restriction': 151, 'access': 151, 'dnfpb': 145, 'funds': 143, 'format': 136, 'administrativepenalties': 127, 'general': 125, 'update': 120, 'customer': 115, 'supplyofservices': 115, 'supervisoryauthority': 105, 'corporateliability': 103, 'intlsanctions': 95, 'npo': 93, 'rbs': 88, 'publication': 81, 'non': 80, 'pcp': 79, 'freezeorder': 77, 'shippingaircraft': 77, 'exceptions': 76, 'legalarrangements': 76, 'legalentities': 75, 'confiscation': 74, 'contravention': 73, 'dealing': 73, 'resourcing': 72, 'delisting': 71, 'freeze': 68, 'thirdparties': 68, 'fitproper': 64, 'courtreview': 62, 'grounds': 62, 'suspiciousreporting': 60, 'register': 60, 'extradition': 58, 'correspondent': 54, 'prepenalty': 52, 'asset': 51, 'agent': 50, 'tippingoff': 50, 'commercial': 48, 'supervisory': 45, 'grievance': 44, 'civilpenalties': 43, 'mis': 42, 'immigration': 42, 'competentauthority': 39, 'technologytransfer': 37, 'correspondentbanking': 35, 'exclusion': 32, 'compensation': 31, 'lawful': 31, 'recordkeeping': 28, 'tieredkyc': 26, 'sanction': 25, 'unexplained': 24, 'corruption': 22, 'liftingsanctions': 21, 'operation': 21, 'withoutdelay': 18, 'nominee': 17, 'injunctions': 11, 'competentauthorities': 7, 'listedentity': 7, 'drugtrafficking': 7, 'proliferation': 5, 'duration': 4, 'listing': 4, 'vasp': 4, 'bribery': 3, 'armstrafficking': 1})\n",
      "Unique Level_2 Tags: 130\n",
      "Level_3 Tags: Counter({'obligation': 1576, 'offence': 938, 'reporting': 687, 'thirdparty': 622, 'meaning': 611, 'pep': 609, 'individuals': 525, 'interprocedures': 512, 'ongoingdiligence': 487, 'exposure': 481, 'higherrisk': 463, 'beneficialowners': 455, 'employees': 453, 'lowrisk': 414, 'sanction': 377, 'powers': 329, 'measures': 317, 'acceptable': 311, 'trustees': 305, 'definition': 303, 'req': 302, 'requirements': 297, 'tippingoff': 291, 'triggerevent': 274, 'extent': 255, 'identity': 240, 'information': 240, 'criteria': 226, 'authorisation': 225, 'reportingentity': 217, 'agents': 216, 'format': 213, 'function': 208, 'nonfacetoface': 205, 'amlmeasures': 203, 'timing': 202, 'riskprofiling': 201, 'procedure': 197, 'purposerelationship': 192, 'significantcontrol': 183, 'sanctions': 183, 'other': 174, 'funds': 173, 'business': 172, 'investigation': 169, 'type': 168, 'riskmitigation': 166, 'cooperation': 164, 'screeningprocedures': 163, 'sharing': 157, 'licensing': 153, 'appointment': 152, 'regulation': 142, 'shellbanks': 139, 'thresholdtransaction': 139, 'rba': 130, 'prosecutions': 127, 'corporates': 123, 'existingcustomers': 121, 'originator': 120, 'partnership': 118, 'msb': 114, 'lfs': 111, 'sroduties': 111, 'frozenfunds': 110, 'considerations': 96, 'governance': 96, 'apparent': 87, 'domestic': 85, 'accountopening': 84, 'contents': 80, 'jurisdiction': 80, 'compliance': 80, 'supervisoryauthorities': 74, 'insurance': 73, 'kyc': 71, 'fundavailability': 71, 'equivalent': 70, 'order': 69, 'obligations': 69, 'beneficiary': 68, 'orders': 61, 'additional': 60, 'export': 60, 'proportionality': 58, 'listing': 56, 'financialinstitutions': 56, 'dealing': 52, 'dnfbp': 50, 'anonymous': 49, 'reliance': 47, 'property': 47, 'particulars': 47, 'sanctionlist': 47, 'assetfreeze': 46, 'test': 45, 'international': 45, 'vigilance': 37, 'clients': 35, 'arrest': 32, 'anonymity': 32, 'data': 32, 'kyc ': 32, 'suspicioustransactions': 29, 'lawyers': 27, 'accessinfo': 25, 'restriction': 25, 'sroidentity': 25, 'closedaccounts': 25, 'sanctionthematic': 24, 'redressal': 24, 'activeaccounts': 22, 'refuseservices': 22, 'program': 20, 'cashcouriers': 20, 'import': 18, 'training': 17, 'breaches': 17, 'geographicrisk': 16, 'vasp': 16, 'grievance': 16, 'casinostresholds': 16, 'competentauthority': 16, 'realestatetresholds': 16, 'proliferationfinancing': 15, 'trust': 15, 'higherriskdnfbp': 14, 'termination': 14, 'dnfbpthirdparty': 14, 'sanctioncountry': 12, 'firms': 12, 'kycnoncustomer': 10, 'metaltransactions': 10, 'fiu': 10, 'policy': 8, 'foreignlist': 8, 'rights': 6, 'ongoing': 6, 'dnfbppep': 6, 'accountants': 6, 'fitproper': 6, 'thresholdeltransaction': 6, 'locallist': 6, 'riskexposure': 5, 'accountant': 5, 'lowvalue': 4, 'highvalue': 3, 'metalstreshold': 3, 'companyservice ': 3, 'notaries': 2, 'dnfbpriskmitigation': 2, 'frequency': 2, 'gifts': 2, 'director': 1})\n",
      "Unique Level_3 Tags: 154\n",
      "Total Unique Tags: 304\n",
      "Level_0 Mapping Dictionary: {'aml': 0}\n",
      "Level_1 Mapping Dictionary: {'str': 0, 'tfs': 1, 'definitions': 2, 'other': 3, 'cdd': 4, 'customeridentification': 5, 'riskassessment': 6, 'pcp': 7, 'recordkeeping': 8, 'competentauthority': 9, 'intlcooperation': 10, 'assetfreeze': 11, 'penalties': 12, 'wiretransfer': 13, 'mlro': 14, 'bo': 15, 'sanctionrights': 16, 'activitybasedsanction': 17, 'internationalcooperation': 18}\n",
      "Level_2 Mapping Dictionary: {'other': 0, 'firms': 1, 'reporting': 2, 'tfs': 3, 'financialinstitutions': 4, 'terroristfinancing': 5, 'customer': 6, 'pep': 7, 'beneficialownership': 8, 'applicablefirms': 9, 'measures': 10, 'obligation': 11, 'verification': 12, 'prohibition': 13, 'identificationdocs': 14, 'newtechnology': 15, 'enhanced': 16, 'counterparty': 17, 'amlprogram': 18, 'complianceofficer': 19, 'grouplevel': 20, 'immunity': 21, 'cdd': 22, 'transactions': 23, 'format': 24, 'supervisory': 25, 'regulation': 26, 'information': 27, 'collaboration': 28, 'monitoring': 29, 'dnfbp': 30, 'administrativepenalties': 31, 'lawenforcement': 32, 'legalassistance': 33, 'exemption': 34, 'remoteonboarding': 35, 'simplified': 36, 'correspondentbank': 37, 'audit': 38, 'training': 39, 'designation': 40, 'moneylaundering': 41, 'dnfpb': 42, 'firm': 43, 'highlevel': 44, 'restriction': 45, 'corporateliability': 46, 'investigation': 47, 'criminalpenalties': 48, 'appointment': 49, 'employees': 50, 'liftingsanctions': 51, 'freeze': 52, 'funds': 53, 'fiu': 54, 'procedure': 55, 'fortfeiture': 56, 'courtreview': 57, 'duration': 58, 'extradition': 59, 'suspiciousreporting': 60, 'recordkeeping': 61, 'pcp': 62, 'rba': 63, 'agent': 64, 'kyc': 65, 'additional': 66, 'periodicrba': 67, 'mis': 68, 'update': 69, 'thirdparties': 70, 'contravention': 71, 'customerrisk': 72, 'dealingassets': 73, 'exceptions': 74, 'sanctionspermit': 75, 'resourcing': 76, 'corruption': 77, 'tippingoff': 78, 'sanction': 79, 'correspondent': 80, 'publication': 81, 'non': 82, 'internalreporting': 83, 'fitproper': 84, 'grounds': 85, 'sanctions': 86, 'obligations': 87, 'freezeorder': 88, 'dealing': 89, 'access': 90, 'compensation': 91, 'general': 92, 'legalarrangements': 93, 'asset': 94, 'tieredkyc': 95, 'npo': 96, 'supplyofgoods': 97, 'supplyofservices': 98, 'intlsanctions': 99, 'shippingaircraft': 100, 'supervisoryauthority': 101, 'technologytransfer': 102, 'exclusion': 103, 'competentauthorities': 104, 'register': 105, 'commercial': 106, 'confiscation': 107, 'legalentities': 108, 'withoutdelay': 109, 'immigration': 110, 'correspondentbanking': 111, 'listing': 112, 'delisting': 113, 'prepenalty': 114, 'grievance': 115, 'lawful': 116, 'listedentity': 117, 'rbs': 118, 'operation': 119, 'civilpenalties': 120, 'vasp': 121, 'competentauthority': 122, 'nominee': 123, 'unexplained': 124, 'proliferation': 125, 'injunctions': 126, 'armstrafficking': 127, 'bribery': 128, 'drugtrafficking': 129}\n",
      "Level_3 Mapping Dictionary: {'reporting': 0, 'other': 1, 'sanctionthematic': 2, 'obligation': 3, 'assetfreeze': 4, 'format': 5, 'thirdparty': 6, 'pep': 7, 'tippingoff': 8, 'lawyers': 9, 'notaries': 10, 'cooperation': 11, 'timing': 12, 'sanction': 13, 'meaning': 14, 'regulation': 15, 'exposure': 16, 'criteria': 17, 'extent': 18, 'individuals': 19, 'accountopening': 20, 'trustees': 21, 'agents': 22, 'lowrisk': 23, 'employees': 24, 'investigation': 25, 'offence': 26, 'procedure': 27, 'interprocedures': 28, 'nonfacetoface': 29, 'riskmitigation': 30, 'considerations': 31, 'shellbanks': 32, 'originator': 33, 'beneficiary': 34, 'screeningprocedures': 35, 'partnership': 36, 'significantcontrol': 37, 'definition': 38, 'acceptable': 39, 'beneficialowners': 40, 'purposerelationship': 41, 'ongoingdiligence': 42, 'apparent': 43, 'business': 44, 'existingcustomers': 45, 'function': 46, 'contents': 47, 'thresholdtransaction': 48, 'prosecutions': 49, 'identity': 50, 'powers': 51, 'governance': 52, 'req': 53, 'information': 54, 'order': 55, 'arrest': 56, 'amlmeasures': 57, 'appointment': 58, 'measures': 59, 'lfs': 60, 'training': 61, 'equivalent': 62, 'additional': 63, 'triggerevent': 64, 'corporates': 65, 'insurance': 66, 'higherrisk': 67, 'jurisdiction': 68, 'domestic': 69, 'sharing': 70, 'reportingentity': 71, 'kycnoncustomer': 72, 'anonymous': 73, 'type': 74, 'suspicioustransactions': 75, 'reliance': 76, 'orders': 77, 'riskprofiling': 78, 'funds': 79, 'rba': 80, 'requirements': 81, 'authorisation': 82, 'policy': 83, 'compliance': 84, 'test': 85, 'listing': 86, 'breaches': 87, 'property': 88, 'program': 89, 'anonymity': 90, 'msb': 91, 'sanctioncountry': 92, 'sanctions': 93, 'frozenfunds': 94, 'licensing': 95, 'supervisoryauthorities': 96, 'data': 97, 'sroduties': 98, 'highvalue': 99, 'kyc': 100, 'geographicrisk': 101, 'export': 102, 'import': 103, 'financialinstitutions': 104, 'obligations': 105, 'proportionality': 106, 'vasp': 107, 'dnfbp': 108, 'cashcouriers': 109, 'accessinfo': 110, 'dealing': 111, 'particulars': 112, 'restriction': 113, 'fundavailability': 114, 'grievance': 115, 'rights': 116, 'redressal': 117, 'sroidentity': 118, 'metaltransactions': 119, 'kyc ': 120, 'activeaccounts': 121, 'closedaccounts': 122, 'lowvalue': 123, 'refuseservices': 124, 'ongoing': 125, 'clients': 126, 'higherriskdnfbp': 127, 'sanctionlist': 128, 'termination': 129, 'casinostresholds': 130, 'competentauthority': 131, 'vigilance': 132, 'international': 133, 'dnfbpthirdparty': 134, 'dnfbppep': 135, 'realestatetresholds': 136, 'foreignlist': 137, 'accountants': 138, 'director': 139, 'riskexposure': 140, 'fitproper': 141, 'thresholdeltransaction': 142, 'proliferationfinancing': 143, 'firms': 144, 'fiu': 145, 'locallist': 146, 'trust': 147, 'dnfbpriskmitigation': 148, 'accountant': 149, 'metalstreshold': 150, 'frequency': 151, 'companyservice ': 152, 'gifts': 153}\n",
      "Decoded Labels: ['aml-str-other-reporting', 'aml-str-firms-other']\n"
     ]
    }
   ],
   "source": [
    "import detailed_labels_handler\n",
    "\n",
    "tag_statistics, mapping_dicts = detailed_labels_handler.extract_and_analyze_tags(data_dict, HIER_LABELS_LEVELS)\n",
    "\n",
    "# Calculate unique tag counts\n",
    "unique_level_tags = {level: len(tags) for level, tags in tag_statistics.items()}\n",
    "total_unique_tags = sum(len(tags) for tags in tag_statistics.values())\n",
    "\n",
    "if not HIER_LABELS_LEVELS:\n",
    "    print(\"Note: no HIER_LABELS_LEVELS provided.\")\n",
    "# Print statistics and unique counts\n",
    "for level, tags in tag_statistics.items():\n",
    "    print(f\"{level.capitalize()} Tags:\", tags)\n",
    "    print(f\"Unique {level.capitalize()} Tags:\", unique_level_tags[level])\n",
    "\n",
    "# Print full tag count if available\n",
    "if 'full_tags' in tag_statistics:\n",
    "    print(\"Full Tags Count:\", tag_statistics['full_tags'])\n",
    "print(\"Total Unique Tags:\", total_unique_tags)\n",
    "\n",
    "# Print mapping dictionaries\n",
    "for level, mapping_dict in mapping_dicts.items():\n",
    "    print(f\"{level.capitalize()} Mapping Dictionary:\", mapping_dict)\n",
    "\n",
    "# Example encoded labels to decode\n",
    "encoded_labels = [[0, 0, 0, 0], [0, 0, 1, 1]]\n",
    "decoded_labels = detailed_labels_handler.decode_detailed_labels(encoded_labels, mapping_dicts, HIER_LABELS_LEVELS)\n",
    "print(\"Decoded Labels:\", decoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function to calculate sentence embeddings using the sentence transformer models\n",
    "# The sentence transformer models correspond to the first-level transformer network in the hierarchical sentence-level model\n",
    "# We calculate these embeddings as part of the dataset, so that they can easily be re-used and don't have to be recalculated each time\n",
    "def calculate_sentence_embeddings(sentences):\n",
    "    return sentence_model.encode(sentences, show_progress_bar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to create dataset for the sentence-level model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Datadict from /home/tlh45/rds/hpc-work/Model/datasets/sentence-level/dataset_dict_1149-docs_roberta-base-model_transformer-07-05.hf.\n",
      "Datadict successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "# Note: the dataset creation should best happen on a cpu due to multiprocessing\n",
    "from functools import partial\n",
    "import unicodedata\n",
    "\n",
    "def save_documents_to_files(data_dict, temp_folder):\n",
    "    '''\n",
    "    Helper function to save each document in the data_dict to a separate file in the temp_folder\n",
    "    '''\n",
    "    if not os.path.exists(temp_folder):\n",
    "        os.makedirs(temp_folder, exist_ok=True)\n",
    "    for doc_id, doc_data in tqdm(data_dict.items(), desc=\"Saving documents\"):\n",
    "        file_path = os.path.join(temp_folder, f\"{doc_id}.pkl\")\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(doc_data, f)\n",
    "\n",
    "def label_sentences_with_token_overlap(sentences, regions, threshold=0.9, short_sentence_length=20):\n",
    "    '''\n",
    "    Function that labels sentences based on token overlap with regions (more inaccurate, but used as fallback)\n",
    "    '''\n",
    "    bio_labels = []\n",
    "    detailed_labels = []\n",
    "    region_info = [(region['text'].split(), region.get('tags', None)) for region in regions]\n",
    "    \n",
    "    def sentence_in_region(sentence_tokens, region_tokens):\n",
    "        '''\n",
    "        Helper function to check if a sentence is contained within a region based on token overlap\n",
    "        '''\n",
    "        matched_token_count = 0\n",
    "        region_index = 0\n",
    "        for token in sentence_tokens:\n",
    "            if token in region_tokens[region_index:]:\n",
    "                matched_token_count += 1\n",
    "                region_index = region_tokens.index(token, region_index) + 1\n",
    "        match_ratio = matched_token_count / len(sentence_tokens)\n",
    "        return match_ratio >= threshold\n",
    "\n",
    "    # Iterate over each sentence and check if it is contained within a region\n",
    "    for sentence in sentences:\n",
    "        tokenized_sentence = sentence.split() # simply tokenize by whitespace\n",
    "        bio_label = 'O'\n",
    "        detailed_label = 'N/A'\n",
    "        sentence_matched = False\n",
    "\n",
    "        # For each sentence we go through all regions and check if the sentence is contained within a region\n",
    "        for region_tokens, region_tags in region_info:\n",
    "            if sentence_in_region(tokenized_sentence, region_tokens):\n",
    "                sentence_matched = True\n",
    "                if not bio_labels or bio_labels[-1] == 'O':  # If the previous sentence was outside a region\n",
    "                    bio_label = 'B'\n",
    "                else:\n",
    "                    bio_label = 'I'\n",
    "                if HIER_LABELS_LEVELS:\n",
    "                    detailed_label = ';'.join(region_tags)  # Join tags if there are multiple\n",
    "                break  # Stop checking once a match is found\n",
    "\n",
    "        if not sentence_matched and HIER_LABELS_LEVELS:\n",
    "            detailed_label = 'N/A'\n",
    "\n",
    "        bio_labels.append(bio_label)\n",
    "        if HIER_LABELS_LEVELS:\n",
    "            detailed_labels.append(detailed_label)\n",
    "            \n",
    "    numerical_bio_labels = [label_to_index[label] for label in bio_labels]\n",
    "    \n",
    "    if HIER_LABELS_LEVELS:\n",
    "        return numerical_bio_labels, detailed_labels\n",
    "    else:\n",
    "        return numerical_bio_labels\n",
    "\n",
    "def is_unwanted_character(c):\n",
    "    '''\n",
    "    Helper function to check if a character is unwanted (e.g. control characters, formatting characters, etc.)\n",
    "    '''\n",
    "    unwanted_categories = ['Cc', 'Cf', 'Co', 'Cs', 'Cn', 'Zl', 'Zp', 'Sm', 'Po']\n",
    "    if unicodedata.category(c) in unwanted_categories:\n",
    "        return True\n",
    "    unwanted_chars = ['\\xad', '\\ufeff', '[', ']', '\\\\', '/', 'ï¿½'] \n",
    "    if c in unwanted_chars:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def label_sentences_by_indices(sentences, regions, tokenized_page_text, doc_id, page_index):\n",
    "    '''\n",
    "    Function similar to the above, but labels sentences based on indices of regions in the tokenized page text.\n",
    "    This is more accurate and hence used as the primary method for labeling sentences.\n",
    "    For more details, refer to the report.\n",
    "    '''\n",
    "    try:\n",
    "        bio_labels = []\n",
    "        detailed_labels = []\n",
    "        sentence_boundaries = [] \n",
    "\n",
    "        # STEP 1: Map sentences to indices in page\n",
    "        current_string = \"\"\n",
    "        sentence_idx = 0\n",
    "        current_token_list = []\n",
    "        idx = 0 \n",
    "\n",
    "        while idx < len(tokenized_page_text):\n",
    "            token = tokenized_page_text[idx]\n",
    "            if sentence_idx >= len(sentences):\n",
    "                break\n",
    "            \n",
    "            sentence = sentences[sentence_idx]\n",
    "            current_token_list.append(token)\n",
    "            current_string = tokenizer.convert_tokens_to_string(current_token_list)\n",
    "            \n",
    "            clean_current_string = \"\".join(current_string.split())\n",
    "            clean_current_string = \"\".join([char for char in clean_current_string if not is_unwanted_character(char)])\n",
    "            \n",
    "            clean_sentence = \"\".join(sentence.split())\n",
    "            clean_sentence = \"\".join([char for char in clean_sentence if not is_unwanted_character(char)])[2:]\n",
    "\n",
    "            # Match exactly for single-token sentences\n",
    "            if len(clean_sentence) <= 1:\n",
    "                if sentence_idx == len(sentences) - 1:\n",
    "                    end_idx = len(tokenized_page_text) - 1\n",
    "                else:\n",
    "                    end_idx = idx\n",
    "                sentence_boundaries.append((idx, end_idx))\n",
    "                idx -= 1 # move back to allow last sentnce to continue matchng\n",
    "                current_token_list = []\n",
    "                sentence_idx += 1\n",
    "\n",
    "            # Match by start or end for multi-token sentences\n",
    "            elif clean_current_string.startswith(clean_sentence) or clean_current_string.endswith(clean_sentence) or clean_sentence in clean_current_string:\n",
    "                if sentence_idx == len(sentences) - 1:\n",
    "                    end_idx = len(tokenized_page_text) - 1\n",
    "                else:\n",
    "                    end_idx = idx\n",
    "                sentence_boundaries.append((idx - len(current_token_list) + 1, end_idx))\n",
    "                current_token_list = []\n",
    "                sentence_idx += 1\n",
    "            idx += 1 \n",
    "        assert len(sentence_boundaries) == len(sentences), f\"Number of sentences and boundaries do not match in {doc_id=}, {page_index=}: {len(sentences)} vs {len(sentence_boundaries)}\"\n",
    "    \n",
    "    # If we have a mismatch in the number of sentences and boundaries, fall back to overlap matching\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {doc_id=}, {page_index=}: {e}. Falling back to overlap matching.\")\n",
    "        return label_sentences_with_token_overlap(sentences, regions) if HIER_LABELS_LEVELS else (label_sentences_with_token_overlap(sentences, regions), [])\n",
    "    \n",
    "    # STEP 2: label sentences based on region indices\n",
    "    # This logic is described in more detail in Section 5.2.2 of the report\n",
    "    for (start_idx, end_idx), sentence in zip(sentence_boundaries, sentences):\n",
    "        label = 'O'  # Default label\n",
    "        detailed_label = \"N/A\"\n",
    "        for region in regions:\n",
    "            region_start_idx = region['start_idx_in_page']\n",
    "            region_end_idx = region['end_idx_in_page']\n",
    "            # Check if sentence starts inside the region or overlaps with it\n",
    "            if start_idx <= region_start_idx and end_idx > region_start_idx:\n",
    "                label = 'B'\n",
    "                detailed_label = region.get('tags', \"N/A\")\n",
    "                break\n",
    "\n",
    "            # Check if sentence is fully within the region\n",
    "            elif start_idx > region_start_idx and end_idx <= region_end_idx:\n",
    "                label = 'I'\n",
    "                detailed_label = region.get('tags', \"N/A\")\n",
    "                break\n",
    "    \n",
    "        bio_labels.append(label)\n",
    "        if HIER_LABELS_LEVELS:\n",
    "            if isinstance(detailed_label, list):\n",
    "                detailed_label = ';'.join(detailed_label)\n",
    "            detailed_labels.append(detailed_label)\n",
    "\n",
    "    # Convert labels to numerical form\n",
    "    # label_to_index = {\"B\": 0, \"I\": 1, \"O\": 2} --> use from above\n",
    "    numerical_bio_labels = [label_to_index[label] for label in bio_labels]\n",
    "    \n",
    "    if HIER_LABELS_LEVELS:\n",
    "        return numerical_bio_labels, detailed_labels\n",
    "    else:\n",
    "        return numerical_bio_labels\n",
    "\n",
    "def worker_create_training_samples(doc_id, temp_folder, save_folder):\n",
    "    '''\n",
    "    Worker function to create training samples for a single document\n",
    "    '''\n",
    "    file_path = os.path.join(temp_folder, f\"{doc_id}.pkl\")\n",
    "    with open(file_path, 'rb') as f:\n",
    "        page_data = pickle.load(f)\n",
    "    gc.collect()\n",
    "    return create_training_samples_per_document((doc_id, page_data), save_folder)\n",
    "\n",
    "def create_training_samples_per_document(doc_data, save_folder=None):\n",
    "    '''\n",
    "    Inner function to create training samples for a single document. Called by the worker function.\n",
    "    '''\n",
    "    doc_id, pages = doc_data\n",
    "    page_samples = []\n",
    "    for page_index, page_content in pages.items():\n",
    "        if page_index in [\"title\", \"doc_long_id\"]:\n",
    "            continue\n",
    "        \n",
    "        # Tokenie into sentence (default: use blocks for higher accuracy)\n",
    "        if SPLIT_BASE == \"full_text\":\n",
    "            page_text = page_content['full_text']\n",
    "            sentences = sentenizer.tokenize_into_sentences(page_text)\n",
    "        elif SPLIT_BASE == \"blocks\":\n",
    "            blocks = page_content['blocks']\n",
    "            sentences = []\n",
    "            # Iterate over each block and tokenize its text into sentences\n",
    "            for block in blocks:\n",
    "                block_text = block['text']  # Extract the text for the current block\n",
    "                block_sentences = sentenizer.tokenize_into_sentences(block_text)\n",
    "                sentences.extend(block_sentences)\n",
    "        else:\n",
    "            raise\n",
    "        \n",
    "        if not sentences:\n",
    "            continue\n",
    "            \n",
    "        # Get sentence embeddings (i.e. run through first transformer network in the hierarchical model)\n",
    "        sentence_embeddings = calculate_sentence_embeddings(sentences)\n",
    "        \n",
    "        tokenized_page_text = page_content['tokenized_full_text']\n",
    "        refined_regions = page_content['refined_regions']\n",
    "\n",
    "        # Get labels for each sentence based on region indices\n",
    "        bio_labels, detailed_labels = label_sentences_by_indices(sentences, refined_regions, tokenized_page_text, doc_id, page_index) if HIER_LABELS_LEVELS else (label_sentences_by_indices(sentences, refined_regions, tokenized_page_text, doc_id, page_index), [])\n",
    "        \n",
    "        # Create sample for each page\n",
    "        page_sample = {\n",
    "            'sentences': sentences,\n",
    "            'embeddings': [embedding.tolist() for embedding in sentence_embeddings],\n",
    "            'labels': bio_labels,\n",
    "            'detailed_labels': detailed_labels_handler.encode_detailed_labels(detailed_labels, mapping_dicts, NUMBER_OF_LEVELS),\n",
    "            'metadata': {'doc_id': doc_id, 'page_id': page_index},\n",
    "        }\n",
    "        page_samples.append(page_sample)\n",
    "    \n",
    "    if not DEBUG:\n",
    "        temp_file = os.path.join(save_folder, f\"{doc_id}_page_samples.pkl\")\n",
    "        with open(temp_file, 'wb') as f:\n",
    "            pickle.dump(page_samples, f)\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def process_documents_per_document(doc_ids_list, num_docs_dict, temp_folder, save_folder, num_workers=None, chunk_size=1):    \n",
    "    '''\n",
    "    Process documents in parallel using multiprocessing.\n",
    "    Calls the worker function to create training samples for each document.\n",
    "    '''\n",
    "    if USE_MULTIPROCESSING:\n",
    "        if num_workers is None:\n",
    "            num_workers = min(32, cpu_count())\n",
    "        print(f'{num_docs_dict-len(doc_ids_list)} valid documents found. {len(doc_ids_list)} out of {num_docs_dict} documents require processing.')\n",
    "        worker_func = partial(worker_create_training_samples, temp_folder=temp_folder, save_folder=save_folder)\n",
    "\n",
    "        if doc_ids_list:\n",
    "            with Pool(num_workers) as pool:\n",
    "                list(tqdm(pool.imap_unordered(worker_func, doc_ids_list, chunksize=chunk_size), initial=num_docs_dict-len(doc_ids_list), total=num_docs_dict, desc=\"Process documents\"))\n",
    "    else:\n",
    "        for doc_id in tqdm(doc_ids_list):\n",
    "            worker_create_training_samples(doc_id, temp_folder=temp_folder, save_folder=save_folder)\n",
    "    return save_folder\n",
    "\n",
    "def is_valid_data_file(file_path):\n",
    "    '''\n",
    "    Helper function to check if a data file is valid (i.e. contains data)\n",
    "    '''\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            return len(data) > 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def collect_results(temp_folder, valid_doc_ids):\n",
    "    '''\n",
    "    Function to collect results from the processed documents and store them in a dictionary. (necessary for memory reasons)\n",
    "    '''\n",
    "    all_training_samples = {}\n",
    "    for filename in tqdm(os.listdir(temp_folder)):\n",
    "        doc_id = filename.split('.')[0].split('_')[0]\n",
    "        if doc_id in valid_doc_ids:\n",
    "            file_path = os.path.join(temp_folder, filename)\n",
    "            if is_valid_data_file(file_path):\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    document_sample = pickle.load(f)\n",
    "                    if doc_id not in all_training_samples.keys():\n",
    "                        all_training_samples[doc_id] = [document_sample]\n",
    "                    else:\n",
    "                        all_training_samples[doc_id].extend(document_sample)\n",
    "\n",
    "    print(f\"Processed {len(all_training_samples)} items.\")\n",
    "    return all_training_samples\n",
    "\n",
    "# for debugging\n",
    "def create_samples_singleprocessing(data_dict):\n",
    "    all_training_samples = []\n",
    "    for doc_id, pages in tqdm(data_dict.items(), desc=\"Processing\"):\n",
    "        if DEBUG and doc_id != \"19865597\":  \n",
    "            continue\n",
    "        for page_index, page_content in pages.items():\n",
    "            if DEBUG and page_index != \"2\":\n",
    "                continue\n",
    "\n",
    "            samples = create_training_samples_per_document((doc_id, {page_index: page_content}))\n",
    "    return all_training_samples\n",
    "\n",
    "def filter_samples_based_on_splits(all_samples, splits):\n",
    "    '''\n",
    "    Filters samples into training, validation, and test sets based on document ID splits.\n",
    "    '''\n",
    "    \n",
    "    train_samples, val_samples, test_samples = [], [], []\n",
    "    \n",
    "    train_ids = set(splits['train_ids'])\n",
    "    val_ids = set(splits['val_ids'])\n",
    "    test_ids = set(splits['test_ids'])\n",
    "\n",
    "    for doc_id, samples in all_samples.items():\n",
    "        if doc_id in train_ids:\n",
    "            train_samples.extend(samples)\n",
    "        elif doc_id in val_ids:\n",
    "            val_samples.extend(samples)\n",
    "        elif doc_id in test_ids:\n",
    "            test_samples.extend(samples)\n",
    "\n",
    "    return train_samples, val_samples, test_samples\n",
    "\n",
    "def create_dataset_from_samples(dataset_name: str, folder_path: str):\n",
    "    '''\n",
    "    Function to create a Hugging Face Dataset from the samples stored in the folder_path.\n",
    "    '''\n",
    "    def create_df(samples):\n",
    "        flattened_samples = [page for document in samples for page in document]\n",
    "\n",
    "        df_samples = pd.DataFrame({\n",
    "            'sentences': [page['sentences'] for page in flattened_samples],\n",
    "            'embeddings': [page['embeddings'] for page in flattened_samples],\n",
    "            'bio_labels': [page['labels'] for page in flattened_samples],\n",
    "            'detailed_labels': [page['detailed_labels'] for page in flattened_samples],\n",
    "            'metadata': [page['metadata'] for page in flattened_samples],\n",
    "        })\n",
    "        return df_samples\n",
    "\n",
    "    def save_dataset_to_file(dataset, dataset_name):\n",
    "        dataset_path = f'{DATA_PATH}/Model/datasets/sentence-level/{dataset_name}_dataset.hf'\n",
    "        dataset.save_to_disk(dataset_path)\n",
    "        return dataset_path\n",
    "\n",
    "    # Process samples in batches\n",
    "    batch_files = os.listdir(folder_path)\n",
    "    num_batches = len(batch_files)\n",
    "    print(f\"Total number of batches: {num_batches}\")\n",
    "\n",
    "    dataset = None\n",
    "    for i, file in enumerate(batch_files):\n",
    "        print(f\"Processing batch {i + 1}/{num_batches}\")\n",
    "        with open(f\"{folder_path}/{file}\", \"rb\") as batch_file:\n",
    "            batch_samples = pickle.load(batch_file)\n",
    "        \n",
    "        # Convert samples to pandas DataFrame\n",
    "        df_batch = create_df(batch_samples)\n",
    "        del batch_samples\n",
    "        gc.collect()\n",
    "\n",
    "        # Convert DataFrame to Hugging Face Dataset\n",
    "        batch_dataset = Dataset.from_pandas(df_batch)\n",
    "        del df_batch\n",
    "        gc.collect()\n",
    "\n",
    "        # Append to the main dataset or save to file\n",
    "        if dataset is None:\n",
    "            dataset = batch_dataset\n",
    "        else:\n",
    "            dataset = concatenate_datasets([dataset, batch_dataset])\n",
    "\n",
    "        # Save memory by deleting the batch dataset\n",
    "        del batch_dataset\n",
    "        gc.collect()\n",
    "\n",
    "    # Save final dataset to file\n",
    "    dataset_path = save_dataset_to_file(dataset, dataset_name)\n",
    "\n",
    "    return dataset_path\n",
    "\n",
    "def save_samples_in_batches(samples, batch_size, folder):\n",
    "    '''\n",
    "    Helper function to save samples in batches to a folder\n",
    "    '''\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    num_samples = len(samples)\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, num_samples)\n",
    "        batch_samples = samples[start_idx:end_idx]\n",
    "        file_path = f\"{folder}/batch_{i}.pkl\"\n",
    "        print(f\"Saving batch {i + 1}/{num_batches} to {file_path}\")\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(batch_samples, f)\n",
    "\n",
    "\n",
    "def combine_datasets_into_dict(train_dataset_path, val_dataset_path, test_dataset_path):\n",
    "    '''\n",
    "    Function to combine the training, validation, and test datasets into a single DatasetDict.\n",
    "    '''\n",
    "    train_dataset = load_from_disk(train_dataset_path)\n",
    "    val_dataset = load_from_disk(val_dataset_path)\n",
    "    test_dataset = load_from_disk(test_dataset_path)\n",
    "    \n",
    "    dataset_dict = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'validation': val_dataset,\n",
    "        'test': test_dataset\n",
    "    })\n",
    "    return dataset_dict\n",
    "\n",
    "\n",
    "# Main entry point for the dataset creation\n",
    "\n",
    "DEBUG = False\n",
    "dataset_dict = None\n",
    "\n",
    "if DATASET is not None:\n",
    "    print(f\"Load Datadict from {DATA_PATH}/{DATASET}.\")\n",
    "    dataset_dict = load_from_disk(f\"{DATA_PATH}/{DATASET}\")\n",
    "    print(\"Datadict successfully loaded.\")\n",
    "else:\n",
    "    if DEBUG:\n",
    "        all_training_samples = create_samples_singleprocessing(data_dict)\n",
    "    else:\n",
    "        temp_folder = f\"{DATA_PATH}/preprocessing/temp_data_dict_singles\"\n",
    "        save_folder = f\"{DATA_PATH}/preprocessing/dataset_files\"\n",
    "        os.makedirs(save_folder, exist_ok=True) \n",
    "\n",
    "        valid_doc_ids = set(data_dict.keys())\n",
    "\n",
    "        # Initialize a set for documents that need to be processed\n",
    "        docs_to_process = set()\n",
    "\n",
    "        # Check each document to see if it needs processing\n",
    "        print('Check which documents need processing.')\n",
    "        print(f'You might want to empty the {save_folder=} to force preprocessing from scratch.')\n",
    "        for doc_id in tqdm(data_dict.keys()):\n",
    "            file_path = os.path.join(save_folder, f\"{doc_id}_page_samples.pkl\")\n",
    "            # Check if file exists and contains valid data\n",
    "            if not os.path.exists(file_path) or not is_valid_data_file(file_path):\n",
    "                docs_to_process.add(doc_id)\n",
    "        \n",
    "        doc_ids_list = list(docs_to_process)\n",
    "        num_docs_dict = len(data_dict.keys())\n",
    "        print(f\"{len(doc_ids_list)} out of {num_docs_dict} documents need to be processed.\")\n",
    "\n",
    "        print(f\"Save data_dict to individual files for memory reduction. (Folder: {temp_folder})\")\n",
    "        save_documents_to_files(data_dict, temp_folder)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        # 8 and 1 works quite okay\n",
    "        process_documents_per_document(doc_ids_list, num_docs_dict, temp_folder, save_folder, num_workers=110, chunk_size=1) #100\n",
    "\n",
    "        all_training_samples_document_level_dict = collect_results(save_folder, valid_doc_ids)\n",
    "\n",
    "        print(f\"Total number of documents collected: {len(all_training_samples_document_level_dict)}\")\n",
    "        print(f\"Load split information from {SPLITS_JSON}.\")\n",
    "        with open(SPLITS_JSON, 'r') as file:\n",
    "            document_splits = json.load(file)\n",
    "\n",
    "        # below we keep triggering the garbage collector to avoid memory issues\n",
    "        train_samples, val_samples, test_samples = filter_samples_based_on_splits(all_training_samples_document_level_dict, document_splits)\n",
    "        \n",
    "        del all_training_samples_document_level_dict\n",
    "        gc.collect()\n",
    "        \n",
    "        batch_size = 200 # documents\n",
    "        save_samples_in_batches(train_samples, batch_size=batch_size, folder = f'{DATA_PATH}/Model/datasets/sentence-level/train_samples_{DATA_SIZE}-docs_{batch_size}_samples')\n",
    "        del train_samples\n",
    "        gc.collect()\n",
    "        save_samples_in_batches(val_samples, batch_size=batch_size, folder = f'{DATA_PATH}/Model/datasets/sentence-level/val_samples_{DATA_SIZE}-docs_{batch_size}_samples')\n",
    "        del val_samples\n",
    "        gc.collect()\n",
    "        save_samples_in_batches(test_samples, batch_size=batch_size, folder = f'{DATA_PATH}/Model/datasets/sentence-level/test_samples_{DATA_SIZE}-docs_{batch_size}_samples')\n",
    "        del test_samples\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"Create single datasets.\")\n",
    "        train_dataset_path = create_dataset_from_samples(\"train\", f'{DATA_PATH}/Model/datasets/sentence-level/train_samples_{DATA_SIZE}-docs_{batch_size}_samples')\n",
    "        validation_dataset_path = create_dataset_from_samples(\"validation\", f'{DATA_PATH}/Model/datasets/sentence-level/val_samples_{DATA_SIZE}-docs_{batch_size}_samples')\n",
    "        test_dataset_path = create_dataset_from_samples(\"test\", f'{DATA_PATH}/Model/datasets/sentence-level/test_samples_{DATA_SIZE}-docs_{batch_size}_samples')\n",
    "\n",
    "        print(\"Create dataset_dict.\")\n",
    "        dataset_dict = combine_datasets_into_dict(train_dataset_path, validation_dataset_path, test_dataset_path)\n",
    "\n",
    "        # Save the dataset\n",
    "        file_path = f'{DATA_PATH}/Model/datasets/sentence-level/dataset_dict_{DATA_SIZE}-docs_{MODEL.replace(\"/\", \"-\")}-model_{SENTENCE_SPLITTER_MODEL}-07-05.hf'\n",
    "        print(f'Saving dataset_dict to {file_path}')\n",
    "        dataset_dict.save_to_disk(file_path)\n",
    "        print(\"Dataset successfully saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manually analyse preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Document 27363191 is in the test dataset.\n"
     ]
    }
   ],
   "source": [
    "def check_dataset_of_id(doc_id:str):\n",
    "    for name in ['train', 'validation', 'test']:\n",
    "        for metadata_entry in dataset_dict[name]['metadata']:\n",
    "            if doc_id == metadata_entry['doc_id']:\n",
    "                print(f\" Document {doc_id} is in the {name} dataset.\")\n",
    "                break\n",
    "\n",
    "doc_id = \"27363191\"\n",
    "check_dataset_of_id(doc_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to display sample for a specific page in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e0403965e1467999041deef7cc7b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6908 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################ Sentence: ################################\n",
      "\n",
      "3\n",
      "\n",
      "######## Label: O | Detailed Label:  ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "1. 14\n",
      "\n",
      "######## Label: O | Detailed Label:  ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "\" provision of correspondent banking \" to reflect our interpretation that the intent is to refer to customers of RFIs which are themselves Banks and are using the RFI to provide correspondent banking services to them.\n",
      "\n",
      "######## Label: O | Detailed Label:  ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "1. 19\n",
      "\n",
      "######## Label: O | Detailed Label:  ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "We fully support the sentiment expressed here, but would note our concerns about the lack of \" Safe Harbour \" provisions within the Legislative framework in the context of the desire to \" share data across the AML / ATF community to reduce harm \".\n",
      "\n",
      "######## Label: O | Detailed Label:  ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "We would request explicit guidance to indicate what \" data \" may be shared in what circumstances to avoid potential conflicts with \" tipping off ' provisions and guidance.\n",
      "\n",
      "######## Label: O | Detailed Label:  ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "The Authority will share anonymised data and is cognisant of the tipping â off provisions.\n",
      "\n",
      "######## Label: B | Detailed Label: aml-str-firms-tippingoff ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "The Authority will not share data that may lead to tipping â off.\n",
      "\n",
      "######## Label: I | Detailed Label: aml-str-firms-tippingoff ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "Examples of data shared can be reports on surveys conducted.\n",
      "\n",
      "######## Label: I | Detailed Label: aml-str-firms-tippingoff ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "1. 20 - 1. 21\n",
      "\n",
      "######## Label: O | Detailed Label:  ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "The Guidance Notes require the Compliance Officer to be appointed to a senior management position.\n",
      "\n",
      "######## Label: O | Detailed Label:  ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "As defined under paragraph 1. 20 this is either a Director or Senior Executive level.\n",
      "\n",
      "######## Label: O | Detailed Label:  ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "Whilst larger, more complex banks may warrant scope and affordability of this at an executive level position, this additional executive post will prove extremely onerous to the smaller, less complex banks.\n",
      "\n",
      "######## Label: O | Detailed Label:  ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "We request this be amended to conform with the regulation 18A ( l ) which stipulates this appointment be ' at managerial level '.\n",
      "\n",
      "######## Label: O | Detailed Label:  ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "To ensure unimpeded access between Executive, Board and the Compliance Officer the sub - Committee recommends this be particularly reflected in Guidance Notes.\n",
      "\n",
      "######## Label: O | Detailed Label:  ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "We agree with the comment and will amend the AML / ATF GN to state that the compliance officer be appointed at the managerial level who reports to senior management and remove all references that the compliance officer be from senior management.\n",
      "\n",
      "######## Label: O | Detailed Label:  ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "1. 29 - 1. 35\n",
      "\n",
      "######## Label: O | Detailed Label:  ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "Please provide clarification whether the required AML policy statement must be submitted to the BMA for approval.\n",
      "\n",
      "######## Label: O | Detailed Label:  ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "The policy statement does not need to be submitted to the Authority for approval.\n",
      "\n",
      "######## Label: O | Detailed Label:  ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "1. 38\n",
      "\n",
      "######## Label: O | Detailed Label:  ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "We would appreciate guidance on what would constitute appropriate qualifications for a member of the RFI ' s staff to demonstrate they are \" qualified \" to fulfil the Reporting Officer role.\n",
      "\n",
      "######## Label: B | Detailed Label: aml-pcp-employees-screeningprocedures ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "The Authority will not clarify what is deemed â qualified â other than the person is fit and proper for the role.\n",
      "\n",
      "######## Label: I | Detailed Label: aml-pcp-employees-screeningprocedures ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "Paragraph 1. 72 - 1. 73 requires the RFI to screen employees & third parties to ensure they are competent for the role.\n",
      "\n",
      "######## Label: I | Detailed Label: aml-pcp-employees-screeningprocedures ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "Given the varying complexities and risk profiles of RFIs, the RFI needs to determine and ensure that the person appointed can competently perform the role / activity.\n",
      "\n",
      "######## Label: I | Detailed Label: aml-pcp-employees-screeningprocedures ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "1. 50.\n",
      "\n",
      "######## Label: I | Detailed Label: aml-pcp-employees-screeningprocedures ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "1. 51\n",
      "\n",
      "######## Label: I | Detailed Label: aml-pcp-employees-screeningprocedures ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "It is unclear if the compliance report under paragraph 1. 50 is to be an exception or standard report per paragraph 1. 51, i. e. do the periodic reports constitute satisfaction of the ' at least once a year ' stipulation?\n",
      "\n",
      "######## Label: O | Detailed Label:  ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "The exception report should form part of the standard report.\n",
      "\n",
      "######## Label: B | Detailed Label: aml-str-other ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "This periodic report, as stated in Paragraph 1. 50, should be done minimally once per year, however it can be more frequently as determined by the RFI.\n",
      "\n",
      "######## Label: I | Detailed Label: aml-str-other ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "1. 50 - 1. 53\n",
      "\n",
      "######## Label: O | Detailed Label:  ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "Company [ name redacted ] is concerned with the breadth of the yearly periodic report, both in what the report must encompass and the frequency of the report.\n",
      "\n",
      "######## Label: O | Detailed Label:  ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "The Authority expects senior management to be advised of the items in Paragraph 1. 51, and minimally at least once per year.\n",
      "\n",
      "######## Label: O | Detailed Label:  ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "1. 57 - 1. 58, 1. 64 - 1. 65\n",
      "\n",
      "######## Label: O | Detailed Label:  ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "Paragraph 5. 140\n",
      "\n",
      "######## Label: O | Detailed Label:  ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "acknowledges that foreign branches and majority owned subsidiaries of the group apply AML / ATF measures that are consistent with the group â s home country AML / ATF requirements.\n",
      "\n",
      "######## Label: O | Detailed Label:  ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "This is typically achieved by adopting a Group AML / ATF Programme based on the\n",
      "\n",
      "######## Label: B | Detailed Label: aml-pcp-grouplevel-amlmeasures ########n\n",
      "################################ Sentence: ################################\n",
      "\n",
      "Where Bermuda is the host jurisdiction and the parent company â s home jurisdiction is AML / ATF complaint and is on par with or exceeds Bermuda standards â the Authority may\n",
      "\n",
      "######## Label: O | Detailed Label:  ########n\n"
     ]
    }
   ],
   "source": [
    "def display_sentence_labels(dataset, doc_id, page_id, mapping_dicts):\n",
    "    page_id = str(page_id)\n",
    "\n",
    "    for i, sample in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "        metadata = sample['metadata']\n",
    "        if metadata['doc_id'] == doc_id and metadata['page_id'] == page_id:\n",
    "            for sentence, bio_label, detailed_label in zip(sample['sentences'], sample['bio_labels'], sample['detailed_labels']):\n",
    "                label = index_to_label[bio_label]\n",
    "                filtered_detailed_labels = [detailed_label[index] for index in HIER_LABELS_LEVELS if index < len(detailed_label)]\n",
    "                detailed_label = detailed_labels_handler.decode_detailed_labels([filtered_detailed_labels], mapping_dicts, HIER_LABELS_LEVELS)\n",
    "                print(\"################################ Sentence: ################################\")\n",
    "                print(f\"\\n{sentence}\\n\")\n",
    "                print(f\"######## Label: {label} | Detailed Label: {detailed_label} ########n\")\n",
    "            break\n",
    "\n",
    "doc_id = \"27363191\"\n",
    "page_id = \"3\"\n",
    "display_sentence_labels(dataset_dict['test'], doc_id, page_id, mapping_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oiHQRN'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRETRAINED_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tlh45/project/code/.venv/lib64/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully loaded from /home/tlh45/rds/hpc-work/Model/results/saved_model/oiHQRN/torch_model.pth.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import sentence_level_models as SLM\n",
    "importlib.reload(SLM)\n",
    "\n",
    "if PRETRAINED_MODEL:   \n",
    "    # paths\n",
    "    model_load_path = f\"{DATA_PATH}/Model/results/saved_model/{PRETRAINED_MODEL}/torch_model.pth\"\n",
    "    config_save_path = os.path.join(os.path.dirname(model_load_path), \"config.json\")\n",
    "    \n",
    "    # Load configuration dictionary from the JSON file\n",
    "    with open(config_save_path, 'r') as config_file:\n",
    "        loaded_config = json.load(config_file)\n",
    "\n",
    "    model_type = loaded_config.pop(\"model_type\")\n",
    "\n",
    "    # Use loaded configuration to recreate the model instance\n",
    "    if model_type in [\"BiLSTM\", \"BiLSTM_CRF\"]:\n",
    "        loaded_model = SLM.SentenceTaggingBiLSTM(**loaded_config)\n",
    "    elif model_type == \"Transformer\":\n",
    "        loaded_config.pop(\"bidirectional\")\n",
    "        loaded_model = SLM.STATO(**loaded_config, positional_encoding=USE_POS_ENCODING)\n",
    "\n",
    "    # Get model weights from saved file\n",
    "    loaded_model.load_state_dict(torch.load(model_load_path))\n",
    "    loaded_model.eval()\n",
    "    \n",
    "    print(f\"Model successfully loaded from {model_load_path}.\")\n",
    "    \n",
    "    model = loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code to make predictions using the sentence-level model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "from detailed_labels_handler import decode_detailed_labels\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def predict_page(sentences, sentence_embeddings=None, prob_threshold = None):\n",
    "    '''\n",
    "    Function to predict the labels for a page of sentences using the sentence-level model.\n",
    "    '''\n",
    "    \n",
    "    # Calculate sentence embeddings\n",
    "    if sentence_embeddings is None:\n",
    "        sentence_embeddings = calculate_sentence_embeddings(sentences)\n",
    "    sentence_tensors = [torch.tensor(embedding, dtype=torch.float) for embedding in sentence_embeddings]\n",
    "    \n",
    "    if not sentence_tensors:\n",
    "        print(\"Warning: No sentences or sentence embeddings found. Skipping...\")\n",
    "        \n",
    "        \n",
    "    # Convert list of embeddings into a single tensor\n",
    "    inputs = torch.stack(sentence_tensors).to(device)\n",
    "    \n",
    "    inputs = inputs.unsqueeze(1)  # Add batch dimension\n",
    "\n",
    "    # Get predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits, *model_outputs = model(inputs)\n",
    "        detailed_logits = model_outputs[0] if model_outputs else None\n",
    "        \n",
    "    # Handle shape of logits to properly extract probabilities\n",
    "    if logits.dim() == 3 and logits.shape[1] == 1:  # Shape [seq_len, 1, num_classes]\n",
    "        bio_probabilities = softmax(logits, dim=-1).squeeze(1)  # Remove batch dimension, now [seq_len, num_classes]\n",
    "    else:\n",
    "        bio_probabilities = softmax(logits, dim=-1)\n",
    "        \n",
    "    bio_probability_lists = [probs.tolist() for probs in bio_probabilities]\n",
    "    \n",
    "    # Get BIO labels (main objective) based on threshold\n",
    "    predicted_bio_tags = []\n",
    "    for probs in bio_probabilities:\n",
    "        if prob_threshold and probs[1] > prob_threshold: \n",
    "            predicted_bio_tags.append(index_to_label[1])\n",
    "        else:\n",
    "            predicted_label_index = probs.argmax().item()\n",
    "            predicted_bio_tags.append(index_to_label[predicted_label_index])\n",
    "    \n",
    "    # Get detailed labels (auxiliary objective) if available\n",
    "    detailed_predictions = []\n",
    "    if detailed_logits is not None and HIER_LABELS_LEVELS:\n",
    "        encoded_detailed_labels_per_sentence = [[] for _ in range(len(sentences))]\n",
    "        for level_logits in detailed_logits:\n",
    "            level_probabilities = softmax(level_logits, dim=-1)\n",
    "            predicted_detailed_labels = torch.argmax(level_probabilities, dim=-1).squeeze()\n",
    "            if predicted_detailed_labels.dim() == 0:\n",
    "                # in case of only one senetnce --> scalar\n",
    "                encoded_detailed_labels_per_sentence[0].append(predicted_detailed_labels.item())\n",
    "            else:\n",
    "                # Otherwise, iterate as normal\n",
    "                for sentence_idx, label in enumerate(predicted_detailed_labels):\n",
    "                    encoded_detailed_labels_per_sentence[sentence_idx].append(label.item())\n",
    "\n",
    "        # decode detailed labels using helper function\n",
    "        for encoded_labels in encoded_detailed_labels_per_sentence:\n",
    "            decoded_label = decode_detailed_labels([encoded_labels], mapping_dicts, HIER_LABELS_LEVELS)\n",
    "            detailed_predictions.append(decoded_label)\n",
    "    else:\n",
    "        detailed_predictions = [\"N/A\" for _ in sentences]\n",
    "\n",
    "    # zip predictions data together\n",
    "    sentence_tag_details = list(zip(sentences, predicted_bio_tags, detailed_predictions, bio_probability_lists))\n",
    "\n",
    "    return sentence_tag_details\n",
    "\n",
    "# Exemplary usage, using a page from the data_dict (not available here due to confidentiaity reasons)\n",
    "doc_id = \"27367085\"\n",
    "page_index = \"2\"\n",
    "\n",
    "# Split page into  sentences using either fulltext or blocks\n",
    "if SPLIT_BASE == \"full_text\":\n",
    "    page_text = data_dict[doc_id][page_index]['full_text']\n",
    "    sentences = sentenizer.tokenize_into_sentences(page_text)\n",
    "elif SPLIT_BASE == \"blocks\":\n",
    "    blocks = data_dict[doc_id][page_index]['blocks']\n",
    "    sentences = []\n",
    "    # Iterate over each block and tokenize its text into sentences\n",
    "    for block in blocks:\n",
    "        block_text = block['text']  # Extract the text for the current block\n",
    "        block_sentences = sentenizer.tokenize_into_sentences(block_text)\n",
    "        sentences.extend(block_sentences)\n",
    "else:\n",
    "    raise\n",
    "\n",
    "# Get predictions for the page\n",
    "sentence_tag_details = predict_page(sentences, prob_threshold=0.4)\n",
    "\n",
    "# Create snippets (cell below must be run first)\n",
    "snippets = create_snippets(sentence_tag_details, look_ahead=0)\n",
    "\n",
    "# Print\n",
    "for snippet in snippets:\n",
    "    print(\"-------------------\")\n",
    "    print(snippet)\n",
    "print(\"-\" * 100)\n",
    "# Print each sentence with its predicted BIO tag and detailed labels\n",
    "for sentence, bio_tag, detailed_tags_str, bio_probs in sentence_tag_details:\n",
    "    print(f\"Sentence: {sentence[:100]}...\")\n",
    "    print(f\"BIO Tag: {bio_tag} ({bio_probs})\")\n",
    "    print(f\"Detailed Tags: aml-{detailed_tags_str}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'customeridentification'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing decoding of detailed labels\n",
    "decode_detailed_labels([[5, 35]], mapping_dicts=mapping_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert into snippets and evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom post-processing function for the sentence-level model to create snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_snippets(sentence_tag_pairs, look_ahead=0):\n",
    "    snippets = []\n",
    "    current_snippet = []\n",
    "    buffer = []\n",
    "\n",
    "    def get_upcoming_tags(index, look_ahead, pairs):\n",
    "        '''\n",
    "        Helper to get the upcoming tags for a given index and look-ahead window\n",
    "        '''\n",
    "        return [pair[1] for pair in pairs[index + 1:index + 1 + look_ahead]]\n",
    "\n",
    "    for index, (sentence, tag, *_) in enumerate(sentence_tag_pairs):\n",
    "        # Handle either binary or BIO case\n",
    "        # \"B\" is the beginning of a snippet\n",
    "        # \"I\" is the continuation of a snippet\n",
    "        # \"O\" is outside of a snippet\n",
    "        # For \"O\", we check if the buffer contains a snippet and if the upcoming tags do not contain \"B\" or \"I\"\n",
    "        if SENTENCE_MODEL_IS_BINARY:\n",
    "            if tag == \"B\":\n",
    "                current_snippet.append(sentence)\n",
    "            elif tag == \"O\":\n",
    "                if current_snippet:\n",
    "                    buffer.append(sentence)\n",
    "                    upcoming_tags = get_upcoming_tags(index, look_ahead, sentence_tag_pairs)\n",
    "                    if 'B' not in upcoming_tags:\n",
    "                        current_snippet.extend(buffer)\n",
    "                        snippets.append(' '.join(current_snippet))\n",
    "                        current_snippet = []\n",
    "                        buffer = []\n",
    "                    else:\n",
    "                        current_snippet.extend(buffer)\n",
    "                        buffer = []\n",
    "        else:\n",
    "            if tag == \"B\":\n",
    "                if current_snippet:\n",
    "                    snippets.append(' '.join(current_snippet))\n",
    "                    current_snippet = []\n",
    "                current_snippet.extend(buffer)\n",
    "                buffer = []\n",
    "                current_snippet.append(sentence)\n",
    "            elif tag == \"I\":\n",
    "                current_snippet.extend(buffer)\n",
    "                buffer = []\n",
    "                current_snippet.append(sentence)\n",
    "            elif tag == \"O\":\n",
    "                if current_snippet:\n",
    "                    buffer.append(sentence)\n",
    "                    upcoming_tags = get_upcoming_tags(index, look_ahead, sentence_tag_pairs)\n",
    "                    if 'B' not in upcoming_tags and 'I' not in upcoming_tags:\n",
    "                        current_snippet.extend(buffer)\n",
    "                        snippets.append(' '.join(current_snippet))\n",
    "                        current_snippet = []\n",
    "                        buffer = []\n",
    "                    else:\n",
    "                        current_snippet.extend(buffer)\n",
    "                        buffer = []\n",
    "\n",
    "    # Handle any remaining snippets\n",
    "    if current_snippet:\n",
    "        current_snippet.extend(buffer)\n",
    "        snippets.append(' '.join(current_snippet))\n",
    "\n",
    "    return snippets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect doc_ids and pages of test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df4c32441a56406fb522d6a205de5997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6908 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'20541584': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81'], '20277318': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163'], '22670729': ['0', '1', '2', '3', '4', '5', '6'], '26349157': ['0', '1', '2', '3', '4'], '22763928': ['0', '1', '2', '3', '4', '5', '6'], '20518933': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107'], '20552382': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10'], '20541933': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50'], '17463996': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14'], '25663731': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208'], '22328152': ['0', '1', '2'], '19915714': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34'], '24901055': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], '20586791': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10'], '21977611': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33'], '20482099': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38'], '22588069': ['0', '1', '2'], '21742915': ['0', '2', '4', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17'], '27768326': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30'], '17443351': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], '20585275': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34'], '22897084': ['0', '1', '2', '3'], '22020617': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74'], '20933298': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], '19878678': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43'], '20727225': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53'], '21961580': ['0', '1', '2', '3', '4', '5'], '25921453': ['0'], '20964525': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96'], '26362223': ['0', '1'], '21984087': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], '20005327': ['0', '1', '2', '3', '4', '5', '6'], '27363191': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18'], '22589981': ['0', '1', '2'], '17288753': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35'], '17464867': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25'], '27606369': ['0', '1', '2', '3', '4'], '26629918': ['0', '1', '2'], '20238840': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54'], '17436964': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42'], '20125201': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15'], '19878014': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20'], '20516734': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41'], '24979267': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77'], '18384589': ['0', '2', '3', '4', '5', '6', '7', '8', '9', '10', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151'], '22035581': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90'], '22344282': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15'], '20237404': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76'], '25022146': ['0', '1', '2', '3', '4', '5', '6'], '22355765': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11'], '22389811': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98'], '22683449': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '19'], '19652709': ['0', '1', '2', '3', '4', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '152', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184'], '19180743': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102'], '17327384': ['0', '1', '2', '3', '4'], '24899980': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10'], '20749604': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], '22057835': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', '384', '385', '386', '387', '388', '389', '390', '391', '392', '393', '394', '395', '396', '397', '398', '399', '400', '401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '417', '418', '419', '420', '421', '422', '423', '424', '425', '426', '427', '428', '429', '430', '431', '432', '433'], '22673873': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49'], '26369414': ['0', '1'], '20896104': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157'], '20584932': ['0', '2', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43'], '20775630': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18'], '19956318': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45'], '20529447': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58'], '17327278': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44'], '23236402': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], '25975161': ['0', '1', '2', '3'], '25514187': ['0', '1', '2', '3'], '26349760': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], '25397198': ['0', '1', '2', '3', '4', '5'], '23032540': ['0', '1', '2', '3', '4', '5', '6', '7'], '23967461': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19'], '25028856': ['0', '1', '2', '3', '4', '5', '6', '7'], '17256448': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35'], '21647665': ['0'], '26349206': ['0', '1', '2', '3'], '19073734': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83'], '20516186': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57'], '20545737': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187'], '22031499': ['0', '1', '2', '3', '4', '5', '6', '7', '8'], '22327359': ['0', '1', '2', '3', '4'], '22163997': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17'], '25089855': ['0', '1', '2', '3', '4'], '26379220': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40'], '23300080': ['0', '1', '2'], '23967054': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19'], '17423567': ['0', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18'], '20537645': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46'], '17434617': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], '23015474': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11'], '22345529': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28'], '24903205': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11'], '20954200': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103'], '22370345': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43'], '22587202': ['0', '1', '2', '3', '4', '5'], '27319298': ['0', '1', '2', '3', '4', '5'], '20599451': ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71'], '25609721': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16'], '22206856': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40'], '22207010': ['0', '1', '2', '3', '4'], '22655321': ['0', '1', '2', '3', '4', '5', '6', '7'], '27250743': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21'], '20553655': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], '24818318': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11'], '23301817': ['0', '1', '2', '3'], '17261805': ['0', '1', '2', '3', '4', '5', '6', '7', '8'], '22066857': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45'], '23297055': ['0', '1', '2', '3'], '20507366': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10'], '27764290': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21'], '19332218': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75'], '23125862': ['0', '1', '2'], '22327553': ['0', '1', '2', '3', '4', '5'], '20032040': ['0', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67'], '22241592': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27'], '22670537': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57'], '25540124': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24'], '21973170': ['0', '1', '2'], '20829680': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229'], '22673491': ['0', '1', '2', '3', '4', '5', '6', '7'], '20553950': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106'], '23153136': ['0', '1', '2', '3', '4'], '26623069': ['0', '1'], '26349060': ['0', '1', '2', '3'], '20831958': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248'], '20549197': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57'], '25397300': ['0', '1', '2'], '27674598': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42'], '25396558': ['0', '1', '2', '3', '4'], '17330210': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32'], '20783878': ['0', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24'], '21016674': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148'], '18438905': ['0', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84'], '17433849': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11'], '17468405': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14'], '20896248': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119'], '22372809': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189'], '25470593': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132'], '21506126': ['0'], '23125840': ['0', '1', '2'], '20550555': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79'], '19855885': ['0', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17'], '21568646': ['0'], '19229964': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109'], '24542494': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28'], '23303155': ['0', '1', '2', '3', '4'], '22879037': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23'], '22195940': ['0', '1', '2', '3', '4', '5', '6', '7', '8'], '20229625': ['0', '1', '2', '3', '4', '5', '6', '7'], '25609839': ['0', '1', '2', '3'], '17258397': ['0', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13'], '26493766': ['0', '1', '2', '3', '4', '5'], '25020089': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], '20772759': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56'], '20513362': ['0', '1', '2', '3', '4', '5', '6', '7'], '22185878': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81'], '20125196': ['0', '1', '2', '3', '4', '5', '6', '7'], '24902593': ['0'], '22383937': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45'], '21968455': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36'], '25019428': ['0', '1', '2', '3', '5'], '19937806': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29'], '20106601': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72'], '22240601': ['0', '1', '2', '3', '4', '5', '6', '7'], '25930300': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19'], '22947225': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], '20584552': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14'], '22683511': ['0', '1', '2', '3', '4', '5', '6', '7'], '17223398': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36'], '25288167': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71'], '27345847': ['0', '1', '2', '3', '4', '5', '6'], '17293979': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21']}\n"
     ]
    }
   ],
   "source": [
    "test_docs_pages = {}\n",
    "\n",
    "for sample in tqdm(dataset_dict['test']):\n",
    "    # Extract document ID and page index from each sample\n",
    "    doc_id = sample['metadata']['doc_id']\n",
    "    page_index = sample['metadata']['page_id'] \n",
    "    if doc_id not in test_docs_pages:\n",
    "        test_docs_pages[doc_id] = [page_index]\n",
    "    else:\n",
    "        # If the document ID already in the dictionary, append current page to its list if not already present\n",
    "        if page_index not in test_docs_pages[doc_id]:\n",
    "            test_docs_pages[doc_id].append(page_index)\n",
    "# sort pages\n",
    "for doc_id in test_docs_pages.keys():\n",
    "    test_docs_pages[doc_id] = sorted(test_docs_pages[doc_id], key=lambda x: int(x))\n",
    "\n",
    "print(test_docs_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_snippets_path = f'{DATA_PATH}/predicted_snippets/predicted_snippets_sentence_level_{DOMAIN}_17-05.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05e41234aea42308e8277f51ae63111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (612 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 129.26 seconds\n",
      "Predicted snippets saved to /home/tlh45/rds/hpc-work/predicted_snippets/predicted_snippets_sentence_level_AML_17-05.pkl.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Initialise new dictionary for storing predicted snippets\n",
    "predicted_snippets_dict = {}\n",
    "total_time = 0\n",
    "number_of_documents = len(test_docs_pages)\n",
    "number_of_pages = 0\n",
    "\n",
    "for doc_id in tqdm(test_docs_pages):\n",
    "    if doc_id not in predicted_snippets_dict:\n",
    "        predicted_snippets_dict[doc_id] = {}\n",
    "    \n",
    "    for page_index in test_docs_pages[doc_id]:\n",
    "        \n",
    "        # 1. Split the page into sentences\n",
    "        if SPLIT_BASE == \"full_text\":\n",
    "            page_text = data_dict[doc_id][page_index]['full_text']\n",
    "            sentences = sentenizer.tokenize_into_sentences(page_text)\n",
    "        elif SPLIT_BASE == \"blocks\":\n",
    "            blocks = data_dict[doc_id][page_index]['blocks']\n",
    "            sentences = []\n",
    "            for block in blocks:\n",
    "                block_text = block['text']\n",
    "                block_sentences = sentenizer.tokenize_into_sentences(block_text)\n",
    "                sentences.extend(block_sentences)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid SPLIT_BASE value\")\n",
    "        \n",
    "        # print(doc_id, page_index)\n",
    "        start_time = time.time()\n",
    "        sentence_tag_details = predict_page(sentences, prob_threshold=0.4)\n",
    "        end_time = time.time()\n",
    "        total_time += end_time - start_time\n",
    "        snippets = create_snippets(sentence_tag_details, look_ahead=1)\n",
    "        \n",
    "        # Create snippets in structure expected by the evaluation script and similar to the data_dict\n",
    "        predicted_snippets = []\n",
    "        for snippet in snippets:\n",
    "            snippet_text = snippet\n",
    "            snippet_tokenized_text = tokenizer.tokenize(snippet_text)\n",
    "            dict_to_add = {'text': snippet_text, 'tokenized_text': snippet_tokenized_text}\n",
    "            predicted_snippets.append(dict_to_add)\n",
    "        \n",
    "        # Add predicted snippets for the current page to the new dictionary\n",
    "        predicted_snippets_dict[doc_id][page_index] = predicted_snippets\n",
    "        number_of_pages += 1\n",
    "\n",
    "with open(predicted_snippets_path, 'wb') as f:\n",
    "    pickle.dump(predicted_snippets_dict, f)\n",
    "    \n",
    "print(f\"Total time taken: {total_time:.2f} seconds\")\n",
    "print(f\"Predicted snippets saved to {predicted_snippets_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update data_dict with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c30bc7dd3c44ea5b0a1dba6eff691d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load from pickle file\n",
    "with open(predicted_snippets_path, 'rb') as f:\n",
    "    loaded_predicted_snippets_dict = pickle.load(f)\n",
    "\n",
    "# update data_dict with predicted snippets\n",
    "for doc_id in tqdm(loaded_predicted_snippets_dict):\n",
    "    if doc_id in data_dict:\n",
    "        for page_index in loaded_predicted_snippets_dict[doc_id]:\n",
    "            if page_index in data_dict[doc_id]:\n",
    "                \n",
    "                # Update the specific page with predicted snippets\n",
    "                data_dict[doc_id][page_index]['predicted_snippets'] = loaded_predicted_snippets_dict[doc_id][page_index]\n",
    "                \n",
    "                # Change gold data to match tokenization approach of sentence level model!!!!\n",
    "                # Senetnce level removes formatting!!!!\n",
    "                \n",
    "                # 1. get sentences for full text:\n",
    "                blocks = data_dict[doc_id][page_index]['blocks']\n",
    "                sentences = []\n",
    "                for block in blocks:\n",
    "                    block_text = block['text']\n",
    "                    block_sentences = sentenizer.tokenize_into_sentences(block_text)\n",
    "                    sentences.extend(block_sentences)\n",
    "                # concat sentences to obtain full text\n",
    "                full_text = ' '.join(sentences)\n",
    "                \n",
    "                # 2. tokenize full text\n",
    "                tokenized_text = tokenizer.tokenize(full_text)\n",
    "                \n",
    "                data_dict[doc_id][page_index]['full_text'] = full_text\n",
    "                data_dict[doc_id][page_index]['tokenized_text'] = tokenized_text\n",
    "                \n",
    "                # 3. chnage refined regions to match sentence level tokenization (important step for evalaution consistency!)\n",
    "                refined_regions_original = list(data_dict[doc_id][page_index]['refined_regions'])  # Convert tuple to list\n",
    "                for idx, refined_region_original in enumerate(refined_regions_original):\n",
    "                    refined_region_original_text = refined_region_original['text']\n",
    "                    sentences = sentenizer.tokenize_into_sentences(refined_region_original_text)\n",
    "                    reconstructed_text = ' '.join(sentences)\n",
    "                    reconstructed_tokens = tokenizer.tokenize(reconstructed_text)\n",
    "                    new_dict = {'text': reconstructed_text, 'tokenized_text': reconstructed_tokens, 'tags': refined_region_original['tags']}\n",
    "                    refined_regions_original[idx] = new_dict\n",
    "                data_dict[doc_id][page_index]['refined_regions'] = tuple(refined_regions_original)  # Convert back to tuple\n",
    "            else:\n",
    "                # Handle cases where the page_index does not exist in data_dict[doc_id]\n",
    "                print(f\"Page index {page_index} for document ID {doc_id} was not found in data_dict.\")\n",
    "    else:\n",
    "        # Handle cases where the doc_id does not exist in data_dict\n",
    "        print(f\"Document ID {doc_id} was not found in data_dict.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate snippets vs. ground-truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1255c7e20b446d2881fd8226edca49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.6154113052658\n",
      "Average Recall Score: 0.7528642310241872\n",
      "Average F1 Score: 0.6772337103370286\n",
      "Average Iou Score: 0.6049309109738842\n",
      "Average Bleu Score: 0.5151440009778114\n",
      "Average Jaccard Score: 0.6051036822247481\n",
      "Average Edit_distance Score: 138.73980649619904\n",
      "Average Precision_region_lvl Score: 0.7012859916489036\n",
      "Average Recall_region_lvl Score: 0.7951105926751747\n",
      "Average F1_region_lvl Score: 0.6608763024807144\n",
      "Average Rouge-1-f Score: 0.7215226353820612\n",
      "Average Rouge-2-f Score: 0.6673863751875584\n",
      "Average Rouge-l-f Score: 0.7175806180757638\n",
      "{'precision': 0.6154113052658, 'recall': 0.7528642310241872, 'f1': 0.6772337103370286, 'iou': 0.6049309109738842, 'bleu': 0.5151440009778114, 'jaccard': 0.6051036822247481, 'edit_distance': 138.73980649619904, 'precision_region_lvl': 0.7012859916489036, 'recall_region_lvl': 0.7951105926751747, 'f1_region_lvl': 0.6608763024807144, 'rouge-1-f': 0.7215226353820612, 'rouge-2-f': 0.6673863751875584, 'rouge-l-f': 0.7175806180757638, 'inference_time': 129.26438164710999, 'pages': 6908, 'batch_size': 'All sentences of a page', 'GPU': 'NVIDIA A100-SXM4-80GB'}\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import evaluator\n",
    "importlib.reload(evaluator)\n",
    "\n",
    "metrics_config = {\n",
    "    'iou': True,\n",
    "    'bleu': True,\n",
    "    'jaccard': True,\n",
    "    'precision': True,\n",
    "    'recall': True,\n",
    "    'f1': True,\n",
    "    'precision_region_lvl': True,\n",
    "    'recall_region_lvl': True,\n",
    "    'f1_region_lvl': True,\n",
    "    'edit_distance': True,\n",
    "    'rouge-1-f': True,\n",
    "    'rouge-2-f': True,\n",
    "    'rouge-l-f': True,\n",
    "    'pk': False,\n",
    "    'windowdiff': False,\n",
    "    'cohen_kappa': False\n",
    "}\n",
    "\n",
    "aggregated_metrics_snippets = evaluator.evaluate_snippets_parallel(data_dict, \"predicted_snippets\", \"refined_regions\", metrics_config, DEBUG=False)\n",
    "aggregated_metrics_snippets['inference_time'] = total_time\n",
    "aggregated_metrics_snippets['pages'] = number_of_pages\n",
    "aggregated_metrics_snippets['batch_size'] = \"All sentences of a page\"\n",
    "aggregated_metrics_snippets['GPU'] = torch.cuda.get_device_properties(0).name\n",
    "print(aggregated_metrics_snippets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AML'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOMAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "with open(f\"Evaluation/{DOMAIN}/sentence_eval_final_{PRETRAINED_MODEL}_different_tokenization.json\", 'w') as f:\n",
    "    json.dump(aggregated_metrics_snippets, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate similarity between blocks and refined regions\n",
    "Note: this is unrelated to the sentence-level model and simply evaluates the similarity between blocks and regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20d3d1eb50540ccb0f9ed0b0938b318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.8613154066652959\n",
      "Average Recall Score: 0.818381861647683\n",
      "Average F1 Score: 0.8392999372804058\n",
      "Average Iou Score: 0.807934991304034\n",
      "Average Bleu Score: 0.7315791964983502\n",
      "Average Jaccard Score: 0.8081440359699109\n",
      "Average Edit_distance Score: 81.82154553689638\n",
      "Average Precision_region_lvl Score: 0.9091115198953504\n",
      "Average Recall_region_lvl Score: 0.846457969011016\n",
      "Average F1_region_lvl Score: 0.8371476073282121\n",
      "Average Rouge-1-f Score: 0.8747242314469701\n",
      "Average Rouge-2-f Score: 0.8557768136541352\n",
      "Average Rouge-l-f Score: 0.874457730429435\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import evaluator\n",
    "importlib.reload(evaluator)\n",
    "\n",
    "metrics_config = {\n",
    "    'iou': True,\n",
    "    'bleu': True,\n",
    "    'jaccard': True,\n",
    "    'precision': True,\n",
    "    'recall': True,\n",
    "    'f1': True,\n",
    "    'precision_region_lvl': True,\n",
    "    'recall_region_lvl': True,\n",
    "    'f1_region_lvl': True,\n",
    "    'edit_distance': True,\n",
    "    'rouge-1-f': True,\n",
    "    'rouge-2-f': True,\n",
    "    'rouge-l-f': True,\n",
    "    'pk': False,\n",
    "    'windowdiff': False,\n",
    "    'cohen_kappa': False\n",
    "}\n",
    "\n",
    "aggregated_metrics_snippets = evaluator.evaluate_snippets_parallel(data_dict, \"blocks\", \"refined_regions\", metrics_config, DEBUG=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
